{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#ros2-labs","title":"ROS2 Labs","text":"<p>Practical Robotics Courses at the University of Sheffield with ROS2 and the TurtleBot3 Waffle. </p> <p> </p> <p>By Tom Howard Department of Multidisciplinary Engineering Education  </p> <p>(Image courtesy of Andy Brown)</p> <p>Find out more...</p> <p></p>"},{"location":"about/","title":"About this Site","text":""},{"location":"about/#welcome","title":"Welcome","text":"<p>This is the home of The ROS2 Course Assignments for COM2009: a second-year undergraduate module for The School of Computer Science. </p> <p>This course is designed to teach students how to use ROS2 (The Robot Operating System, Version 2) with TurtleBot3 Waffle robots, using a mix of simulation-based learning and real robot hardware. Most of the initial learning is done in simulation, after which students are able to apply their new-found ROS knowledge to our real TurtleBot3 Waffle Robots.</p> <p>The materials here are developed by Dr Tom Howard, a University Teacher in the Multidisciplinary Engineering Education Team in The Diamond.</p> <p>These resources are also used to teach masters-level Mechatronic and Robotic Engineering students in The School of Electrical and Electronic Engineering (ACS6121). </p>"},{"location":"about/#license","title":"License","text":"<p>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</p>"},{"location":"about/#robots","title":"The TurtleBot3 Waffle","text":""},{"location":"about/#turtlebot-what","title":"Turtlebot what?!","text":"<p>To teach ROS here we use the TurtleBot3 Waffle robot, made by Robotis. This is the 3rd Generation Robot in the TurtleBot family (which has been the reference hardware platform for ROS since 2010). The TurtleBot Robot family exists to provide accessible and relatively low-cost hardware and open-source software on a robot platform, to encourage people to learn robotics and ROS and make it as easy as possible to do so.</p>"},{"location":"about/#ebook","title":"The (Free) TurtleBot3 eBook","text":"<p>The TurtleBot3 Waffle developers (Robotis) have written a book on programming robots with ROS. This is available as a free eBook, which you can download here, and we recommend that you do so! This is a great resource which provides a detailed introduction to what ROS is and how it works, as well as a comprehensive \"Beginners Guide\" to ROS programming. The other great thing about this is that it is tailored to the TurtleBot3 Robot specifically, providing examples of how to use a range of TurtleBot3 packages along with a detailed description of how they work.</p> <p>We recommend that you have a look at this book to learn more about the concepts that you are exploring in this course.</p>"},{"location":"about/#our-waffles","title":"Our Waffles","text":"<p>Here in the Diamond we have a total of 50 customised TurtleBot3 Waffles (aka \"The Waffles\") specifically for teaching the courses here:</p> <p> </p> <p>Our robots are an enhanced version of the TurtleBot3 WafflePi that you can buy from Robotis. We've made a few adjustments, as shown below:</p> <p></p> <p>The Waffles have the following core hardware elements:</p> <ul> <li>An OpenCR Micro-Controller Board to power and control the wheel motors, distribute power to other hardware elements and provide an interface for additional sensors.</li> <li>An UP Squared Single-Board Computer (SBC) with an Intel Processor and 32GB of on-board eMMC storage. This board acts as the \"brain\" of the robot.</li> <li>Independent left and right wheel motors (DYNAMIXEL XM430\u2019s) to drive the robot using a differential drive configuration.</li> </ul> <p>This drive configuration allows the robots to move with the following maximum velocities: </p> <p> Velocity Component Upper Limit Units Linear 0.26 m/s Angular 1.82 rad/s <p></p> <p>In addition to this, the robots are equipped with the following sensors:</p> <ul> <li>A Light Detection and Ranging (or LiDAR) sensor, which spins continuously when the robot is in operation. This uses light in the form of laser pulses to allow the robot to measure the distance to surrounding objects, providing it with a 360\u00b0 view of its environment.</li> <li>An Intel RealSense D435 Camera with left and right imaging sensors, allowing depth sensing as well as standard image capture.</li> <li>A 9-Axis Inertial Measurement Unit (or IMU) on-board the OpenCR Micro Controller board, which uses an accelerometer, gyroscope and magnetometer to measure the robot's specific force, acceleration and orientation. </li> <li>Encoders in each of the DYNAMIXEL wheel motors, allowing measurement of speed and rotation count for each of the wheels.</li> </ul>"},{"location":"about/#software","title":"Software","text":"<p>Our robots currently run ROS 2 Humble Hawksbill (or \"Humble\" for short). The courses here are therefore based around this version of ROS. The easiest way to install Humble is via Deb packages for Ubuntu Jammy (22.04). This is the setup we recommend and - as such - all out robotics hardware runs with this OS/Software setup.</p> <p>To deliver the simulation-based parts of this course, we've created a custom simulation environment using the Windows Subsystem for Linux (WSL). This has been developed primarily to run on University of Sheffield Managed Desktop Computers, which run Windows 10, but it's also possible to run this on other machines too. We call this simulation environment \"WSL-ROS\". See here for more details (TODO).</p> <p>You can find out more about installing ROS on your own system here (TODO).</p>"},{"location":"about/#laptops","title":"Laptops","text":"<p>In the Diamond, we have dedicated Robot Laptops running the same OS &amp; ROS version as above. We use these when working with the robots in the lab. See here for more details (TODO). </p>"},{"location":"about/#version-history","title":"Version History","text":""},{"location":"about/#iteration-1","title":"Iteration 1","text":"<p>Academic Year: 2024-25 </p> <ul> <li>This course used to live here, and was based on ROS 1 Noetic. This year, we've upgraded everything to ROS 2, re-written the courses and moved everything over to this new site. See the previous version history here.</li> <li>Other notable changes:<ul> <li>COM2009 Assignment #2 now only involves three programming tasks (where previously it was four), with an additional assessment on documentation now included instead.</li> </ul> </li> </ul>"},{"location":"course/","title":"COM2009 Course Assignments","text":"<p>For the COM2009 Robotics course you must complete two lab assignments:</p> <ul> <li> <p>Assignment #1: \"An Introduction to ROS (the Robot Operating System)\".</p> <p>Here you will learn what ROS is and how to use it. You will complete this assignment individually, and in your own time.</p> <p>Weighting: 25% of the overall COM2009 module mark.</p> </li> <li> <p>Assignment #2: \"Team Robotics Project\".</p> <p>Here you will work in teams of 3-4 to complete a series of real-world robotics tasks using our Tutlebot3 Waffle Robots in the Lab (Diamond Computer Room 5).</p> <p>Weighting: 30% of the overall COM2009 module mark</p> </li> </ul>"},{"location":"course/assignment1/","title":"Assignment #1: An Introduction to ROS","text":""},{"location":"course/assignment1/#overview","title":"Overview","text":"<p>Assignment #1 is an X-part course, which you should complete in full and in order. The course is designed to be completed in simulation, so you will therefore need access to a ROS installation which can either be installed on your own machine, or accessed on a range of managed computers across the University of Sheffield campus. See here for more information on how to install ROS (TODO).</p> <p>Each part of the course comprises a series of step-by-step instructions and exercises to teach you how ROS works, and introduces you to the core principles of the framework. The exercises give you the opportunity to see how to apply these principles to practical robotic applications. Completing this course is essential for obtaining all the necessary skills for Assignment #2: the Team Robotics Project, where you will work in teams to program our real TurtleBot3 Waffle robots (TODO).  </p>"},{"location":"course/assignment1/#the-course","title":"The Course","text":"<ul> <li> <p>Part 1: Getting Started with ROS2</p> <p>In this first part you will learn the basics of ROS and become familiar with some key tools and principles of this framework, allowing you to program robots and work with ROS applications effectively.</p> </li> <li> <p>Part 2: Odometry &amp; Navigation</p> <p>In this session you'll learn about Odometry data, which informs us of a robot's position in an environment. You'll also learn how to control a ROS robot's velocity (and thus its position) using both open and closed-loop control methods.</p> </li> <li> <p>Part 3: SLAM &amp; Autonomous Navigation</p> <p>Here you'll take your first look at the LiDAR sensor, the data that it generates, and how this can be of huge benefit for robotics applications. You'll see this in practice by leveraging the mapping and autonomous navigation tools within ROS.</p> </li> <li> <p>Part 4: ROS Services</p> <p>In this part of the course you'll learn about ROS Services, which offer an alternative way for nodes to communicate in ROS. You will see how this framework can be used to control a robot or invoke certain behaviours more effectively for certain tasks.</p> </li> <li> <p>Part 5: ROS Actions</p> <p>Building on what you learnt about ROS Services in Part 4, here you will look at ROS Actions, which are similar to Services, but with a few key differences.</p> </li> <li> <p>Part 6: Cameras, Machine Vision &amp; OpenCV</p> <p>Here you'll learn how to work with images from an on-board camera. You will look at techniques to detect features within these images, and use this to inform robot decision-making.</p> </li> </ul>"},{"location":"course/assignment1/#assessment","title":"Assessment","text":"<p>This assignment is worth 25% of the overall mark for COM2009, and is assessed via an on-campus Blackboard-based test taking place in week 7 or 8 of the Spring Semester. </p>"},{"location":"course/assignment1/_part4/","title":"Part 4: Services","text":""},{"location":"course/assignment1/_part4/#introduction","title":"Introduction","text":"<p> Exercises: X</p> <p> Estimated Completion Time: Y hours</p>"},{"location":"course/assignment1/_part4/#aims","title":"Aims","text":"<p>In this part you will learn about ROS Services, a communication method that facilitates request-response interactions between nodes. You will understand how to use ROS services in combination with standard publisher/subscriber principles to enhance control for specific operations. Additionally, you'll create custom messages and services for tailored communication.</p>"},{"location":"course/assignment1/_part4/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Recognise how ROS Services differ from the standard topic-based publisher-subscriber approach, and identify appropriate use-cases for this type of messaging system.</li> <li>Implement Python node pairs to observe services in action, and understand how they work.</li> <li>Invoke different services using a range of service message types.</li> <li>Develop Python Service nodes of your own to perform specific robotic tasks.</li> <li>Harness Services, in combination with LiDAR data, to implement a basic obstacle avoidance behaviour </li> <li>Develop custom ROS messages and services (still need to think about the task for this) </li> <li>Demonstrate your understanding of ROS2 so far by developing a Python node which incorporates elements from this and previous parts of this course.</li> </ol>"},{"location":"course/assignment1/_part4/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: </li> </ul>"},{"location":"course/assignment1/_part4/#additional-resources","title":"Additional Resources","text":""},{"location":"course/assignment1/_part4/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, ensure that you have the following:</p> <ol> <li>ROS2 Humble installed on your system</li> <li>Cloned the tuos package from github</li> <li>Basic understanding of ROS2 concepts like nodes and topics</li> </ol>"},{"location":"course/assignment1/_part4/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>If you haven't done so already, launch your ROS environment now:</p> <p>Step 2: Restore your work (todo)</p> <p>Step 3: Launch VS Code (todo) </p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Once again, it's worth quickly checking that the Course Repo is up-to-date before you start on the Part 4 exercises. Go back to Part 1 if you haven't installed it yet (really?!). For the rest, run the following commands: <pre><code>cd ~/ros2_ws/src/tuos_ros/ &amp;&amp; git pull\n</code></pre></p> <p>Now build with colcon: <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build \n</code></pre> Finally, re-source the environment  <pre><code>source ~/.bashrc\n</code></pre></p>"},{"location":"course/assignment1/_part4/#step-5-launch-the-robot-simulation","title":"Step 5: Launch the Robot Simulation","text":"<p>From TERMINAL 1, launch the TurtleBot3 Waffle \"Empty World\"  simulation:</p> <p>TERMINAL 1: <pre><code>ros2 launch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre> ...and then wait for the Gazebo window to open:</p>"},{"location":"course/assignment1/_part4/#an-introduction-to-services","title":"An Introduction to Services","text":"<p>So far, we've learnt about ROS topics and messages, and how individual nodes can access data on a robot by simply subscribing to topics that are being published by any other node on the system.  In addition to this, we also learnt how any node can publish messages to any topic: this essentially broadcasts the data contained in the message across the ROS Network, making it available to any other node on the network that may wish to access it.</p> <p>ROS2 uses interface as a communication structure that allow different nodes to exchange data. These interfaces are broadly categorized into three types:</p> <ol> <li>Messages</li> <li>Services</li> <li>Actions</li> </ol> <p>These are different to messages in that \"Service calls\" (that is, the process of requesting a service) occur only between one node and another:</p> <ul> <li>One node (a Service Client) sends a Request to another node.</li> <li>Another node (a Service Server) processes that request, performs an action and then sends back a Response.</li> </ul> <p> </p> \"Understanding services: Single Service Client\".Taken from the ROS 2 Documentation (Humble)\u00a9 Copyright 2025, Open Robotics licensed under CC BY 4.0 <p>Services are Synchronous (or sequential): When a ROS node sends a request to a service (as a Service Client) it can't do anything else until the service has been completed and the Service Server has sent a response back. This can be useful for a few reasons:</p> <ul> <li> <p>There can be multiple service clients using the same service but only one service server for a service. </p> </li> <li> <p>Discrete, short-duration actions: A robot might need to do something before it can move on to something else, e.g.:</p> <ul> <li>A robot needs to see something before it can move towards it.</li> <li>High definition cameras generate large amounts of data and consume battery power, so you may wish to turn a camera on for a specific amount of time (e.g. until an image has been captured) and then turn it off again.</li> </ul> </li> <li> <p>Computations: Remember that ROS is network-based, so you might want to offload some computations to a remote computer or a different device on a robot, e.g.:</p> <ul> <li>A client might send some data and then wait for another process (the server) to process it and send back the result.</li> </ul> </li> </ul> <p>It's also worth noting that any number of ROS Client nodes can call a service, but you can only have a single Server providing that particular service at any one time.</p> <p> </p> \"Understanding services: Multiple Service Clients\".Taken from the ROS 2 Documentation (Humble)\u00a9 Copyright 2025, Open Robotics licensed under CC BY 4.0 <p>Question</p> <p>Can you think of any other scenarios where this type of communication protocol might be useful?</p>"},{"location":"course/assignment1/_part4/#ex1","title":"Exercise 1: Creating a Service Server in Python and calling it from the command-line","text":"<p>To start with, let's set up a service and learn how to make a call to it from the command-line to give you an idea of how this all works and why it might be useful.</p> <ol> <li> <p>First open up a new terminal instance (TERMINAL 2) and source your ROS2 environment as you did in part 1: </p> <ol> <li>Now navigate into the Course Repo <code>ros2_ws/src/tuos_ros</code> and run the helper script agian as you did in part 1 to create a new package called <code>part4_services</code>:</li> </ol> <p>TERMINAL 2: <pre><code>./create_pkg.sh part4_services        \n</code></pre></p> <p>Your terminal will return a message verifying the creation of your package.</p> </li> <li> <p>Navigate into the new package directory using cd:</p> <p>TERMINAL 2:</p> <pre><code>cd ../part4_services/\n</code></pre> </li> <li> <p>Then navigate into the <code>scripts</code> folder in the package directory (using cd) and create an empty file called <code>move_server.py</code> using <code>touch</code> command.     <pre><code>touch move_server.py\n</code></pre></p> </li> <li> <p>Open the file in VS Code, copy and paste this code and then save it.  (todo: need to add the template)</p> <p>Note</p> <p>It's really important that you understand how the code above works, so that you know how to build your own service Servers in Python. Also, make sure that the <code>move_server</code> file is executable using <code>chmod +x</code> command. </p> </li> <li> <p>Next, we need to add our <code>move_server.py</code> file as an executable to our package's <code>CMakeLists.txt</code>. This will ensure that it then gets built when we run <code>colcon build</code> (in the next step):</p> <p>In VS Code, open the <code>CMakeLists.txt</code> file that is at the root of your <code>part4_services</code> package directory (<code>ros2_ws/src/part4_services/CMakeLists.txt</code>). Locate the lines (near the bottom of the file) that read:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/minimal_node.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> <p>Replace <code>minimal_node.py</code> with <code>move_server.py</code> to define this as a Python executable in your package:</p> <p><pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/publisher.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> i.  It's a good practice to run <code>rosdep</code> in the root of your workspace (<code>ros2_ws</code>) to check for missing dependencies before building:</p> <pre><code>rosdep install -i --from-path src --rosdistro humble -y\n</code></pre> </li> <li> <p>Then, use Colcon to build your new package and its contents :</p> <p>TERMINAL 2: <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-select part4_services --symlink-install\n</code></pre> i. Finally re-source your <code>bashrc</code>:  <pre><code>source ~/.bashrc\n</code></pre></p> </li> <li> <p>Now, we are ready to run the node. Use <code>ros2 run</code> and observe what is displayed on the terminal </p> <p>TERMINAL 2: <pre><code>ros2 run part4_services move_server.py\n</code></pre> You should see this message:  <pre><code>The 'move_service' server is ready to be called...\n</code></pre></p> </li> <li> <p>Open a new terminal window (TERMINAL 3) </p> </li> <li> <p>While the node is running, use <code>ros2 service</code> command to view all the currently active services on the system:</p> <p>TERMINAL 3: <pre><code>ros2 service list\n</code></pre> You should see the <code>/move_service</code> service that we defined in the Python code: <pre><code>service_name = \"move_service\"\n</code></pre></p> </li> <li> <p>We can find out more about this using the <code>ros2 service type</code> command:</p> <p>TERMINAL 3: <pre><code>ros2 service type /move_service \n</code></pre> Which should provide the following output:</p> <pre><code>std_srvs/srv/SetBool \n</code></pre> <p>This shows that the <code>move_service_server</code> node is using <code>SetBool</code> service (or interface) type defined in the <code>std_srv</code> package</p> <p>Tip</p> <p>You can also view the type of all services at the same time by adding <code>-t</code> to the <code>ros2 service list</code> command.</p> </li> <li> <p>We can also call this service from the command line using <code>ros2 service call</code>. <pre><code>ros2 service call &lt;service_name&gt; &lt;service_type&gt; &lt;arguments&gt; \n</code></pre></p> <p>In this case,  <pre><code>ros2 service call /move_service std_srvs/srv/SetBool \"data: false\"\n</code></pre></p> </li> <li> <p>Press Enter to issue the command and make a call to the service.      You should see the following response:     <pre><code>requester: making request: std_srvs.srv.SetBool_Request(data=False)\n\nresponse:\nstd_srvs.srv.SetBool_Response(success = False, message=\"Nothing happened, set request_signal to 'true' next time.\")\n</code></pre></p> </li> <li> <p>Arrange the terminals 1 and 3 so that you can see both the Gazebo simulation and the terminal that you just issued the <code>ros2 service call</code> command in.</p> </li> <li> <p>In TERMINAL 3 enter the <code>ros2 service call</code> command again, but this time set the <code>data</code> input to <code>true</code>. You should be able to see the response of the robot in Gazebo simulation. Switch back to TERMINAL 2 and observe the terminal output there as well. </p> </li> </ol> <p>Summary</p> <p>In the section above, you learned how to create a Service Server node. This node sits idle and keeps waiting for its service to be called. Then you called the service through command line which prompted the Server to carry out the following tasks defined in the Python code,     1. Start a timer     1. Issue a velocity commnad to the robot to move it forward     1. Wait for 5 seconds     1. Issue a velocity command to stop the robot     1. Get the service Response and issue it as an output to the terminal in which the service is called</p>"},{"location":"course/assignment1/_part4/#understanding-key-features-of-ros2-service","title":"Understanding key features of ros2 service","text":"<p>In Part 2, you learned how to find out more about a particular message type, using the <code>ros2 interface show</code> command. You can do the same to find out the details of service type as follow: </p> <p>Terminal 3 <pre><code>ros2 interface show std_srvs/srv/SetBool\n</code></pre> which will give the following output: </p> <pre><code>bool data   # e.g. for hardware enabling / disabling \n---\nbool success     # indicate successful run of triggered service\nstring message   # informational, e.g. for error messages\n</code></pre>"},{"location":"course/assignment1/_part4/#the-format-of-setbool-service","title":"The Format of SetBool Service","text":"<p>The service above is structured in two parts separated by three hyphens (<code>---</code>). The part above the hyphens is called the Service Request while the part below is Service Response:</p> <pre><code>bool data        &lt;-- Request \n---\nbool success     &lt;-- Response (Parameter 1 of 2)\nstring message   &lt;-- Response (Parameter 2 of 2)\n</code></pre> <p>In order to Call a service, we need to provide data to it in the format specified in the Request section. A service Server (like the Python node we created above) will then send data back to the caller in the format specified in the Response section.</p> <p>The <code>std_srvs/srv/SetBool</code> service that we're working with here has one request parameter:</p> <ol> <li>A boolean input called <code>data</code>     ...which is the only thing we need to send to the Service Server in  order to call the service.</li> </ol> <p>There are then two response parameters:</p> <ol> <li>A boolean flag called <code>success</code></li> <li>A text string called <code>message</code>     ...both of these will be returned to the client, by the server, once  the Service has completed.</li> </ol>"},{"location":"course/assignment1/_part4/#ex2","title":"Exercise 2: Creating a Python Service Client Node","text":"<p>Instead of calling a service from command-line we can also build Python Service Client Nodes to do the same. In this exercise you will learn how this is done.</p> <ol> <li> <p>TERMINAL 3 should be idle, so from here navigate to the <code>scripts</code> folder within the <code>part4_services</code> package that we created earlier:</p> <p>TERMINAL 3: <pre><code>cd ~/ros2_ws/src/part4_services/scripts\n</code></pre></p> </li> <li> <p>Create a new file called <code>move_client.py</code></p> </li> <li> <p>Now as you did in the previous exercise, open the VS Code, copy and paste this code and then save it. </p> <p>Note</p> <p>Once again, be sure to read the code and understand how this Python Service Client Node works too!</p> </li> <li> <p>Return to TERMINAL 3 and launch the node using <code>ros2 run</code>:</p> <pre><code>ros2 run part4_services move_client.py \n</code></pre> </li> </ol> <p>The response should be exactly the same when we called the service from the command line. </p>"},{"location":"course/assignment1/_part4/#ex3","title":"Exercise 3: Learn to create custom services","text":"<p>In previous exercises you learned about messages, topics and services by using the predefined definitions of them. While using predefined interfaces is considered a good practice, it is also important to know how you can custom define these interfaces based on your own need. This exercise will teach you, step-by-step, how to create custom service definition and use it to move the robot to the requested position (providing x and y coordinates).</p> <p>Procedure </p> <ol> <li>Close down the Service Server that is currently running in TERMINAL 2</li> <li> <p>Navigate to your <code>part4_services</code> package </p> <p>TERMINAL 2: <pre><code>cd ~ros2_ws/src/part4_services\n</code></pre> and make a new directory <code>srv</code> by running the following command: <pre><code>mkdir srv\n</code></pre></p> </li> <li> <p>Now navigate into the newly created directory <code>srv</code> and create new file called <code>MoveToPosition.srv</code></p> <p>Note</p> <p>It is important that your file name should end with <code>.srv</code> extension as this identifies the file as a ROS service.</p> </li> <li> <p>As we learned earlier, a service file consists of two parts: <code>Request</code> and <code>Response</code>. Here we will provide our own definition for each one of these parts as follow:</p> <p><pre><code>float32 goal_x      &lt;-- request parameter 1 of 2\nfloat32 goal_y      &lt;-- request parameter 2 of 2\n---\nbool success        &lt;-- response \n</code></pre>  The service takes in two user inputs <code>goal_x</code> and <code>goal_y</code> of type <code>float</code> for the <code>x</code> and <code>y</code> coordinates to where the robot needs to move.</p> </li> <li> <p>Open the VS Code, copy and paste the above lines and save the file. </p> </li> <li> <p>We need to add a few lines in the <code>CMakeList.txt</code> to convert the defined service into language-specific code (C++ and Python) and make it usable:</p> <pre><code>find_package(rosidl_default_generators REQUIRED)\nrosidl_generate_interfaces(${PROJECT_NAME}\n\"srv/MoveToPosition.srv\"\n)\n</code></pre> </li> <li> <p>Now open the <code>Package.xml</code> file and add the following lines:     <pre><code>&lt;build_depend&gt;rosidl_default_generators&gt;&lt;/build_depend&gt;\n&lt;exec_depend&gt;rosidl_default_runtime&gt;&lt;/exec_depend&gt;\n&lt;member_of_group&gt;rosidl_interface_packages&gt;&lt;/member_of_group&gt;\n</code></pre>     These lines specify the dependencies required to run the custom service. </p> </li> <li> <p>Next, navigate to the <code>scripts</code> folder of the <code>part4_services</code> package and create an empty file called <code>MoveToPosition.py</code>. </p> </li> <li> <p>Open the newly created file in VS Code. Copy and paste the code provided here. </p> </li> <li> <p>Now modify the code as follow:</p> <ol> <li>Change the imports to utilise the service we just created </li> <li>Develop the callback_function() to: <ol> <li>Process the two parameters that will be provided to the server via the `service request</li> <li>Retrieve the current position and calculate the difference to goal</li> <li>Generate movement command to the specific coordinates</li> <li>Return a correctly formatted service response message to the service caller</li> </ol> </li> <li>Launch the server node using <code>ros2 run</code> command from TERMINAL 2 and <code>call</code> the service from the command-line using <code>ros2 service call</code> in TERMINAL 3 as you did earlier</li> </ol> </li> <li> <p>Make sure you build the package in the root directory of ros2 workspace using <code>colcon</code> and source the environment: </p> Tip <p>For convenience, you can use a handy alias <code>src</code> instead of writing the whole <code>source ~/.bashrc</code></p> </li> </ol>"},{"location":"course/assignment1/_part4/#a-recap-on-everything-youve-learnt-so-far","title":"A recap on everything you've learnt so far...","text":"<p>You should now hopefully understand how to use the ROS2 Service architecture and understand why, and in what context, it might be useful to use this type of communication method in a robot application.</p> <p>Remember</p> <p>Services are synchronous and are useful for one-off, quick actions or for offloading jobs or computations that might need to be done before something else can happen. (Think of it as a transaction that you might make in a shop: You hand over some money, and in return you get a chocolate bar, for example!)</p>"},{"location":"course/assignment1/_part4/#ex4","title":"Exercise 4: Creating your own Service","text":"<p>In this exercise you will create your own service Server to make the Waffle perform a specific movement for a given amount of time and then stop.</p>"},{"location":"course/assignment1/part1/","title":"Part 1: Getting Started with ROS 2","text":""},{"location":"course/assignment1/part1/#introduction","title":"Introduction","text":"<p> Exercises: 8 Estimated Completion Time: 2 hours</p>"},{"location":"course/assignment1/part1/#aims","title":"Aims","text":"<p>In the first part of this lab course you will learn the basics of ROS and become familiar with some key tools and principles of the framework which will allow you to program robots and work with ROS applications effectively.  For the most part, you will interact with ROS using the Linux command line and so you will also become familiar with some key Linux command line tools that will help you.  Finally, you will learn how to create some basic ROS Nodes using Python and get a taste of how ROS topics and messages work.</p>"},{"location":"course/assignment1/part1/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:  </p> <ol> <li>Control a TurtleBot3 Robot, in simulation, using ROS.</li> <li>Launch ROS applications using <code>ros2 launch</code> and <code>ros2 run</code>.</li> <li>Interrogate running ROS applications using key ROS command line tools.</li> <li>Create a ROS package comprised of multiple nodes and program these nodes (in Python) to communicate with one another using ROS Communication Methods.</li> <li>Create a custom ROS message interface and create Python Nodes to use this.</li> <li>Navigate a Linux filesystem and learn how to do various filesystem operations from within a Linux Terminal.</li> </ol>"},{"location":"course/assignment1/part1/#quick-links","title":"Quick Links","text":""},{"location":"course/assignment1/part1/#exercises","title":"Exercises","text":"<ul> <li>Exercise 1: Launching a simulation and making a robot move</li> <li>Exercise 2: Visualising the ROS Network</li> <li>Exercise 3: Exploring ROS Topics and Messages</li> <li>Exercise 4: Creating your own ROS Package</li> <li>Exercise 5: Creating a publisher node</li> <li>Exercise 6: Creating a subscriber node</li> <li>Exercise 7: Defining our own message</li> <li>Exercise 8: Using a custom ROS Message</li> </ul>"},{"location":"course/assignment1/part1/#additional-resources","title":"Additional Resources","text":"<ul> <li>A Simple Python Publisher</li> <li>A Simple Python Subscriber</li> </ul>"},{"location":"course/assignment1/part1/#first-steps","title":"First Steps","text":"<p>Step 1: Accessing a ROS2 Environment for this Course</p> <p>If you haven't done so already, see here for all the details on how to install or access a ROS environment for this course (TODO).</p> <p>Step 2: Launch ROS</p> <p>Launch your ROS environment.</p> <ol> <li>OPTION 1</li> <li>OPTION 2</li> <li>etc...</li> </ol> <p>Either way, you should now have access to ROS via a Linux terminal instance, and we'll refer to this terminal instance as TERMINAL 1.</p> <p>Step 3: Download The Course Repo</p> <p></p> <p>We've put together a few ROS packages specifically for this course. These all live within this GitHub repo, and you'll need to download and install this into your ROS environment now, before going any further.</p> <p>[TODO: Create a ROS2 Workspace first??]</p> <ol> <li> <p>In TERMINAL 1, Navigate into the \"ROS2 Workspace\" using the <code>cd</code> command<sup>1</sup>:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Then, run the following command to clone the Course Repo from GitHub:</p> <p>TERMINAL 1: <pre><code>git clone https://github.com/tom-howard/tuos_ros.git -b humble\n</code></pre></p> </li> <li> <p>Once this is done, you'll need to build this using a tool called \"Colcon\"<sup>2</sup>:</p> <p>TERMINAL 1: <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-up-to tuos_ros &amp;&amp; source ~/.bashrc\n</code></pre></p> </li> </ol> <p>Don't worry too much about what you just did, for now. We'll cover this in more detail throughout the course. That's it for now though, we'll start using some of the packages that we've just installed a bit later on.</p>"},{"location":"course/assignment1/part1/#ex1","title":"Exercise 1: Launching a simulation and making a robot move","text":"<p>Now that you're all up and running, let's launch ROS and fire up a simulation of our TurtleBot3 Waffle robot... </p> <ol> <li> <p>In the terminal enter the following command to launch a simulation of a TurtleBot3 Waffle in an empty world:  </p> <p>TERMINAL 1: <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre></p> </li> <li> <p>A Gazebo simulation window should open and within this you should see a basic representation of the robot's that you'll work with in the lab (TODO):</p> </li> </ol> <p>[FIGURE]     </p> <ol> <li> <p>With the Gazebo simulation up and running, return to your terminal and open up a second terminal instance (TERMINAL 2)</p> <p>[TODO: use tmux??]</p> </li> <li> <p>In this new terminal instance enter the following command:</p> <p>TERMINAL 2: <pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around using specific buttons on your keyboard:</p> <p> </p> </li> </ol>"},{"location":"course/assignment1/part1/#summary","title":"Summary","text":"<p>You have just launched a number of different applications on a ROS Network using two different ROS commands - <code>ros2 launch</code> and <code>ros2 run</code>: </p> <ol> <li><code>ros2 launch turtlebot3_gazebo empty_world.launch.py</code></li> <li><code>ros2 run turtlebot3_teleop teleop_keyboard</code></li> </ol> <p>These two commands have a similar structure, but work slightly differently. </p> <p>The first command you used was a <code>launch</code> command, which has the following two parts to it (after the <code>launch</code> bit):</p> <pre><code>ros2 launch {[1] Package name} {[2] Launch file}\n</code></pre> <p>Part [1] specifies the name of the ROS package containing the functionality that we want to execute. Part [2] is a file within that package that tells ROS exactly what scripts ('nodes') that we want to launch. We can launch multiple nodes at the same time from a single launch file.</p> <p>The second command was a <code>run</code> command, which has a structure similar to <code>launch</code>:</p> <pre><code>ros2 run {[1] Package name} {[2] Node name}\n</code></pre> <p>Here, Part [1] is the same as the <code>launch</code> command, but Part [2] is slightly different: <code>{[2] Node name}</code>. Here we are directly specifying a single script that we want to execute. We therefore use <code>ros2 run</code> if we only want to launch a single node on the ROS network: the <code>teleop_keyboard</code> node (a Python script), in this case.</p>"},{"location":"course/assignment1/part1/#ros-packages-nodes","title":"ROS Packages &amp; Nodes","text":""},{"location":"course/assignment1/part1/#packages","title":"Packages","text":"<p>ROS applications are organised into packages. Packages are basically folders containing scripts, configurations and launch files (ways to launch those scripts and configurations), all of which relate to some common robot functionality. ROS uses packages as a way to organise all the programs running on a robot. </p> <p>Info</p> <p>The package system is a fundamental concept in ROS and all ROS programs are organised in this way.</p> <p>You will create a number of packages throughout this course, each containing different nodes, launch files and other things too. We'll start to explore this later on in this part of the course.</p>"},{"location":"course/assignment1/part1/#nodes","title":"Nodes","text":"<p>ROS Nodes are executables that perform specific robot tasks and operations. Earlier on we used <code>ros2 run</code> to execute a node called <code>teleop_keyboard</code>, which allowed us to remotely control (or \"teleoperate\") the robot, for example. </p> <p>Question</p> <p>What was the name of the ROS package that contained the <code>teleop_keyboard</code> node? (Remember: <code>ros2 run {[1] Package name} {[2] Node name}</code>)</p> <p>A ROS robot might have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to share data with the other nodes on the ROS Network.</p>"},{"location":"course/assignment1/part1/#the-ros-network","title":"The ROS Network","text":"<p>We can use the <code>ros2 node</code> command to view all the nodes that are currently active on a ROS Network.</p>"},{"location":"course/assignment1/part1/#ex2","title":"Exercise 2: Visualising the ROS Network","text":"<p>You should currently have two terminal instances active: the first in which you launched the Gazebo simulation (TERMINAL 1) and the second with your <code>teleop_keyboard</code> node active (TERMINAL 2).</p> <ol> <li>Open up a new terminal instance now (TERMINAL 3).</li> <li> <p>Use the following command to have a look at which nodes are currently active on the network:</p> <p>TERMINAL 3: <pre><code>ros2 node list\n</code></pre></p> <p>Only a handful of nodes should be listed:</p> <pre><code>/camera_driver\n/gazebo\n/teleop_keyboard\n/turtlebot3_diff_drive\n/turtlebot3_imu\n/turtlebot3_joint_state\n/turtlebot3_laserscan\n</code></pre> </li> <li> <p>We can visualise the connections between the active nodes by using an application called RQT. RQT is a collection of graphical tools that allow us to interact with and interrogate the ROS network. Launch the main RQT application by entering <code>rqt</code> in TERMINAL 3 (you might see some warnings in the terminal when you do this, but don't worry about them):</p> <p>TERMINAL 3: <pre><code>rqt\n</code></pre></p> <p>A window should then open:</p> <p> </p> </li> <li> <p>From here, we then want to load the Node Graph plugin. From the top menu select <code>Plugins</code> &gt; <code>Introspection</code> &gt; <code>Node Graph</code>.</p> </li> <li> <p>Select <code>Nodes/Topics (all)</code> from the top-left most dropdown, and in the <code>Hide</code> section uncheck everything except <code>Debug</code> and <code>Params</code> (you may then need to hit the refresh button):</p> <p> </p> <p>Here, nodes are represented by rectangles and topics by ellipses (hover over a region of the graph to enable colour highlighting).</p> <p>This tool shows us that (amongst other things) the <code>/teleop_keyboard</code> and <code>/turtlebot3_diff_drive</code> nodes are communicating with one another. The direction of the arrow tells us that <code>/teleop_keyboard</code> is a Publisher and <code>/turtlebot3_diff_drive</code> is a Subscriber. The two nodes communicate via a ROS Topic called <code>/cmd_vel</code>. </p> </li> </ol>"},{"location":"course/assignment1/part1/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. Data is published to topics using ROS Messages. As we've just learnt, the <code>teleop_keyboard</code> node was publishing messages to a topic (<code>/cmd_vel</code>) to make the robot move earlier.</p> <p>Let's have a look at this in a bit more detail...</p>"},{"location":"course/assignment1/part1/#ex3","title":"Exercise 3: Exploring ROS Topics and Messages","text":"<p>We can find out more about the <code>/cmd_vel</code> topic by using the <code>ros2 topic</code> command.</p> <ol> <li> <p>Open up yet another new terminal instance (TERMINAL 4) and type the following:</p> <p>TERMINAL 4: <pre><code>ros2 topic list\n</code></pre></p> <p>This shows us all the topics that are currently available on the ROS network (a lot of which we saw in the RQT Node Graph above):</p> <pre><code>/camera/camera_info\n/camera/image_raw\n/clock\n/cmd_vel\n/imu\n/joint_states\n/odom\n/parameter_events\n/performance_metrics\n/robot_description\n/rosout\n/scan\n/tf\n/tf_static\n</code></pre> <p>Let's find out a bit more about <code>/cmd_vel</code>...</p> </li> <li> <p>Use the <code>topic info</code> command now:</p> <p>TERMINAL 4: <pre><code>ros2 topic info /cmd_vel\n</code></pre></p> <p>This should provide the following output:</p> <pre><code>Type: geometry_msgs/msg/Twist\nPublisher count: 1\nSubscription count: 1\n</code></pre> <p>We've now established the following information about <code>/cmd_vel</code>:</p> <ol> <li>The topic has 1 publisher writing data to it</li> <li>The topic also has 1 subscriber reading this data</li> <li>From RQT Node Graph we know that the <code>/teleop_keyboard</code> node is the publisher (i.e. the node writing data to the topic)</li> <li>The <code>/turtlebot3_diff_drive</code> node is receiving this data (and acting upon it). This node therefore monitors (i.e. subscribes to) the <code>/cmd_vel</code> topic and makes the robot move in the simulator whenever a velocity command is published.</li> <li> <p>Data is transmitted on the <code>/cmd_vel</code> topic using an Interface. This particular interface type is: <code>geometry_msgs/msg/Twist</code>. </p> <p>The type field has three parts to it:</p> <ol> <li><code>geometry_msgs</code>: the name of the ROS package that this interface belongs to.</li> <li><code>msg</code>: that this is a topic message rather than another type of interface (there are three types of interface, and we'll learn about the other two later in this course).</li> <li><code>Twist</code>: the actual message type (i.e., the way the data is structured)</li> </ol> <p>In summary then, we've established that if we want to make the robot move we need to publish <code>Twist</code> messages to the <code>/cmd_vel</code> topic.</p> </li> </ol> </li> <li> <p>We can use the <code>ros2 interface</code> command to provide further information about the message structure:</p> <p>TERMINAL 4: <pre><code>ros2 interface show geometry_msgs/msg/Twist\n</code></pre></p> <p>From this, we obtain the following:</p> <pre><code># This expresses velocity in free space broken into its linear and angular parts.\n\nVector3  linear\n        float64 x\n        float64 y\n        float64 z\nVector3  angular\n        float64 x\n        float64 y\n        float64 z\n</code></pre> <p>We'll learn more about what this means in Part 2.</p> </li> <li> <p>To finish, enter Ctrl+C in each of the three terminals that should currently have ROS processes running (Terminals 1, 2 and 3). The associated Gazebo and RQT Node Graph windows should close as a result of this too.</p> </li> </ol> <p>Tip</p> <p>Whenever you need to stop any ROS process use Ctrl+C in the terminal it's running in.</p>"},{"location":"course/assignment1/part1/#creating-your-first-ros-applications","title":"Creating Your First ROS Applications","text":"<p>Shortly you will create some simple publisher and subscriber nodes in Python and send simple ROS messages between them. As we learnt earlier though, ROS applications must be contained within packages, and so we need to create a package first in order to start creating our own ROS nodes. </p> <p>It's important to work in a specific filesystem location when we create and work on our own ROS packages. These are called \"Workspaces\" and you should already have one ready to go within your local ROS environment<sup>3</sup>:</p> <pre><code>~/ros2_ws/src/\n</code></pre> <p>Note</p> <p><code>~</code> is an alias for your home directory. So <code>cd ~/ros2_ws/src/</code> is the same as typing <code>cd /home/{your username}/ros2_ws/src/</code>.</p> <p>Important</p> <p>All new packages must be located in the <code>src</code> folder of the workspace!!</p>"},{"location":"course/assignment1/part1/#ex4","title":"Exercise 4: Creating your own ROS Package","text":"<p>The <code>ros2</code> Command Line Interface (CLI) includes a tool to create a new ROS packages: <code>ros2 pkg create</code>. This tool supports two different \"build types:\"</p> <ol> <li> <p>CMake (for packages containing nodes written in C++):</p> <p><code>ros2 pkg create --build-type ament_cmake</code></p> </li> <li> <p>Python (for packages containing nodes written in well, er, Python!):</p> <p><code>ros2 pkg create --build-type ament_python</code></p> <p>Packages are structured slightly differently in each case.</p> </li> </ol> <p>You can learn more about all this from the Official ROS2 Tutorials (if you're interested).</p> <p>We'll be using Python throughout this course, but we'll actually take a slightly different approach to package creation that will provide us with a little more flexibility and ease of use (particularly for things we'll do later on in the Assignment #1 course and in Assignment #2). We've therefore created a helper script (inside the <code>tuos_ros</code> Course Repo) to help you create packages without using either of the above two commands. The approach we'll take is based on this tutorial (courtesy of the Robotics Backend), so feel free to look at this if you'd like to find out more. Then, simply follow the steps below to create your first ROS package for this course, using the <code>create_pkg.sh</code> helper tool.</p> <ol> <li> <p>Navigate into the <code>tuos_ros</code> Course Repo that you downloaded earlier by using the Linux <code>cd</code> command (change directory). In TERMINAL 1 enter the following:</p> <p>TERMINAL 1: <pre><code>cd ~/ros2_ws/src/tuos_ros/\n</code></pre></p> </li> <li> <p>Here you'll find the <code>create_pkg.sh</code> helper script. Run this now using the following command to create a new package called <code>part1_pubsub</code>:</p> <p>TERMINAL 1: <pre><code>./create_pkg.sh part1_pubsub\n</code></pre></p> </li> <li> <p>Navigate into this new package directory (using <code>cd</code>):</p> <p>TERMINAL 1: <pre><code>cd ../part1_pubsub/\n</code></pre></p> <p>Info</p> <p><code>..</code> means \"go back one directory,\" so that command above is telling <code>cd</code> to navigate out of the <code>tuos_ros</code> directory (and therefore back to <code>~/ros2_ws/src/</code>), and then go into the <code>part1_pubsub</code> directory from there.</p> </li> <li> <p><code>tree</code> is a Linux command which shows us the content of the current directory in a nice tree-like format. Use <code>tree</code> now to show the current content of the <code>part1_pubsub</code> directory:</p> <pre><code>~/ros2_ws/src/part1_pubsub$ tree\n.\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 include\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 part1_pubsub\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 minimal_header.hpp\n\u251c\u2500\u2500 package.xml\n\u251c\u2500\u2500 part1_pubsub\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tb3_tools.py\n\u251c\u2500\u2500 scripts\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 minimal_node.py\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 minimal_node.cpp\n\n5 directories, 7 files\n</code></pre> <ul> <li><code>scripts</code>: is a directory that will contain all the Python Nodes that we'll create (you'll notice a <code>minimal_node.py</code> already exists in there).</li> <li> <p><code>part1_pubsub</code>: is a directory that we can use to store Python modules, that we can then import into our main Python nodes</p> <p>(<code>from part1_pubsub.tb3_tools import ...</code>, for example)</p> </li> <li> <p><code>package.xml</code> and <code>CMakeLists.txt</code>: are both files that define our package, and how it must be built (using <code>colcon build</code>). We'll explore these more shortly... </p> </li> </ul> </li> </ol>"},{"location":"course/assignment1/part1/#ex5","title":"Exercise 5: Creating a publisher node","text":"<ol> <li>From the root of your <code>part1_pubsub</code> package, navigate to the <code>scripts</code> folder using the <code>cd</code> command.</li> <li> <p><code>touch</code> is a Linux command that we can use to create an empty file. Use this to create an empty file called <code>publisher.py</code>, which we will add content to shortly:</p> <p>TERMINAL 1: <pre><code>touch publisher.py\n</code></pre></p> </li> <li> <p>Use <code>ls</code> to verify that the file has been created, but use the <code>-l</code> option with this, so that the command provides its output in \"a long listing format\":</p> <p>TERMINAL 1: <pre><code>ls -l\n</code></pre></p> <p>This should output something similar to the following:</p> <pre><code>~/ros2_ws/src/part1_pubsub/scripts$ ls -l\ntotal 4\n-rwxr-xr-x 1 student student 339 MMM DD HH:MM minimal_node.py\n-rw-r--r-- 1 student student   0 MMM DD HH:MM publisher.py\n</code></pre> <p>This confirms that the file exists, and the <code>0</code> in the middle of the bottom line there indicates that the file is empty (i.e. its current size is 0 bytes), which is what we'd expect.</p> </li> <li> <p>We therefore now need to open the file and add content to it. We'd recommend using Visual Studio Code (VS Code) as an IDE for this course, which can be launched with the following command in TERMINAL 1:</p> <p>TERMINAL 1: <pre><code>code ~\n</code></pre></p> <p>[TODO: does this work for Docker??]</p> </li> <li> <p>Using the VS Code File Explorer, locate the <code>publisher.py</code> file that you have just created (<code>ros2_ws/src/part1_pubsub/scripts/</code>) and click on the file to open it in the main editor. </p> </li> <li> <p>Once opened, copy the code provided here into the empty file and save it. </p> <p>Note</p> <p>It's important that you understand how this code works, so make sure that you read the annotations!</p> </li> <li> <p>Next, we need to add our <code>publisher.py</code> file as an executable to our package's <code>CMakeLists.txt</code>. This will ensure that it then gets built when we run <code>colcon build</code> (in the next step).</p> <p>In VS Code, open the <code>CMakeLists.txt</code> file that is at the root of your <code>part1_pubsub</code> package directory (<code>ros2_ws/src/part1_pubsub/CMakeLists.txt</code>). Locate the lines (near the bottom of the file) that read:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/minimal_node.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> <p>Replace <code>minimal_node.py</code> with <code>publisher.py</code> to define this as a Python executable in your package:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/publisher.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Now, use <code>colcon</code> to build your package.</p> <ol> <li> <p>You MUST run this from the root of your Colcon Workspace (i.e.: <code>~/ros2_ws/</code>), NOT the <code>src</code> directory (<code>~/ros2_ws/src/</code>), so navigate there now using <code>cd</code>:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Then, use the following <code>colcon</code> command to build your package:</p> <pre><code>colcon build --packages-select part1_pubsub --symlink-install\n</code></pre> <p>What do the additional arguments above do?</p> <ul> <li><code>--packages-select</code>: Build only the <code>part1_pubsub</code> package, nothing else (without this <code>colcon</code> would attempt to build every package in the workspace).</li> <li><code>--symlink-install</code>: Ensures that you don't have to re-run <code>colcon build</code> every time you make a change to your package's executables (i.e. your Python files in the <code>scripts</code> directory).</li> </ul> </li> <li> <p>Finally, \"re-source\" your <code>bashrc</code><sup>4</sup>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>We should now be able to run this node using the <code>ros2 run</code> command. </p> <p>Remember: <code>ros2 run {package name} {script name}</code>, so:</p> <p>TERMINAL 1: <pre><code>ros2 run part1_pubsub publisher.py\n</code></pre></p> <p>... Hmm, something not quite right? If you typed the command exactly as above and then tried to run it, you probably just received the following error:</p> <pre><code>$ ros2 run part1_pubsub publisher.py\nNo executable found\n</code></pre> <p></p> <p>When we create a file using <code>touch</code> it is given certain permissions by default. Run <code>ls -l</code> again (making sure that your terminal is in the right location: <code>~/ros2_ws/src/part1_pubsub/scripts/</code>).</p> <p>The first bit tells us about the permissions that are currently set: <code>-rw-r--r--</code>. This tells us who has permission to do what with this file and (currently) the first bit: <code>-rw-</code>, tells us that we have permission to read or write to it. There is a third option we can set too though, which is the execute permission, and we can set this using the <code>chmod</code> Linux command...</p> </li> <li> <p>Run the <code>chmod</code> command as follows:</p> <p>TERMINAL 1: <pre><code>chmod +x publisher.py\n</code></pre></p> </li> <li> <p>Now, run <code>ls -l</code> again to see what has changed:</p> <p>TERMINAL 1: <pre><code>ls -l\n</code></pre></p> <p>We have now granted permission for the file to be executed too:</p> <pre><code>-rwxr-xr-x 1 student student 1125 MMM DD HH:MM publisher.py\n</code></pre> </li> <li> <p>OK, now use <code>ros2 run</code> again to (hopefully!) run the <code>publisher.py</code> node (remember: <code>ros2 run {package name} {script name}</code>).</p> <p>If you see a message in the terminal similar to the following then the node has been launched successfully:</p> <pre><code>[INFO] [#####] [simple_publisher]: The 'simple_publisher' node is inisialised.\n</code></pre> <p>Phew!</p> </li> <li> <p>We can further verify that our publisher node is running using a number of different tools. Try running the following commands in TERMINAL 2:</p> <ol> <li><code>ros2 node list</code>: This will provide a list of all the nodes that are currently active on the system. Verify that the name of our publisher node is visible in this list (it's probably the only item in the list at the moment!)</li> <li><code>ros2 topic list</code>: This will provide a list of the topics that are currently being used by nodes on the system. Verify that the name of the topic that our publisher is publishing messages to (<code>/my_topic</code>) is present within this list.</li> </ol> </li> </ol>"},{"location":"course/assignment1/part1/#rostopic","title":"Interrogating ROS Topics","text":"<p>So far we have used the <code>ros2 topic</code> ROS command with two additional arguments: [TODO: check this!]</p> <ul> <li><code>list</code>: to provide us with a list of all the topics that are active on our ROS system, and</li> <li><code>info</code>: to provide us with information on a particular topic of interest.</li> </ul> <p>We can use the autocomplete functionality of the Linux terminal to provide us with a list of all the available options that we can use with the <code>ros2 topic</code> command.  To do this type <code>ros2 topic</code> followed by a Space and then press the Tab key twice:</p> <pre><code>rostopic[SPACE][TAB][TAB]\n</code></pre> <p>You should then be presented with a list of all options:</p> <p>[TODO: a gif]</p> <ul> <li> <p><code>ros2 topic hz {topic name}</code> provides information on the frequency (in Hz) at which messages are being published to a topic:</p> <pre><code>ros2 topic hz /my_topic\n</code></pre> <p>This should tell us that our publisher node is publishing messages to the <code>/my_topic</code> topic at (or close to) 1 Hz, which is exactly what we ask for in the <code>publisher.py</code> file (in the <code>__init__</code> part of our <code>Publisher</code> class). Enter Ctrl+C to stop this command.</p> </li> <li> <p><code>ros2 topic echo {topic name}</code> shows the messages being published to a topic:</p> <pre><code>ros2 topic echo /my_topic\n</code></pre> <p>This will provide a live stream of the messages that our <code>publisher.py</code> node is publishing to the <code>/my_topic</code> topic. Enter Ctrl+C to stop this.</p> </li> <li> <p>We can see some additional options for the <code>echo</code> command by viewing the help documentation for this too:</p> <pre><code>ros2 topic echo --help\n</code></pre> <p>From here, for instance, we can learn that if we just wanted to print the first message that was received we could use the <code>-once</code> option, for example:</p> <pre><code>ros2 topic echo /my_topic --once\n</code></pre> </li> </ul>"},{"location":"course/assignment1/part1/#ex6","title":"Exercise 6: Creating a subscriber node","text":"<p>To illustrate how information can be passed from one node to another (via topics and messages) we'll now create another node to subscribe to the topic that our publisher node is broadcasting messages to.</p> <ol> <li> <p>In TERMINAL 2 use the filesystem commands that were introduced earlier (<code>cd</code>, <code>ls</code>, etc.) to navigate to the <code>scripts</code> folder of your <code>part1_pubsub</code> package.</p> </li> <li> <p>Use the same procedure as before to create a new empty Python file called <code>subscriber.py</code> and remember to make it executable! </p> </li> <li> <p>Then, open the newly created <code>subscriber.py</code> file in VS Code, paste in the code here and save it. </p> <p>Once again, it's important that you understand how this code works, so make sure you read the code annotations! </p> </li> <li> <p>Next, we need to add this as an additional executable for our package. </p> <p>Open up the <code>CMakeLists.txt</code> file at the root of your <code>part1_pubsub</code> package directory again, head back to the <code># Install Python executables</code> section and add the <code>subscriber.py</code> file:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/publisher.py\n  scripts/subscriber.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Now we need to <code>colcon build</code> again.</p> <ol> <li> <p>Make sure you're at the root of the Colcon Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Run <code>colcon build</code> on only the <code>part1_pubsub</code> package:</p> <pre><code>colcon build --packages-select part1_pubsub --symlink-install\n</code></pre> </li> <li> <p>And then re-source the <code>bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Use <code>ros2 run</code> to execute your newly created <code>subscriber.py</code> node (remember: <code>ros2 run {package name} {script name}</code>). If your publisher and subscriber nodes are working correctly you should see an output like this:</p> <p>[TODO: another gif]</p> </li> <li> <p>Interrogate your ROS network:</p> <ol> <li> <p>As before, we can find out what nodes are running on our system by using the <code>ros2 node list</code> command. Run this in TERMINAL 3, you should see both your publisher and subscriber nodes listed there.</p> </li> <li> <p>Use the <code>ros2 topic</code> command to list all the topics that are available on the network. You should see <code>/my_topic</code> listed there.</p> </li> <li> <p>Use the <code>ros2 topic</code> command again to find more info on <code>my_topic</code>. </p> </li> <li> <p>Use the <code>ros2 interface</code> command to show you what type of data is being sent between the two nodes.</p> </li> </ol> </li> <li> <p>Finally, close down your publisher and subscriber nodes by entering Ctrl+C in the terminals where they are running (should be 1 &amp; 2).</p> </li> </ol>"},{"location":"course/assignment1/part1/#ex7","title":"Exercise 7: Defining our own message","text":"<p>We've just created a publisher and subscriber that were able to communicate with one another via a topic. The data that the publisher was sending to the topic was very simple: a <code>example_interfaces/msg/String</code> type message.</p> <p>This message just has one field called <code>data</code> of the type <code>string</code>:</p> <pre><code>$ ros2 interface show example_interfaces/msg/String\n\nstring data\n</code></pre> <p>ROS messages will generally be more complex than this, typically containing several fields in a single message. We'll define our own custom message now, this time with two fields, so you can see how things work with slightly more complex data types. </p> <ol> <li> <p>Message interfaces must be defined within a <code>msg</code> folder at the root of our package directory, so let's create this folder now in TERMINAL 1:</p> <ol> <li> <p>First, navigate into your package:</p> <pre><code>cd ~/ros2_ws/src/part1_pubsub\n</code></pre> </li> <li> <p>Then use <code>mkdir</code> to make a new directory:</p> <pre><code>mkdir msg\n</code></pre> </li> </ol> </li> <li> <p>We'll create a message called <code>Example</code>, and to do this we'll need to create a new file called <code>Example.msg</code> inside the <code>msg</code> folder:</p> <pre><code>touch msg/Example.msg\n</code></pre> </li> <li> <p>To define the data structure of this message, we now need to open up the file and add the following content:</p> Example.msg<pre><code>string info\nint32 time\n</code></pre> <p>The message will therefore have two fields:</p> <p> # Field Name Data Type 1 <code>info</code> <code>string</code> 2 <code>time</code> <code>int32</code> <p></p> <p>We can give our fields any name that we want, but the data types must be either built-in-types or other pre-existing ROS interfaces.</p> <li> <p>We now need to declare this message in our package's <code>CMakeLists.txt</code> file, so that the necessary Python code can be created (by <code>colcon build</code>) to allow us to import this message into our own Python files.</p> <p>Add the following lines to your <code>part1_pubsub/CMakeLists.txt</code> file, above the <code>ament_package()</code> line:</p> CMakeLists.txt<pre><code>find_package(rosidl_default_generators REQUIRED)\nrosidl_generate_interfaces(${PROJECT_NAME}\n  \"msg/Example.msg\" \n)\n</code></pre> </li> <li> <p>We also need to modify our <code>package.xml</code> file. Add the following lines to this one, just above the <code>&lt;export&gt;</code> line:</p> package.xml<pre><code>&lt;buildtool_depend&gt;rosidl_default_generators&lt;/buildtool_depend&gt;\n&lt;exec_depend&gt;rosidl_default_runtime&lt;/exec_depend&gt;\n&lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;\n</code></pre> </li> <li> <p>We can now use Colcon to generate the necessary source code for the message:</p> <ol> <li> <p>First, make sure you're in the root of the ROS2 Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Then run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part1_pubsub --symlink-install \n</code></pre> </li> <li> <p>And finally re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>We can now verify that this worked with some more <code>ros2</code> command line tools:</p> <ol> <li> <p>First, list all the ROS messages that are available to us on our system:</p> <pre><code>ros2 interface list -m\n</code></pre> <p>Scroll through this list and see if you can find our message in there (it'll be listed as <code>part1_pubsub/msg/Example</code>)</p> </li> <li> <p>Next, show the data structure of the interface:</p> <pre><code>ros2 interface show part1_pubsub/msg/Example\n</code></pre> <p>This should match with how we defined it in our <code>part1_pubsub/msg/Example.msg</code> file.</p> </li> </ol> </li>"},{"location":"course/assignment1/part1/#ex8","title":"Exercise 8: Using a custom ROS Message","text":"<ol> <li> <p>Create a copy of the <code>publisher.py</code> file from Exercise 5. Let's do this from the command line too:</p> <ol> <li> <p>Navigate into your package's <code>scripts</code> folder:</p> <pre><code>cd ~/ros2_ws/src/part1_pubsub/scripts\n</code></pre> </li> <li> <p>And use the <code>cp</code> command to make a copy of the <code>publisher.py</code> file and call this new file <code>custom_msg_publisher.py</code>:</p> <pre><code>cp publisher.py custom_msg_publisher.py\n</code></pre> </li> <li> <p>Let's create a copy of the <code>subscriber.py</code> file too, while we're here:</p> <pre><code>cp subscriber.py custom_msg_subscriber.py\n</code></pre> </li> </ol> </li> <li> <p>Declare these two new files as additional executables in our <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/publisher.py\n  scripts/subscriber.py\n  scripts/custom_msg_publisher.py   # ADD THIS \n  scripts/custom_msg_subscriber.py  # AND THIS\nDESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Run Colcon again (last time now!):</p> <ol> <li>First:     <pre><code>cd ~/ros2_ws\n</code></pre></li> <li>Then:     <pre><code>colcon build --packages-select part1_pubsub --symlink-install\n</code></pre></li> <li>And finally:     <pre><code>source ~/.bashrc\n</code></pre></li> </ol> </li> <li> <p>Now modify your <code>custom_msg_publisher.py</code> file as follows:</p> custom_msg_publisher.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\n\nfrom part1_pubsub.msg import Example # (1)!\n\nclass SimplePublisher(Node):\n\n    def __init__(self):\n        super().__init__(\"simple_publisher\")\n\n        self.my_publisher = self.create_publisher(\n            msg_type=Example, # (2)!\n            topic=\"my_topic\",\n            qos_profile=10,\n        )\n\n        publish_rate = 1 # Hz\n        self.timer = self.create_timer(\n            timer_period_sec=1/publish_rate,\n            callback=self.timer_callback\n        )\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\"\n        )\n\n    def timer_callback(self):\n        ros_time = self.get_clock().now().seconds_nanoseconds()\n\n        topic_msg = Example() # (3)!\n        topic_msg.info = \"The ROS time is...\"\n        topic_msg.time = ros_time[0]\n        self.my_publisher.publish(topic_msg)\n        self.get_logger().info(\n            f\"Publishing: '{topic_msg.info} {topic_msg.time:.0f}'\"\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    my_simple_publisher = SimplePublisher()\n    rclpy.spin(my_simple_publisher)\n    my_simple_publisher.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>We're now importing the <code>Example</code> message from our own <code>part1_pubsub</code> package.</p> </li> <li> <p>We're also now declaring that <code>\"my_topic\"</code> will use the <code>Example</code> message data structure to send messages.</p> </li> <li> <p>We need to deal with the topic messages differently now, to account for the more complex structure.</p> <p>We now populate our messages with two fields: <code>info</code> (a <code>string</code>) and <code>time</code> (an <code>int</code>). Identify what has changed here...</p> </li> </ol> </li> <li> <p>Final Task:</p> <p>Modify the <code>custom_msg_subscriber.py</code> node now to accommodate the new message type that is being published to <code>/my_topic</code>. </p> </li> </ol>"},{"location":"course/assignment1/part1/#wrapping-up","title":"Wrapping Up","text":"<p>In this session we've covered the basics of ROS, and learnt about some key concepts such as Packages; Nodes; and how to send data across a ROS Network using Topics, Messages, and the Publisher-Subscriber Communication Method.</p> <p>We've learnt how to use some key <code>ros2</code> commands:  </p> <ul> <li><code>launch</code>: to launch multiple ROS Nodes via launch files.</li> <li><code>run</code>: to run executables within a ROS package.</li> <li><code>node</code>: to display information about active ROS Nodes.</li> <li><code>topic</code>: to display information about active ROS topics.</li> <li><code>interface</code>: to display information about all ROS Interfaces that are available to use in a ROS application.</li> </ul> <p>We have also learnt how to work in the Linux Terminal and navigate a Linux filesystem using key commands such as:</p> <ul> <li><code>ls</code>: lists the files in the current directory.</li> <li><code>cd</code>: change directory to move around the file system.</li> <li><code>mkdir</code>: make a new directory (<code>mkdir {new_folder}</code>).</li> <li><code>chmod</code>: modify file permissions (i.e. to add execute permissions to a file for all users: <code>chmod +x {file}</code>).</li> <li><code>touch</code>: create a file without any content.</li> </ul> <p>In addition to this we've also learnt how to create a ROS2 package, and how to create simple Python nodes that can publish and subscribe to topics on a ROS network. </p> <p>We've worked with pre-made ROS messages to do this and also created our own custom message interface to offer more advanced functionality.</p> <ol> <li> <p>What is a ROS2 Workspace? You can find out more here.\u00a0\u21a9</p> </li> <li> <p>What is Colcon? Find out more here.\u00a0\u21a9</p> </li> <li> <p>You can learn more about ROS2 Workspaces here.\u00a0\u21a9</p> </li> <li> <p>What does <code>source ~/.bashrc</code> do? See here for an explanation.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment1/part2/","title":"Part 2: Odometry & Navigation","text":""},{"location":"course/assignment1/part2/#introduction","title":"Introduction","text":"<p> Exercises: 6 Estimated Completion Time: 3 hours</p>"},{"location":"course/assignment1/part2/#aims","title":"Aims","text":"<p>In Part 2 you will learn how to control a ROS robot's position and velocity from both the command line and through ROS Nodes. You will also learn how to interpret the data that allows us to monitor a robot's position in its physical environment (odometry).  The things you will learn here form the basis for all robot navigation in ROS, from simple open-loop methods to more advanced closed-loop control (both of which you will explore).</p>"},{"location":"course/assignment1/part2/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Interpret the Odometry data published by a ROS Robot and identify the parts of these messages that are relevant to a 2-wheeled differential drive robot (such as the TurtleBot3).</li> <li>Develop Python nodes to obtain Odometry messages from an active ROS network and translate them to provide useful information about a robot's pose in a convenient, human-readable way.</li> <li>Implement open-loop velocity control of a robot using ROS command-line tools.</li> <li>Develop Python nodes that use open-loop velocity control methods to make a robot follow a pre-defined motion path.</li> <li>Combine both publisher &amp; subscriber communication methods into a single Python node to implement closed-loop (odometry-based) velocity control of a robot.</li> <li>Explain the limitations of Odometry-based motion control methods. </li> </ol>"},{"location":"course/assignment1/part2/#quick-links","title":"Quick Links","text":""},{"location":"course/assignment1/part2/#exercises","title":"Exercises","text":"<ul> <li>Exercise 1: Exploring Odometry Data</li> <li>Exercise 2: Creating a Python Node to Process Odometry Data</li> <li>Exercise 3: Controlling Velocity with the ROS2 CLI</li> <li>Exercise 4: Creating a Python Node to Make a Robot Move in a circle</li> <li>Exercise 5: Implementing a Shutdown Procedure</li> <li>Exercise 6: Making our Robot Follow a Square Motion Path</li> </ul>"},{"location":"course/assignment1/part2/#additional-resources","title":"Additional Resources","text":"<ul> <li>An Odometry Subscriber Node</li> <li>A Simple Velocity Control Node (Move Circle)</li> <li>Odometry-based Navigation (Move Square)</li> </ul>"},{"location":"course/assignment1/part2/#getting-started","title":"Getting Started","text":"<p>TODO:</p> <p>Step 1: Launch your ROS Environment</p> <p>If you haven't done so already, launch your ROS environment now:</p> <ol> <li>OPTION 1</li> <li>OPTION 2</li> <li>etc...</li> </ol> <p>You should now have access to ROS via a Linux terminal instance. We'll refer to this terminal instance as TERMINAL 1.</p> <p>Step 2: Make Sure The Course Repo is Up-To-Date</p> <p></p> <p>In Part 1 you should have downloaded and installed The Course Repo into your ROS environment. If you haven't done this yet then go back and do it now. If you have already done it, then it's worth just making sure it's all up-to-date, so run the following command now to do so:</p> <p>TERMINAL 1: <pre><code>cd ~/ros2_ws/src/tuos_ros/ &amp;&amp; git pull\n</code></pre></p> <p>Then build with Colcon: </p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-up-to tuos_ros\n</code></pre> <p>And finally, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Warning</p> <p>If you have any other terminal instances open, then you'll need run <code>source ~/.bashrc</code> in these too, in order for any changes made by the Colcon build process to propagate through to these as well.</p> <p>Step 3: Launch a Waffle Simulation</p> <p>In TERMINAL 1 enter the following command to launch a simulation of a TurtleBot3 Waffle in an empty world:  </p> <p>TERMINAL 1: <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre></p> <p>A Gazebo simulation window should open and within this you should see a TurtleBot3 Waffle in empty space:</p> <p></p> <p>You're all set up and ready to go!</p>"},{"location":"course/assignment1/part2/#velocity","title":"Velocity (Motion)","text":"<p>In Part 1 we learnt about ROS Topics, and about how the <code>teleop_keyboard</code> node could be used to publish messages to a particular topic in order to control the velocity of the robot (and thus change its position).</p> <p>Questions</p> <ol> <li>Which topic is used to control the velocity of the robot?</li> <li>What message type does this topic use?</li> </ol> <p>Return here if you need a reminder on how to find the answers to these questions.</p> <p>We also learnt how to find out more about this particular message type (using the <code>ros2 interface show</code> command): </p> <pre><code>Vector3  linear\n        float64 x\n        float64 y\n        float64 z\nVector3  angular\n        float64 x\n        float64 y\n        float64 z\n</code></pre>"},{"location":"course/assignment1/part2/#velocity-commands","title":"Velocity Commands","text":"<p>When defining velocity commands for a ROS robot, there are six \"fields\" that we can assign values to: two velocity types, each with three velocity components: </p> <p> Velocity Type Component 1 Component 2 Component 3 <code>linear</code> <code>x</code> <code>y</code> <code>z</code> <code>angular</code> <code>x</code> <code>y</code> <code>z</code> <p></p> <p>These relate to a robot's six degrees of freedom, and the topic messages are therefore formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <p> Component (Axis) Linear Velocity Angular Velocity X \"Forwards/Backwards\" \"Roll\" Y \"Left/Right\" \"Pitch\" Z \"Up/Down\" \"Yaw\" <p></p>"},{"location":"course/assignment1/part2/#the-degrees-of-freedom-of-a-waffle","title":"The Degrees of Freedom of a Waffle","text":"<p>The three \"axes\" in the table above are termed the \"Principal Axes.\" In the context of our TurtleBot3 Waffle, these axes and the motion about them are defined as follows:</p> <p></p> <p></p> <p>As discussed above, a mobile robot can have up to six degrees of freedom in total, but this depends upon the robot's design and the actuators it is equipped with. </p> <p>Our TurtleBot3 Waffles only have two motors. These two motors can be controlled independently (in what is known as a \"differential drive\" configuration), which ultimately provides it with a total of two degrees of freedom overall, as illustrated below.</p> <p></p> <p>When issuing Velocity Commands therefore, only two (of the six) velocity command fields are applicable: linear velocity in the x-axis (Forwards/Backwards) and angular velocity about the z-axis (Yaw).</p> <p> Principal Axis Linear Velocity Angular Velocity X \"Forwards/Backwards\" \"Roll\" Y \"Left/Right\" \"Pitch\" Z \"Up/Down\" \"Yaw\" <p></p> <p></p> <p>Maximum Velocity Limits</p> <p>Keep in mind (while we're on the subject of velocity) that our TurtleBot3 Waffles have maximum velocity limits:</p> <p> Velocity Component Upper Limit Units Linear (X) 0.26 m/s Angular (Z) 1.82 rad/s <p></p>"},{"location":"course/assignment1/part2/#odometry","title":"Odometry (Position)","text":""},{"location":"course/assignment1/part2/#odometry-in-action","title":"Odometry In Action","text":"<p>Let's take another look at all the topics that can be used to communicate with our robot:</p> <pre><code>ros2 topic list\n</code></pre> <p>Another topic of interest here is <code>/odom</code>. This topic contains Odometry data, which is also essential for robot navigation, giving us an approximation of a robot's location in its environment.</p> <p>Let's explore this further now, using <code>rqt</code>.</p>"},{"location":"course/assignment1/part2/#ex1","title":"Exercise 1: Exploring Odometry Data","text":"<ol> <li> <p>In TERMINAL 2 launch <code>rqt</code>:</p> <p>TERMINAL 2: <pre><code>rqt\n</code></pre></p> </li> <li> <p>From the top menu select <code>Plugins</code> &gt; <code>Topics</code> &gt; <code>Topic Monitor</code></p> <p>Topic Monitor should then present you with a list of active topics which matches the topic list from the <code>ros2 topic list</code> command that you ran earlier.</p> </li> <li> <p>Check the box next to <code>/odom</code> and click the arrow next to it to expand the topic and reveal four base fields.</p> </li> <li> <p>Expand the <code>pose</code> &gt; <code>pose</code> &gt; <code>position</code> and <code>orientation</code> fields to reveal the data being published to the three position and four orientation values of this message.</p> </li> <li> <p>Also expand the <code>twist</code> &gt; <code>twist</code>, <code>linear</code> and <code>angular</code> fields to reveal the six values being published here too.</p> <p> </p> </li> <li> <p>Next, launch a new terminal instance, we'll call this one TERMINAL 3. Arrange this next to the <code>rqt</code> window, so that you can see them both simultaneously.</p> </li> <li> <p>In TERMINAL 3 launch the <code>teleop_keyboard</code> node as you did in Part 1: </p> <p>TERMINAL 3: <pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre></p> </li> <li> <p>Enter A a couple of times to make the robot rotate on the spot. Observe how the odometry data changes in Topic Monitor.</p> <p>Questions</p> <ol> <li>Which <code>pose</code> fields are changing?</li> <li>Is there anything in the <code>twist</code> part of the message that corresponds to the angular velocity that is being published by the <code>teleop_keyboard</code> node in TERMINAL 3? </li> </ol> </li> <li> <p>Now press the S key to halt the robot, then press W a couple of times to make the robot drive forwards.</p> <p>Questions</p> <ol> <li>Which <code>pose</code> fields are changing now? How does this relate to the position of the robot in the simulated world?</li> <li>How does the <code>twist</code> part of the message now correspond with the linear velocity setting in TERMINAL 3?</li> </ol> </li> <li> <p>Now press D a couple of times and your robot should start to move in a circle.</p> <p>Questions</p> <ol> <li>What linear and angular velocities are you requesting in TERMINAL 3, and how are these represented in the <code>twist</code> part of the <code>/odom</code> message?</li> <li>What about the <code>pose</code> part of the message? How is this data changing as your robot moves in a circular path.</li> <li>What are <code>twist</code> and <code>pose</code> actually telling us?</li> </ol> </li> <li> <p>Press S in TERMINAL 3 to stop the robot (but leave the <code>teleop_keyboard</code> node running).  Then, press Ctrl+C in TERMINAL 2 to close down <code>rqt</code>. </p> </li> <li> <p>Let's look at the Odometry data differently now. With the robot stationary, use <code>ros2 run</code> to run a Python node from the <code>tuos_examples</code> package: </p> <p>TERMINAL 2: <pre><code>ros2 run tuos_examples robot_pose\n</code></pre></p> </li> <li> <p>Now (using the <code>teleop_keyboard</code> node in TERMINAL 3) drive your robot around again, keeping an eye on the outputs that are being printed by the <code>robot_pose</code> node in TERMINAL 2 as you do so.</p> <p>The output of the <code>robot_pose</code> node shows you how the robot's position and orientation (i.e. \"pose\") are changing in real-time as you move the robot around. The <code>\"initial\"</code> column tells us the robot's pose when the node was first launched, and the <code>\"current\"</code> column show us what its pose currently is. The <code>\"delta\"</code> column then shows the difference between the two.</p> <p>Question</p> <p>Which pose parameters haven't changed, and is this what you would expect (considering the robot's principal axes, as illustrated above)?</p> </li> <li> <p>Press Ctrl+C in TERMINAL 2 and TERMINAL 3, to stop the <code>robot_pose</code> and <code>teleop_keyboard</code> nodes. </p> </li> </ol>"},{"location":"course/assignment1/part2/#odometry-explained","title":"Odometry: Explained","text":"<p>Hopefully you have a good idea of what Odometry is now, but let's dig a little deeper using some key ROS command line tools again:</p> <p>TERMINAL 2: <pre><code>ros2 topic info /odom\n</code></pre></p> <p>This provides information about the type of message used by this topic:</p> <pre><code>Type: nav_msgs/msg/Odometry\n</code></pre> <p>We can find out more about this message using the <code>ros2 interface show</code> command:</p> <p>TERMINAL 2: <pre><code>ros2 interface show nav_msgs/msg/Odometry\n</code></pre></p> <p>Look down the far left-hand side to identify the four base fields of the message (i.e. the fields that are not indented):</p> <p></p> <p> # Field Name Field Type 1 <code>header</code> <code>std_msgs/Header</code> 2 <code>child_frame_id</code> <code>string</code> 3 <code>pose</code> <code>geometry_msgs/PoseWithCovariance</code> 4 <code>twist</code> <code>geometry_msgs/TwistWithCovariance</code> <p></p> <p>We saw all these in <code>rqt</code> earlier. As before, its items 3 and 4 that are of most interest to us...</p>"},{"location":"course/assignment1/part2/#pose","title":"Pose","text":"<pre><code># Estimated pose that is typically relative to a fixed world frame.\ngeometry_msgs/PoseWithCovariance pose\n        Pose pose\n                Point position\n                        float64 x\n                        float64 y\n                        float64 z\n                Quaternion orientation\n                        float64 x\n                        float64 y\n                        float64 z\n                        float64 w\n        float64[36] covariance\n</code></pre> <p>As you can see above, there are two key components to Pose:</p> <ol> <li><code>position</code>: Tells us where our robot is located in 3-dimensional space. This is expressed in units of meters.</li> <li><code>orientation</code>: Tells us which way our robot is pointing in its environment. This is expressed in units of Quaternions, which is a mathematically convenient way to store data related to a robot's orientation (it's a bit hard for us humans to understand and visualise this though, so we'll talk about how to convert it to a different format later).</li> </ol> <p>Pose is defined relative to an arbitrary reference point (typically where the robot was when it was turned on), and is determined from:</p> <ul> <li>Data from the Inertial Measurement Unit (IMU) on the OpenCR board</li> <li>Data from both the left and right wheel encoders</li> <li>A kinematic model of the robot</li> </ul> <p>All the above information can then be used to calculate (and keep track of) the distance travelled by the robot from its pre-defined reference point using a process called \"dead-reckoning.\"</p>"},{"location":"course/assignment1/part2/#what-are-quaternions","title":"What are Quaternions?","text":"<p>Quaternions use four values to represent the orientation of something in 3 dimensional space<sup>1</sup>, as we can observe from the structure of the <code>nav_msgs/msg/Odometry</code> ROS message:</p> <pre><code>Quaternion orientation\n        float64 x\n        float64 y\n        float64 z\n        float64 w\n</code></pre> <p>For us, it's easier to think about the orientation of our robot in a \"Euler Angle\" representation, which tell us the degree of rotation about the three principal axes (as discussed above):</p> <ul> <li><code>\u03b8<sub>x</sub></code>, aka: \"Roll\"</li> <li><code>\u03b8<sub>y</sub></code>, aka: \"Pitch\"</li> <li><code>\u03b8<sub>z</sub></code>, aka: \"Yaw\"</li> </ul> <p>Fortunately, the maths involved in converting between these two orientation formats is fairly straight forward (see here).</p> <p>Recall from above however, that our TurtleBot3 can only move in a 2D plane (unfortunately, it can't fly!) and so, actually, its pose can be fully represented by just 3 terms: </p> <ul> <li><code>x</code> &amp; <code>y</code>: the 2D coordinates of the robot in the <code>X-Y</code> plane</li> <li><code>\u03b8<sub>z</sub></code>: the angle of the robot about the <code>z</code> (yaw) axis</li> </ul>"},{"location":"course/assignment1/part2/#twist","title":"Twist","text":"<p>The fourth base field within the <code>nav_msgs/msg/Odometry</code> message is Twist:</p> <pre><code># Estimated linear and angular velocity relative to child_frame_id.\ngeometry_msgs/TwistWithCovariance twist\n        Twist twist\n                Vector3  linear\n                        float64 x\n                        float64 y\n                        float64 z\n                Vector3  angular\n                        float64 x\n                        float64 y\n                        float64 z\n        float64[36] covariance\n</code></pre> <p>This might look familiar from earlier! This tells us the current linear and angular velocities of the robot. These velocities are set by messages published to <code>/cmd_vel</code>, but are then monitored by data coming directly from the robot's wheel encoders, and are provided here as a feedback signal.</p>"},{"location":"course/assignment1/part2/#odometry-data-as-a-feedback-signal","title":"Odometry Data as a Feedback Signal","text":"<p>Odometry data can be really useful for robot navigation, allowing us to keep track of where a robot is, how it's moving and how to get back to where we started. We therefore need to know how to use odometry data effectively within our Python nodes, and we'll explore this now.</p>"},{"location":"course/assignment1/part2/#ex2","title":"Exercise 2: Creating a Python Node to Process Odometry Data","text":"<p>In Part 1 we learnt how to create a package and build simple Python nodes to publish and subscribe to messages on a topic (called <code>/my_topic</code>). In this exercise we'll build a new subscriber node, much like we did previously, but this one will subscribe to the <code>/odom</code> topic that we've been talking about above. We'll also create a new package called <code>part2_navigation</code> for this node to live in!</p> <ol> <li> <p>First, head to the <code>src</code> folder of your ROS2 workspace in your terminal and into the <code>tuos_ros</code> Course Repo:</p> <pre><code>cd ~/ros2_ws/src/tuos_ros/\n</code></pre> </li> <li> <p>Then, use the <code>create_pkg.sh</code> helper script to create your new package:</p> <pre><code>./create_pkg.sh part2_navigation\n</code></pre> </li> <li> <p>Then navigate into the <code>scripts</code> folder of the new package using the <code>cd</code> command again:</p> <pre><code>cd ../part2_navigation/scripts/\n</code></pre> </li> <li> <p>The subscriber that we will build here will be structured in much the same way as the subscriber that we built in Part 1. </p> <p>As a starting point, copy across the <code>subscriber.py</code> file from your <code>part1_pubsub</code> package:</p> <pre><code>cp ~/ros2_ws/src/part1_pubsub/scripts/subscriber.py ./odom_subscriber.py\n</code></pre> </li> <li> <p>Next, follow the steps for converting this into an Odometry subscriber. </p> </li> <li> <p>You'll need to add a new dependency to your package's <code>package.xml</code> file now. Below the <code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;</code> line, add an execution dependency for <code>nav_msgs</code>:</p> package.xml<pre><code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;\n&lt;exec_depend&gt;nav_msgs&lt;/exec_depend&gt;\n</code></pre> </li> <li> <p>Next, declare the <code>odom_subscriber.py</code> node as an executable. Replace <code>minimal_node.py</code> with <code>odom_subscriber.py</code> in the <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/odom_subscriber.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Finally, head back to the terminal and use Colcon to build the package, and the <code>odom_subscriber.py</code> node:</p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-select part2_navigation --symlink-install\n</code></pre> </li> <li> <p>Now we're ready to run this! Do so using <code>ros2 run</code> and see what it does:</p> <pre><code>ros2 run part2_navigation odom_subscriber.py\n</code></pre> </li> <li> <p>Having followed all the steps, the output from your node should be similar to that shown below:</p> <p>[TODO: a gif]</p> </li> <li> <p>Observe how the output (the formatted odometry data) changes whilst you move the robot around using the <code>teleop_keyboard</code> node in a new terminal instance (TERMINAL 3).</p> </li> <li>Stop your <code>odom_subscriber.py</code> node in TERMINAL 2 and the <code>teleop_keyboard</code> node in TERMINAL 3 by entering Ctrl+C in each of the terminals.</li> </ol>"},{"location":"course/assignment1/part2/#basic-navigation-open-loop-velocity-control","title":"Basic Navigation: Open-loop Velocity Control","text":""},{"location":"course/assignment1/part2/#ex3","title":"Exercise 3: Controlling Velocity with the ROS2 CLI","text":"<p>Warning</p> <p>Make sure that you've stopped the <code>teleop_keyboard</code> node before starting this exercise!</p> <p>We can use the <code>ros2 topic pub</code> command to publish data to a topic from a terminal by using the command in the following way:</p> <pre><code>ros2 topic pub {topic_name} {message_type} {message_data}\n</code></pre> <p>As we discovered earlier, the <code>/cmd_vel</code> topic is expecting messages containing linear and angular velocity data, each with an <code>x</code>, <code>y</code> and <code>z</code> component. When publishing topic messages in a terminal the commands can get quite long and complicated, but we can use autocomplete functionality to help us format the full command correctly.</p> <ol> <li> <p>In TERMINAL 3 type the following, using the Tab key where indicated to invoke autocompletion...</p> <ol> <li> <p>First, type the text as shown below and then press the Tab key where indicated to complete the topic name for you:</p> <pre><code>ros2 topic pub /cmd_[TAB]\n</code></pre> </li> <li> <p>Then, type <code>g</code> and then press Tab again to format the rest of the message type for you: </p> <pre><code>ros2 topic pub /cmd_vel g[TAB]\n</code></pre> </li> <li> <p>The message data then needs to be entered inside quotation marks, type <code>\"l</code> and then press Tab again to obtain the format of the message data:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"l[TAB]\n</code></pre> <p>The full command will then be presented:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.0\"\n</code></pre> <p>Tip</p> <p>You can use Tab to autocomplete lots of terminal commands, experiment with it - it'll save you lots of time! </p> </li> </ol> </li> <li> <p>Scroll back through the message using the Left key on your keyboard and then edit the values of the various fields, as appropriate.</p> <p>First, define some values that would make the robot rotate on the spot.  </p> </li> <li> <p>Enter Ctrl+C in TERMINAL 3 to stop the message from being published.</p> <p>What happens to the robot when you stop the <code>ros2 topic pub</code> command?</p> <p>... it keeps on moving at the requested velocity!</p> <p>In order to make the robot actually stop, we need to publish a new message containing alternative velocity commands.</p> </li> <li> <p>In TERMINAL 3 press the Up key on your keyboard to recall the previous command, but don't press Enter just yet! Now press the Left key to track back through the message and change the velocity field values in order to now make the robot stop.</p> </li> <li> <p>Once again, enter Ctrl+C in TERMINAL 3 to stop the publisher from actively publishing new messages, and then follow the same steps as above to compose another new message to now make the robot move in a circle.</p> </li> <li> <p>Enter Ctrl+C to again stop the message from being published, publish a further new message to stop the robot, and then compose (and publish) a message that would make the robot drive in a straight line.</p> </li> <li> <p>Finally, stop the robot again!</p> </li> </ol>"},{"location":"course/assignment1/part2/#ex4","title":"Exercise 4: Creating a Python Node to Make a Robot Move in a circle","text":"<p>Controlling a robot from the terminal (or by using the <code>teleop_keyboard</code> node) is all well and good, but what about if we need to implement some more advanced control or autonomy?</p> <p>We'll now learn how to control the velocity of our robot programmatically, from a Python Node. We'll start out with a simple example to achieve a simple velocity profile (a circle), but this will provide us with the basis on which we can build more complex velocity control algorithms (which we'll look at in the following exercise).</p> <p>In Part 1 we built a simple publisher node, and this one will work in much the same way, but this time however, we need to publish <code>Twist</code> type messages to the <code>/cmd_vel</code> topic instead... </p> <ol> <li> <p>In TERMINAL 2, ensure that you're located within the <code>scripts</code> folder of your <code>part2_navigation</code> package (you could use <code>pwd</code> to check your current working directory).</p> <p>If you aren't located here then navigate to this directory using <code>cd</code>.</p> </li> <li> <p>Create a new file called <code>move_circle.py</code>:</p> <p>TERMINAL 2: <pre><code>touch move_circle.py\n</code></pre> ... and make this file executable using the <code>chmod</code> command (as we did in Part 1).</p> </li> <li> <p>The task is to make the robot move in a circle with a path radius of approximately 0.5 meters.</p> <p>Follow the steps here for building this (using the Part 1 Publisher Node as a starting point). </p> </li> <li> <p>Our <code>move_circle.py</code> node has a new dependency:</p> <pre><code>from geometry_msgs.msg import Twist\n</code></pre> <p>We therefore need to add this dependency to our package's <code>package.xml</code> file.</p> <p>Earlier on we added <code>nav_msgs</code> to this. Below this, add a new <code>&lt;exec_depend&gt;</code> for <code>geometry_msgs</code>:</p> package.xml<pre><code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;\n&lt;exec_depend&gt;nav_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;geometry_msgs&lt;/exec_depend&gt;  &lt;!-- (1)! --&gt;\n</code></pre> <ol> <li>ADD THIS LINE!</li> </ol> </li> <li> <p>Next (hopefully you're getting the idea by now!), declare the <code>move_circle.py</code> node as an executable in the <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/odom_subscriber.py\n  scripts/move_circle.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Finally, head back to the terminal and use Colcon to build the new node alongside everything else in the package:</p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-select part2_navigation --symlink-install\n</code></pre> </li> <li> <p>Run this node now, using <code>ros2 run</code> and see what happens:</p> <pre><code>ros2 run part2_navigation move_circle.py\n</code></pre> <p>Head back to the Gazebo simulation and watch as the robot moves around in a circle of 0.5 meter radius!</p> </li> <li> <p>Once you're done, enter Ctrl+C in TERMINAL 2 to stop the <code>move_circle.py</code> node. Notice what happens to the robot when you do this...</p> <p>Question</p> <p>What does happen to the robot when you hit Ctrl+C to stop the node?</p> <p>Answer: It carries on moving !</p> </li> </ol>"},{"location":"course/assignment1/part2/#ex5","title":"Exercise 5: Implementing a Shutdown Procedure","text":"<p>Clearly, our work on the <code>move_circle.py</code> node isn't quite done. When we terminate our node we'd expect the robot to stop moving, but this (currently) isn't the case. </p> <p>You may have also noticed (with all the nodes that we have created so far) an error traceback in the terminal, every time we hit Ctrl+C. </p> <p>None of this is very good, and we'll address this now by modifying the <code>move_circle.py</code> file to incorporate a proper (and safe) shutdown procedure.</p> <ol> <li> <p>Return to the <code>move_circle.py</code> file in VS Code. </p> </li> <li> <p>First, we need to add an import to our Node:</p> <pre><code>from rclpy.signals import SignalHandlerOptions\n</code></pre> <p>You'll see what this is for shortly...</p> </li> <li> <p>Then move on to the <code>__init__()</code> method of your <code>Circle()</code> class.</p> <p>Add in a boolean flag here called <code>shutdown</code>:</p> <pre><code>self.shutdown = False\n</code></pre> <p>... to begin with, we want this to be set to <code>False</code>.</p> </li> <li> <p>Next, add a new method to your <code>Circle()</code> class, called <code>on_shutdown()</code>:</p> <pre><code>def on_shutdown(self):\n    self.get_logger().info(\n        \"Stopping the robot...\"\n    )\n    self.my_publisher.publish(Twist()) # (1)!\n    self.shutdown = True # (2)!\n</code></pre> <ol> <li>All velocities within the <code>Twist()</code> message class are set to zero by default, so we can just publish this as-is, in order to ask the robot to stop.</li> <li>Set the <code>shutdown</code> flag to true to indicate that a stop message has now been published.</li> </ol> </li> <li> <p>Finally, head to the <code>main()</code> function of the script. This is where most of the changes need to be made...</p> <pre><code>def main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    ) # (1)!\n    move_circle = Circle()\n    try:\n        rclpy.spin(move_circle) # (2)!\n    except KeyboardInterrupt: # (3)!\n        print(\n            f\"{move_circle.get_name()} received a shutdown request (Ctrl+C).\"\n        )\n    finally: \n        move_circle.on_shutdown() # (4)!\n        while not move_circle.shutdown: # (5)!\n            continue\n        move_circle.destroy_node() # (6)!\n        rclpy.shutdown()\n</code></pre> <ol> <li> <p>When initialising <code>rclpy</code>, we're requesting for our <code>move_circle.py</code> node to handle \"signals\" (i.e. events like a Ctrl+C), rather than letting <code>rclpy</code> handle these for us. Here we're using the <code>SignalHandlerOptions</code> object that we imported from <code>rclpy.signals</code> earlier.</p> </li> <li> <p>We set our node to spin inside a Try-Except block now, so that we can catch a <code>KeyboardInterrupt</code> (i.e. a Ctrl+C) and act accordingly when this happens.</p> </li> <li> <p>On detection of the <code>KeyboardInterrupt</code> we print a message to the terminal. After this, the code will move on to the <code>finally</code> block...</p> </li> <li> <p>Call the <code>on_shutdown()</code> method that we defined earlier. This will ensure that a STOP command is published to the robot (via <code>/cmd_vel</code>).</p> </li> <li> <p>This <code>while</code> loop will continue to iterate until our boolean <code>shutdown</code> flag has turned <code>True</code>, to indicate that the STOP message has been published.</p> </li> <li> <p>The rest is the same as before...</p> <p>... destroy the node and then shutdown <code>rclpy</code>.</p> </li> </ol> </li> <li> <p>With all this in place, run the node again now (<code>ros2 run ...</code>).</p> <p>Now, when you hit Ctrl+C you should find that the robot actually stops moving. Ah, much better!</p> </li> </ol>"},{"location":"course/assignment1/part2/#odometry-based-navigation","title":"Odometry-based Navigation","text":"<p>Over the course of the previous two exercises we've created a Python node to make your robot move using open-loop control. To achieve this we published velocity commands to the <code>/cmd_vel</code> topic to make the robot follow a circular motion path.</p> <p>Questions</p> <ol> <li>How do we know if our robot actually achieved the motion path that we asked for?</li> <li>In a real-world environment, what external factors might result in the robot not achieving its desired trajectory?</li> </ol> <p>Earlier on we also learnt about Robot Odometry, which is used by the robot to keep track of its position and orientation (aka Pose) in the environment.  As explained earlier, this is determined by a process called \"dead-reckoning,\" which is only really an approximation, but it's a fairly good one in any case, and we can use this as a feedback signal to understand if our robot is moving in the way that we expect it to.</p> <p>We can therefore build on the techniques that we used in the <code>move_circle.py</code> exercise, and now also build in the ability to subscribe to a topic too and obtain some real-time feedback. To do this, we'll need to subscribe to the <code>/odom</code> topic, and use this to implement some basic closed-loop control.</p>"},{"location":"course/assignment1/part2/#ex6","title":"Exercise 6: Making our Robot Follow a Square Motion Path","text":"<ol> <li> <p>Make sure your <code>move_circle.py</code> node is no longer running in TERMINAL 2, stopping it with Ctrl+C if necessary.</p> </li> <li> <p>Make sure TERMINAL 2 is still located inside your <code>part2_navigation</code> package.</p> </li> <li> <p>Navigate to the package <code>scripts</code> directory and use the Linux <code>touch</code> command to create a new file called <code>move_square.py</code>:</p> <p>TERMINAL 2: <pre><code>touch move_square.py\n</code></pre></p> </li> <li> <p>Then make this file executable using <code>chmod</code>:</p> <p>TERMINAL 2: <pre><code>chmod +x move_square.py\n</code></pre></p> </li> <li> <p>Use the VS Code File Explorer to navigate to this <code>move_square.py</code> file and open it up, ready for editing.</p> </li> <li>There's a template here to help you with this exercise. Copy and paste the template code into your new <code>move_square.py</code> file to get you started. </li> <li> <p>Run the code as it is to see what happens... </p> <p>Fill in the Blank!</p> <p>Something not quite working as expected? Did we forget something very crucial on the very first line of the code template?!</p> </li> <li> <p>Fill in the blank as required and then adapt the code to make your robot follow a square motion path of 1 x 1 meter dimensions.</p> </li> </ol> <p>After following a square motion path a few times, your robot should return to the same location that it started from.</p> <p>Advanced feature</p> <p>Adapt the node to make the robot automatically stop once it has performed two complete loops.</p>"},{"location":"course/assignment1/part2/#wrapping-up","title":"Wrapping Up","text":"<p>In this session we've learnt how to control the velocity and position of a robot from both the command-line (using ROS command-line tools) and from ROS Nodes by publishing correctly formatted messages to the <code>/cmd_vel</code> topic.  </p> <p>We've also learnt about Odometry, which is published by our robot to the <code>/odom</code> topic.  The odometry data tells us the current linear and angular velocities of our robot in relation to its 3 principal axes.  In addition to this though, it also tells us where in physical space our robot is located and oriented, which is determined based on dead-reckoning. </p> <p>Questions</p> <ol> <li>What information (sensor/actuator data) is used to do this?</li> <li>Do you see any potential limitations of this?</li> </ol> <p>Consider reading Chapter 11.1.3 (\"Pose of Robot\") in the ROS Robot Programming eBook that we mentioned here.</p> <p>In the final exercise we explored the development of odometry-based control to make a robot follow a square motion path. You will likely have observed some degree of error in this which could be due to the fact that Odometry data is determined by dead-reckoning and is therefore subject to drift and accumulated error. Consider how other factors may impact the accuracy of control too.</p> <p>Questions</p> <ol> <li>How might the rate at which the odometry data is sampled play a role?</li> <li>How quickly can your robot receive new velocity commands, and how quickly can it respond?</li> </ol> <p>Be aware that we did all this in simulation here too. In fact, in a real world environment, this type of navigation might be less effective, since things such as measurement noise and calibration errors can also have considerable impact. You will have the opportunity to experience this first hand in the labs.</p> <p>Ultimately then, we've seen a requirement here for additional information to provide more confidence of a robot's location in its environment, in order to enhance its ability to navigate effectively and avoid crashing into things! We'll explore this further later in this course.</p>"},{"location":"course/assignment1/part2/#backup","title":"WSL-ROS Managed Desktop Users: Save your work!","text":"<p>Remember, the work you have done in the WSL-ROS environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University <code>U:\\</code> Drive, allowing you to restore it on another managed desktop machine the next time you fire up WSL-ROS.  </p> <ol> <li> <p>Quaternions are explained very nicely here, if you'd like to learn more.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment1/part3/","title":"Part 3: Beyond the Basics","text":"<p>This page isn't finished yet, sorry. </p>"},{"location":"course/assignment1/part3/#introduction","title":"Introduction","text":""},{"location":"course/assignment1/part3/#aims","title":"Aims","text":""},{"location":"course/assignment1/part3/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":""},{"location":"course/assignment1/part3/#quick-links","title":"Quick Links","text":""},{"location":"course/assignment1/part3/#additional-resources","title":"Additional Resources","text":""},{"location":"course/assignment1/part3/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>If you haven't done so already, launch your ROS environment now:</p> <ol> <li>Using WSL-ROS on a university managed desktop machine: follow the instructions here to launch it (TODO).</li> <li>Running WSL-ROS on your own machine (TODO): launch the Windows Terminal to access a WSL-ROS terminal instance.</li> <li>Other Users: Launch a terminal instance with access to your local ROS installation.</li> </ol> <p>You should now have access to a Linux terminal instance, and we'll refer to this terminal instance as TERMINAL 1.</p> <p>Step 2: Restore your work (WSL-ROS Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS Environment will not be preserved between sessions or across different University computers. At the end of Part 2 you should have run the <code>wsl_ros</code> tool to back up your home directory to your University <code>U:\\</code> Drive. Once WSL-ROS is up and running, you should be prompted to restore this:</p> <p>TODO</p> <p>Enter <code>Y</code> to restore your work from last time. You can also restore your work at any time using the following command:</p> <pre><code>wsl_ros restore\n</code></pre> <p>Step 3: Launch VS Code </p> <p>It's also worth launching VS Code now, so that it's ready to go for when you need it later on. </p> <p>WSL Users...</p> <p>It's important to launch VS Code within your ROS environment using the \"WSL\" extension. Always remember to check for this: FIG TODO</p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>In Part 1 you should have downloaded and installed The Course Repo into your ROS environment. Hopefully you've done this by now, but if you haven't then go back and do it now (you'll need it for some exercises here). If you have already done it, then (once again) it's worth just making sure it's all up-to-date, so run the following command now to do so:</p> <p>TERMINAL 1: <pre><code>cd ~/ros2_ws/src/tuos_ros/ &amp;&amp; git pull\n</code></pre></p> <p>Then run <code>colcon build</code> </p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-up-to tuos_ros\n</code></pre> <p>And finally, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Remember</p> <p>If you have any other terminal instances open, then you'll need run <code>source ~/.bashrc</code> in these too, in order for the changes to propagate through to these as well!</p>"},{"location":"course/assignment1/part3/#launch-files","title":"Launch Files","text":"<p>So far (in Parts 1 &amp; 2) we've used the <code>ros2 run</code> command to execute a variety of ROS nodes, such as <code>teleop_keyboard</code>, as well as a number of nodes that we've created of our own. You may also have noticed that we've used a <code>ros2 launch</code> command now and again too, mainly to launch Gazebo Simulations of our robot, but why do we have these two commands, and what's the difference between them?</p> <p>Complex ROS applications typically require the execution of multiple nodes at the same time. The <code>ros2 run</code> command only allows us to execute a single node, and so this isn't that convenient for such complex applications, where we'd have to open multiple terminals, use <code>ros2 run</code> multiple times and make sure that we ran everything in the correct order without forgetting anything! <code>ros2 launch</code>, on the other hand, provides a means to launch multiple ROS nodes simultaneously by defining exactly what we want to launch within launch files. This makes the execution of complex applications more reliable, repeatable and easier for others to launch these applications correctly. </p>"},{"location":"course/assignment1/part3/#ex1","title":"Exercise 1: Creating a Launch File","text":"<p>In order to see how launch files work, let's create some of our own!</p> <p>In Part 1 we created <code>publisher.py</code> and <code>subscriber.py</code> nodes that could talk to one another via a topic called <code>/my_topic</code>. We launched these independently using the <code>ros2 run</code> command in two separate terminals. Wouldn't it be nice if we could have launched them both at the same time, from the same terminal instead?</p> <p>To start with, let's create another new package, this time called <code>part3_beyond_basics</code>. </p> <ol> <li> <p>In TERMINAL 1...</p> <ol> <li> <p>Head to the <code>src</code> folder of your ROS workspace, and into the <code>tuos_ros</code> Course Repo from there:</p> <p>TERMINAL 1: <pre><code>cd ~/ros2_ws/src/tuos_ros/\n</code></pre></p> </li> <li> <p>Use the <code>create_pkg.sh</code> helper script to create a new package once again:</p> <pre><code>./create_pkg.sh part3_beyond_basics\n</code></pre> </li> <li> <p>And navigate into the root of this new package, using <code>cd</code>:</p> <pre><code>cd ../part3_beyond_basics/\n</code></pre> </li> </ol> </li> <li> <p>Launch files should be located in a <code>launch</code> directory at the root of the package directory, so use <code>mkdir</code> to do this:</p> <p>TERMINAL 1: <pre><code>mkdir launch\n</code></pre></p> </li> <li> <p>Use the <code>cd</code> command to enter the <code>launch</code> folder that you just created, then use the <code>touch</code> command to create a new empty file called <code>pubsub.launch.py</code>.</p> <p>TERMINAL 1: <pre><code>cd launch &amp;&amp; touch pubsub.launch.py\n</code></pre></p> </li> <li> <p>Open this launch file in VS Code and enter the following:</p> pubsub.launch.py<pre><code>from launch import LaunchDescription # (1)!\nfrom launch_ros.actions import Node # (2)!\n\ndef generate_launch_description(): # (3)!\n    return LaunchDescription([ # (4)!\n        Node( # (5)!\n            package='part1_pubsub', # (6)!\n            executable='publisher.py', # (7)!\n            name='my_publisher' # (8)!\n        )\n    ])\n</code></pre> <ol> <li>Everything that we want to execute with a launch file must be encapsulated within a <code>LaunchDescription</code>, which is imported here from the <code>launch</code> module.</li> <li>In order to execute a node from a launch file we need to define it using the <code>Node</code> class from <code>launch_ros.actions</code> (not to be confused with the ROS Action communication method covered in Part 5!)</li> <li>We encapsulate a Launch Description inside a <code>generate_launch_description()</code> function.</li> <li>Here we define everything that we want this launch file to execute: in this case a Python list <code>[]</code> containing a single <code>Node()</code> item (for now).</li> <li>Here we describe the node that we want to launch.</li> <li>The name of the package that the node is part of.</li> <li>The name of the actual node that we want to launch from the above package.</li> <li> <p>A name to register this node as on the ROS network. While this is also defined in the node itself: </p> <pre><code>super().__init__(\"simple_publisher\")\n</code></pre> <p>...we can override this here with something else. </p> </li> </ol> </li> <li> <p>We need to make sure we tell <code>colcon</code> about our new <code>launch</code> directory, so that it can build the launch files within it when we run <code>colcon build</code>. To do this, we need to add a directory install instruction to our package's <code>CMakeLists.txt</code>:</p> <p>Open up the <code>CMakeLists.txt</code> file and add the following text just above the <code>ament_package()</code> line at the very bottom:</p> part3_beyond_basics/CMakeLists.txt<pre><code>install(DIRECTORY\n  launch\n  DESTINATION share/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Now, let's build the package... </p> <ol> <li> <p>Navigate back to the root of the ROS workspace:</p> <p>TERMINAL 1: <pre><code>cd ~/ros2_ws/\n</code></pre></p> </li> <li> <p>Run <code>colcon build</code> on your new package only:</p> <pre><code>colcon build --packages-select part3_beyond_basics --symlink-install\n</code></pre> </li> <li> <p>And finally, re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Use <code>ros2 launch</code> to launch this file and test it out as it is:</p> <p>TERMINAL 1: <pre><code>ros2 launch part3_beyond_basics pubsub.launch.py\n</code></pre></p> </li> <li> <p>The code that we've given you above will launch the <code>publisher.py</code> node from the <code>part1_pubsub</code> package, but not the <code>subscriber.py</code> node.  We therefore need to add another <code>Node()</code> object to our <code>LaunchDescription</code>:</p> pubsub.launch.py<pre><code>from launch import LaunchDescription \nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='part1_pubsub',\n            executable='publisher.py',\n            name='my_publisher'\n        ),\n        Node(\n            # TODO: describe the subscriber.py node...\n        )\n    ])\n</code></pre> <p>Using the same methods as above, add the necessary definitions for the <code>subscriber.py</code> node into your launch file.</p> </li> <li> <p>Once you've made these changes you'll need to run <code>colcon build</code> again.</p> <p>Warning</p> <p>You'll need to run <code>colcon build</code> every time you make changes to a launch file, even if you use the <code>--symlink-install</code> option (as this only applies to nodes in the <code>scripts</code> directory)</p> </li> <li> <p>Once you've completed this, it should be possible to launch both the publisher and subscriber nodes with <code>ros2 launch</code> and the <code>pubsub.launch.py</code> file. Verify this in TERMINAL 1 by executing the launch file. Soon after launching this, you should see the following messages to indicate that both nodes are alive:</p> <pre><code>[subscriber.py-2] [INFO] [###] [my_subscriber]: The 'my_subscriber' node is initialised.\n[publisher.py-1] [INFO] [###] [my_publisher]: The 'my_publisher' node is initialised.\n</code></pre> <p>... and following this, the outputs of both nodes should be printed to the screen continually:</p> <pre><code>[publisher.py-1] [INFO] [###] [my_publisher]: Publishing: 'The ROS time is 1737545960 (seconds).'\n[subscriber.py-2] [INFO] [###] [my_subscriber]: The 'my_subscriber' node heard:\n[subscriber.py-2] [INFO] [###] [my_subscriber]: 'The ROS time is 1737545960 (seconds).' \n</code></pre> </li> <li> <p>We can further verify this in a new terminal (TERMINAL 2), using commands that we've use in Parts 1 &amp; 2 to list all nodes and topics that are active on our ROS network:</p> <p>TERMINAL 2:</p> <p><pre><code>ros2 node list\n</code></pre> <pre><code>ros2 topic list\n</code></pre></p> <p>Do you see what you'd expect to see in the output of these two commands?</p> </li> </ol>"},{"location":"course/assignment1/part3/#ex2","title":"Exercise 2: Launching Another Launch File","text":"<p>Using the processes above, we can develop launch files to execute as many nodes as we want on a ROS network simultaneously. Another thing we can do with launch files is launch other launch files! </p> <p>To illustrate this, think back to the <code>move_circle.py</code> node that we developed in Part 2, as part of our <code>part2_navigation</code> package. In order to launch this node we must first launch a robot simulation, e.g.: </p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>In this exercise we'll look at how we can launch the above launch file and our <code>move_circle.py</code> node simultaneously from a single <code>ros2 launch</code> command...</p> <ol> <li> <p>Make sure you're in the <code>launch</code> directory of your <code>part3_beyond_basics</code> package, we can actually use a <code>colcon</code> command to get us to the root of the package directory:</p> <p>TERMINAL 1: <pre><code>colcon_cd part3_beyond_basics\n</code></pre></p> <p>... and <code>cd</code> to get us into the <code>launch</code> directory from there:</p> <pre><code>cd launch/\n</code></pre> </li> <li> <p>Make a new launch file in here, called <code>circle.launch.py</code>:</p> <p>TERMINAL 1: <pre><code>touch circle.launch.py\n</code></pre></p> </li> <li> <p>Open this up in VS Code and enter the following:</p> circle.launch.py<pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\nimport os\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    return LaunchDescription([\n        IncludeLaunchDescription( # (1)!\n            PythonLaunchDescriptionSource( # (2)!\n                os.path.join( # (3)!\n                    get_package_share_directory(\"turtlebot3_gazebo\"), \n                    \"launch\", \"empty_world.launch.py\" # (4)!\n                )\n            )\n        )\n    ])\n</code></pre> <ol> <li>To include another launch file in a launch description, we use a <code>IncludeLaunchDescription()</code> class instance (imported from a module called <code>launch.actions</code>).</li> <li> <p>We want to launch the \"Empty World\" simulation from the <code>turtlebot3_gazebo</code> package, which (as we know) can be done from a terminal with the following command:</p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>Based on the above, we know that the launch file itself is a Python launch file, due to the <code>.py</code> file extension at the end.</p> <p>As such, the launch description that we want to include is a Python launch description, which must therefore be defined using a <code>PythonLaunchDescriptionSource()</code> instance (imported from a module called <code>launch.launch_description_sources</code>)</p> </li> <li> <p>The <code>os.path.join()</code> method (from the standard Python <code>os</code> library) can be used to build file paths. </p> </li> <li> <p>The Python Launch Description Source is defined by providing the full path to the launch file that we want to include. We don't necessarily know where this file is on our filesystem, but ROS does!</p> <p>We can therefore use a function called <code>get_package_share_directory()</code> (from a module called <code>ament_index_python.packages</code>) to provide us with the path to the root of this package directory.</p> <p>From there, we know that the launch file itself must exist in a <code>launch</code> directory, so we use the <code>os.path.join()</code> method to construct this full file path for us.</p> </li> </ol> <p>... Currently, the launch file above contains only the code necessary to include the <code>empty_world.launch.py</code> launch file into our <code>circle.launch.py</code> launch description. There's a few new things that have been introduced here to achieve this, so click on the  icons in the code above to find out what all these things are doing.</p> </li> <li> <p>Now, add a <code>Node()</code> item to the launch description so that the <code>move_circle.py</code> node (from your <code>part2_navigation</code> package) is launched after the \"Empty World\" simulation has been launched.</p> <p>Refer back to Exercise 1 for a reminder on how to do this.</p> </li> <li> <p>When you're ready, remember to run <code>colcon build</code> again before attempting to execute your new <code>circle.launch.py</code> launch file:</p> <p>TERMINAL 1: <pre><code>ros2 launch part3_beyond_basics circle.launch.py\n</code></pre></p> </li> </ol> <p>If you've done this successfully, on launching the above command the Gazebo Empty World simulation should launch and, once it's loaded up, the robot should instantly start moving around in a circle (while printing information to TERMINAL 1 at the same time).</p> <p>We've learnt some key launch file techniques now, so let's move on to another more advanced and very important topic...</p>"},{"location":"course/assignment1/part3/#lidar","title":"Laser Displacement Data and The LiDAR Sensor","text":"<p>As you'll know from Part 2, odometry is really important for robot navigation, but it can be subject to drift and accumulated error over time. You may have observed this in simulation during Part 2 Exercise 5, and you would most certainly notice it if you were to do the same on a real robot. Fortunately, The Waffles have another sensor on-board which provides even richer information about the environment, and we can use this to supplement the odometry information and enhance the robot's navigation capabilities.</p>"},{"location":"course/assignment1/part3/#introducing-the-laserscan-interface","title":"Introducing the LaserScan Interface","text":""},{"location":"course/assignment1/part3/#ex3","title":"Exercise 3: Using RViz to Visualise LaserScan Data","text":"<p>We're now going to place the robot in a more interesting environment than the \"empty world\" that we've been working with so far...</p> <ol> <li> <p>In TERMINAL 1 enter the following command to launch this:</p> <p>TERMINAL 1: <pre><code>ros2 launch turtlebot3_gazebo turtlebot3_world.launch.py\n</code></pre></p> <p>A Gazebo simulation should now be launched with a TurtleBot3 Waffle in a new environment:</p> <p> </p> </li> <li> <p>Open a new terminal instance (TERMINAL 2) and enter the following:</p> <p>TERMINAL 2: <pre><code>ros2 launch tuos_simulations rviz.launch.py\n</code></pre></p> <p>On running the command a new window should open:</p> <p> </p> <p>This is RViz, which is a ROS tool that allows us to visualise the data being measured by a robot in real-time. </p> <p>The green dots scattered around the robot represent laser displacement data which is measured by the LiDAR sensor located on the top of the robot, allowing it to measure the distance to any obstacles in its surroundings. </p> <p>The LiDAR sensor spins continuously, sending out laser pulses as it does so. These laser pulses are reflected from any objects and sent back to the sensor. Distance can then be determined based on the time it takes for the pulses to complete the full journey (from the sensor, to the object, and back again), by a process called \"time of flight\". Because the LiDAR sensor spins and performs this process continuously, a full 360\u00b0 scan of the environment can be generated.</p> <p>In this case (because we are working in simulation here) the data represents the objects surrounding the robot in its simulated environment, so you should notice that the green dots produce an outline that resembles the objects in the world that is being simulated in Gazebo (or partially at least).</p> </li> <li> <p>Next, open up a new terminal instance (TERMINAL 3). Laser displacement data from the LiDAR sensor is published by the robot to the <code>/scan</code> topic. We can use the <code>ros2 topic info</code> command to find out more about the nodes that are publishing and subscribing to this topic, as well as the type of interface used to transmit this topic data:</p> <p>TERMINAL 3: <pre><code>ros2 topic info /scan\n</code></pre> <pre><code>Type: sensor_msgs/msg/LaserScan\nPublisher count: 1\nSubscription count: 0\n</code></pre></p> </li> <li> <p>As we can see from above, <code>/scan</code> data is of the <code>sensor_msgs/msg/LaserScan</code> type, and we can find out more about this interface using the <code>ros2 interface show</code> command:</p> <p>TERMINAL 3: <pre><code>ros2 interface show sensor_msgs/msg/LaserScan\n</code></pre> <pre><code># Single scan from a planar laser range-finder\n\nstd_msgs/Header header # timestamp in the header is the acquisition time of\n        builtin_interfaces/Time stamp\n                int32 sec\n                uint32 nanosec\n        string frame_id\n                             # the first ray in the scan.\n                             #\n                             # in frame frame_id, angles are measured around\n                             # the positive Z axis (counterclockwise, if Z is up)\n                             # with zero angle being forward along the x axis\n\nfloat32 angle_min            # start angle of the scan [rad]\nfloat32 angle_max            # end angle of the scan [rad]\nfloat32 angle_increment      # angular distance between measurements [rad]\n\nfloat32 time_increment       # time between measurements [seconds] - if your scanner\n                             # is moving, this will be used in interpolating position\n                             # of 3d points\nfloat32 scan_time            # time between scans [seconds]\n\nfloat32 range_min            # minimum range value [m]\nfloat32 range_max            # maximum range value [m]\n\nfloat32[] ranges             # range data [m]\n                             # (Note: values &lt; range_min or &gt; range_max should be discarded)\nfloat32[] intensities        # intensity data [device-specific units].  If your\n                             # device does not provide intensities, please leave\n                             # the array empty.\n</code></pre></p> </li> </ol>"},{"location":"course/assignment1/part3/#interpreting-laserscan-data","title":"Interpreting LaserScan Data","text":"<p>The <code>LaserScan</code> interface is a standardised ROS message interface (from the <code>sensor_msgs</code> package) that any ROS Robot can use to publish data that it obtains from a Laser Displacement Sensor such as the LiDAR on the TurtleBot3.  </p> <p><code>ranges</code> is an array of <code>float32</code> values (array data-types are suffixed with <code>[]</code>). This is the part of the message containing all the actual distance measurements that are being obtained by the LiDAR sensor (in meters).</p> <p>Consider a simplified example here, taken from a TurtleBot3 robot in a different environment:</p> <p></p> <p>As illustrated in the figure, we can associate each data-point of the <code>ranges</code> array to an angular position by using the <code>angle_min</code>, <code>angle_max</code> and <code>angle_increment</code> values that are also provided within the <code>LaserScan</code> message.  We can use the <code>ros2 topic echo</code> command to find out what their values are:</p> <p><pre><code>$ ros2 topic echo /scan --field angle_min --once\n0.0\n---\n</code></pre> <pre><code>$ ros2 topic echo /scan --field angle_max --once\n6.28000020980835\n---\n</code></pre> <pre><code>$ ros2 topic echo /scan --field angle_increment --once\n0.01749303564429283\n---\n</code></pre></p> <p>Question</p> <p>What do these values represent? (Compare them with the figure above)</p> <p>Tip</p> <p>Notice how we were able to access specific variables within the <code>/scan</code> data using the <code>--field</code> flag, and ask the command to only provide us with a single message by using <code>--once</code>?</p> <p>The <code>ranges</code> array contains 360 values in total, i.e. a distance measurement at every 1\u00b0 (an <code>angle_increment</code> of 0.0175 radians) around the robot. The first value in the <code>ranges</code> array (<code>ranges[0]</code>) is the distance to the nearest object directly in front of the robot (i.e. at \u03b8 = 0 radians, or <code>angle_min</code>). The last value in the <code>ranges</code> array (<code>ranges[359]</code>) is the distance to the nearest object at 359\u00b0 (i.e. \u03b8 = 6.283 radians, or <code>angle_max</code>) from the front of the robot, i.e.: 1 degree to the right of the X-axis. <code>ranges[65]</code>, for example, would represent the distance to the closest object at an angle of 65\u00b0 (1.138 radians) from the front of the robot (anti-clockwise), as shown in the figure.</p> <p>The <code>LaserScan</code> message also contains the parameters <code>range_min</code> and <code>range_max</code>, which represent the minimum and maximum distances (again, in meters) that the LiDAR sensor can detect, respectively. Use the <code>ros2 topic echo</code> command to report these directly too.  </p> <p>Questions</p> <ol> <li>What is the maximum and minimum range of the LiDAR sensor? Use the same technique as we used above to find out.</li> <li> <p>Consider the note against <code>ranges</code> in the <code>ros2 interface show</code> output earlier:</p> <pre><code>float32[] ranges    # range data [m]\n                    # (Note: values &lt; range_min or &gt; range_max should be discarded)\n</code></pre> <p>(this might be worth thinking about).</p> </li> </ol> <p>Finally, use the <code>ros2 topic echo</code> command again to display the <code>ranges</code> portion of the <code>LaserScan</code> data. There's a lot of data here (360 data points per message in fact, as you know from above!):</p> <pre><code>ros2 topic echo /scan --field ranges\n</code></pre> <p>We're dropping the <code>--once</code> option now, so that we can see the data as it comes in, in real-time.  You might need to expand the terminal window so that you can see all the data points; data will be bound by square brackets <code>[]</code>, and there should be a <code>---</code> at the end of each message too, to help you confirm that you are viewing the whole thing.</p> <p>The main thing you'll notice here is that there's lots of information, and it changes rapidly! As you have already seen though, it is the numbers that are flying by here that are represented by red dots in RViz.  Head back to the RViz screen to have another look at this now. As you'll no doubt agree, this is a much more useful way to visualise the <code>ranges</code> data, and illustrates how useful RViz can be for interpreting what your robot can see in real-time.</p> <p>What you may also notice is several <code>inf</code> values scattered around the array.  These represent sensor readings that are outside the sensor's measurement range (i.e. greater than <code>range_max</code> or less than <code>range_min</code>), so the sensor can't report a distance measurement in such cases. Remember from above: </p> <pre><code>(Note: values &lt; range_min or &gt; range_max should be discarded)\n</code></pre> <p>Note</p> <p>This behaviour is different on the real robots! Be aware of this when developing code for real robots!!</p> <p>Stop the <code>ros2 topic echo</code> command from running in the terminal window by entering Ctrl+C in TERMINAL 3. Also close down the RViz process running in TERMINAL 2 now as well, but leave the simulation (in TERMINAL 1 running). </p>"},{"location":"course/assignment1/part3/#ex4","title":"Exercise 4: Building a Basic LaserScan Callback Function","text":"<p>LaserScan data presents us with a new challenge: processing large datasets. In this exercise we'll look at some basic approaches that can be taken to deal with this data, and get something meaningful out of it that can be used in your robot applications.</p> <ol> <li> <p>Navigate into the <code>scripts</code> folder of your <code>part3_beyond_basics</code> package:</p> <p>TERMINAL 2: <pre><code>colcon_cd part3_beyond_basics &amp;&amp; cd scripts/\n</code></pre></p> </li> <li> <p>Create a new node called <code>lidar_subscriber.py</code>:</p> <p>TERMINAL 2: <pre><code>touch lidar_subscriber.py\n</code></pre></p> <p>and give this execute permissions:</p> <pre><code>chmod +x lidar_subscriber.py\n</code></pre> </li> <li> <p>Declare this as a package executable by opening up your package's <code>CMakeLists.txt</code> in VS Code, and replacing <code>minimal_node.py</code> with <code>lidar_subscriber.py</code> as shown below:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/lidar_subscriber.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Then, add some new dependencies to your package's <code>package.xml</code> file, so open this up in VS Code too. Below the <code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;</code> line, add the following:</p> package.xml<pre><code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;\n&lt;exec_depend&gt;sensor_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;python3-numpy&lt;/exec_depend&gt;\n</code></pre> </li> <li> <p>Head back to the terminal and use Colcon to build this (even though <code>lidar_subscriber.py</code> is still just an empty file at this stage):</p> <p>TERMINAL 2: <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-select part3_beyond_basics --symlink-install\n</code></pre></p> <p>And after that, re-source your <code>.bashrc</code>:</p> <pre><code>source /.bashrc\n</code></pre> </li> <li> <p>With all of that out of the way, it's time to start building the <code>lidar_subscriber.py</code> Python Node! Open up the file in VS Code, then follow the steps here to construct it. </p> </li> <li> <p>Once you're happy with what's going on with this, run the node using <code>ros2 run</code>:</p> <p>TERMINAL 2: <pre><code>ros2 run part3_beyond_basics lidar_subscriber.py\n</code></pre></p> </li> <li> <p>Open another terminal (so you can still see the outputs from your <code>lidar_subscriber.py</code> node). Launch the <code>teleop_keyboard</code> node, and drive the robot around, noting how the output from your <code>lidar_subscriber.py</code> node changes as you do so.</p> </li> <li> <p>Close everything down now (including the simulation running in TERMINAL 1). Then launch the \"empty world\" simulation again:</p> <p>TERMINAL 1: <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre></p> </li> <li> <p>Go back to TERMINAL 2 and launch your <code>lidar_subscriber.py</code> node again:</p> <p>TERMINAL 2: <pre><code>ros2 run part3_beyond_basics lidar_subscriber.py\n</code></pre></p> <p>What output do you see from this now?</p> </li> </ol>"},{"location":"course/assignment1/part3/#ex5","title":"Exercise 5: Enhancing the LaserScan Callback","text":"<p>In the previous exercise we performed some processing on the LiDAR <code>ranges</code> array, specifically:</p> <ol> <li>Grab 40 data points to represent all distance readings from a 40\u00b0 arc ahead of the robot</li> <li>Discard any \"out-of-range\" values (<code>inf</code>)</li> <li>Return the average distance value from the data points that remain.</li> </ol> <p>As such, we've condensed a LiDAR array subset of 40 data points into a single point to represent (approximately) how far away an object is ahead of our robot. </p> <p>We could take this approach with other subsets of the <code>ranges</code> array too, to observe what's happening in different points around the robot, while still maintaining a manageable number of data points to monitor overall.</p> <p>Consider the following ...</p> <p>We can modify this \"Empty World\" environment with some basic objects, and we can move these around to further investigate how the <code>lidar_subscriber.py</code> outputs change under different conditions. </p> <p>In the Gazebo simulation window, use the \"Box\" tool in the top toolbar to place a box in front of the robot:</p> <p>Use the \"Scale Mode\" button to resize the box and use the \"Translation Mode\" button to reposition it.</p> <p>Once you are happy with this, right-click on the object and select \"Delete\" to remove it from the world. </p>"},{"location":"course/assignment1/part4/","title":"Part 4: Services","text":"<p>This page is currently unavailable.</p>"},{"location":"course/assignment1/part5/","title":"Part 5: Actions","text":""},{"location":"course/assignment1/part5/#introduction","title":"Introduction","text":"<p> Exercises: 6 (5 core, 1 advanced) Estimated Completion Time: 3 hours (core exercises only)</p>"},{"location":"course/assignment1/part5/#aims","title":"Aims","text":"<p>In this part of the course we'll learn about a third method of communication available in ROS: Actions.  Actions are essentially an advanced version of Services, and we'll look at exactly how these two differ and why you might choose to employ an action over a service for certain tasks. </p>"},{"location":"course/assignment1/part5/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Recognise how ROS Actions differ from ROS Services and explain where this method might be useful in robot applications.</li> <li>Explain the structure of Action messages and identify the relevant information within them, enabling you to build Action Servers and Clients.</li> <li>Implement Python Action Client nodes that utilise concurrency and preemption.</li> <li>Develop Action Server &amp; Client nodes that could be used as the basis for a robotic search strategy.</li> </ol>"},{"location":"course/assignment1/part5/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Launching an Action Server and calling it from the command-line</li> <li>Exercise 2: Building a Python Action Client Node</li> <li>Exercise 3: Creating an Action Interface</li> <li>Exercise 4: Building the \"ExploreForward\" Action Server</li> <li>Exercise 5: Building a Basic \"ExploreForward\" Client</li> <li>Exercise 6 (Advanced): Implementing an Exploration Strategy</li> </ul>"},{"location":"course/assignment1/part5/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>Launch your ROS environment now so that you have access to a Linux terminal instance (aka TERMINAL 1).</p> <p>Step 2: Restore your work (WSL-ROS Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS Environment will not be preserved between sessions or across different University computers, and so you should be backing up your work to your <code>U:\\</code> drive regularly. When prompted (on first launch of WSL-ROS in TERMINAL 1) enter <code>Y</code> to restore this<sup>1</sup>.</p> <p>Step 3: Launch VS Code </p> <p>WSL users remember to check for this (TODO). </p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Check that the Course Repo is up-to-date before you start on these exercises. See here for how to install and/or update (TODO). </p>"},{"location":"course/assignment1/part5/#calling-an-action-server","title":"Calling an Action Server","text":"<p>Before we talk about what actions actually are, we're going to dive straight in and see one in action (excuse the pun). </p>"},{"location":"course/assignment1/part5/#ex1","title":"Exercise 1: Launching an Action Server and calling it from the command-line","text":"<p>We'll play a little game here. We're going to launch our TurtleBot3 Waffle in a mystery environment now, and we're going to do this by launching Gazebo headless i.e. Gazebo will be running behind the scenes, but there'll be no Graphical User Interface (GUI) to show us what the environment actually looks like.  Then, we'll use an action server to make our robot scan the environment and take pictures for us, to reveal its surroundings!</p> <ol> <li> <p>To launch the TurtleBot3 Waffle in this mystery environment, use the following <code>ros2 launch</code> command:</p> <p>TERMINAL 1: <pre><code>ros2 launch tuos_simulations mystery_world.launch.py\n</code></pre></p> <p>Messages in the terminal should indicate that something has happened, but that's about all you will see!</p> </li> <li> <p>Next, open up a new terminal (TERMINAL 2), and have a look at all the topics that are currently active on the ROS network (you should know exactly how to do this by now!)</p> <p>The output of this should confirm that ROS and our robot are indeed active...</p> How? <p>When the robot is active, the output of the <code>ros2 topic list</code> command should provide a long list of topics, a number of which we've been working with throughout this course so far, such as <code>cmd_vel</code>, <code>odom</code>, <code>scan</code>, and so on. If the Waffle simulation isn't active then we would be presented with a much smaller list, containing only the core ROS topics:</p> <p>TERMINAL 2: <pre><code>$ ros2 topic list\n/parameter_events\n/rosout\n</code></pre></p> </li> <li> <p>Next, run the following command to launch an Action Server on the network:</p> <p>TERMINAL 2: <pre><code>ros2 run tuos_examples camera_sweep_action_server.py\n</code></pre></p> </li> <li> <p>Now, open up another new terminal instance (TERMINAL 3), but so that you can view this and TERMINAL 2 side-by-side. Enter the following command to list all actions that are active on the ROS network:</p> <p>TERMINAL 3: <pre><code>ros2 action list\n</code></pre></p> <p>There should be an item here called <code>/camera_sweep</code>, use the <code>info</code> command to find out more about this:</p> <pre><code>ros2 action info /camera_sweep\n</code></pre> <p>This tells us the name of the action: <code>Action: /camera_sweep</code>, as well as the number of client and server nodes this action has. Currently, the action should have 0 clients and 1 server, and the node acting as the server here should be listed as <code>/camera_sweep_action_server_node</code> (the node that we just launched with the <code>ros2 run</code> command in TERMINAL 2).</p> <p>Finally, call the <code>ros2 action info</code> command again, but this time providing an additional argument:</p> <pre><code>ros2 action info -t /camera_sweep\n</code></pre> <pre><code>Action: /camera_sweep\nAction clients: 0\nAction servers: 1\n    /camera_sweep_action_server [tuos_interfaces/action/CameraSweep]\n</code></pre> <p>The <code>-t</code> argument additionally shows the action type against the server node, indicating to us the type of interface used by the server.</p> </li> <li> <p>Let's now find out more about the interface itself. As with any interface (message, service or action) we can use the <code>ros2 interface</code> command to do this.</p> <p>TERMINAL 3: <pre><code>ros2 interface show tuos_interfaces/action/CameraSweep\n</code></pre></p> <p>Which should present us with the following:</p> <pre><code>#goal\nfloat32 sweep_angle    # the angular sweep (in degrees) over which to capture images\nint32 image_count      # the number of images to capture during the sweep\n---\n#result\nstring image_path      # The filesystem location of the captured images\n---\n#feedback\nint32 current_image    # the number of images taken\nfloat32 current_angle  # the current angular position of the robot (in degrees)\n</code></pre> <p>There are three parts to an action interface, and we'll talk about these in a bit more detail later on, but for now, all we need to know is that in order to call an action, we need to send the action server a Goal.</p> Comparing with ROS Services <p>This is a bit like sending a Request to a ROS Service Server, like we did in the previous session.</p> </li> <li> <p>We can issue a goal to an action server from the command-line using the <code>ros2 action</code> command again. Let's give this a go in TERMINAL 3.</p> <p>First, let's identify the right <code>ros2 action</code> sub-command:</p> <p>TERMINAL 3: <pre><code>ros2 action --help\n</code></pre></p> <pre><code>Commands:\n  info       Print information about an action\n  list       Output a list of action names\n  send_goal  Send an action goal\n</code></pre> <p>As above, there are three sub-commands to choose from, and we've already used the first two! Clearly, the <code>send_goal</code> command is the one we want now.</p> <p>Let's get some help on this one:</p> <pre><code>ros2 action send_goal --help\n</code></pre> <p>From this, we learn that there are three positional arguments, which must be supplied in the correct order:</p> <pre><code>ros2 action send_goal action_name action_type goal\n</code></pre> <p>We know from our earlier interrogation with the <code>ros2 action list</code>, <code>info</code> and <code>ros2 interface show</code> commands how to provide the right data here:</p> <ol> <li><code>action_name</code>: <code>/camera_sweep</code></li> <li><code>action_type</code>: <code>tuos_interfaces/action/CameraSweep</code></li> <li><code>goal</code>: a data packet (in YAML format) containing two parameters:<ol> <li><code>sweep_angle</code>: the angle (in degrees) that the robot will rotate on the spot (i.e. 'sweep')</li> <li><code>image_count</code>: the number of images it will capture from its front-facing camera while 'sweeping'</li> </ol> </li> </ol> </li> <li> <p>Now, again in TERMINAL 3, have a go at using the <code>ros2 action send_goal</code> command, but keep an eye on TERMINAL 2 as you do this:</p> <p>TERMINAL 3: <pre><code>ros2 action send_goal /camera_sweep tuos_interfaces/action/CameraSweep \"{sweep_angle: 0, image_count: 0}\"\n</code></pre></p> <p>Having called the action, you should then be presented with a message (in TERMINAL 3) that the <code>Goal was rejected.</code> In TERMINAL 2 (where the action server is running), we should see some additional information about why this was the case. Read this, and then head back to TERMINAL 3 and have another go at sending a goal to the action server, by supplying valid inputs this time!</p> <p>Once valid goal parameters have been supplied, the action server (in TERMINAL 2), will respond to inform you of what it's going to do. You'll then need to wait for it to do its job...</p> </li> <li> <p>Once the action has completed (it could up to 20 seconds), a message should appear in TERMINAL 3 to inform us of the outcome:</p> <p>TERMINAL 3: <pre><code>Result:\n    image_path: ~/myrosdata/action_examples/YYYYMMDD_hhmmss\n\nGoal finished with status: SUCCEEDED\n</code></pre></p> <p>Additionally, we should see some further text in TERMINAL 2 as well:</p> <p>TERMINAL 2: <pre><code>[INFO] [#####] [camera_sweep_action_server]: camera_sweep_action_server completed successfully:\n  - Angular sweep = # degrees\n  - Images captured = #\n  - Time taken = # seconds\n</code></pre></p> <ol> <li> <p>The result of the action (presented to us in TERMINAL 3) is a file path. Navigate to this directory in TERMINAL 3 (using <code>cd</code>) and have a look at the content using <code>ll</code> (a handy alias for the <code>ls</code> command):</p> <p>You should see the same number of image files in there as was requested with the <code>image_count</code> parameter.</p> </li> <li> <p>Launch <code>eog</code> in this directory and click through all the images to reveal your robot's mystery environment:</p> <p>TERMINAL 3: <pre><code>eog .\n</code></pre></p> </li> </ol> </li> <li> <p>Let's do this one more time. Close down the <code>eog</code> window, head back to TERMINAL 3 and issue the <code>ros2 action send_goal</code> command again, but this time use the optional <code>-f</code> flag: </p> <p>TERMINAL 3: <pre><code>ros2 action send_goal -f /camera_sweep tuos_interfaces/action/CameraSweep \"{sweep_angle: 0, image_count: 0}\" \n</code></pre></p> <p>Tip</p> <p>Don't forget to supply valid goal parameters again!</p> <p>Now, as well as being provided with a result once the action has completed, we're also provided with some regular updates while the action is in progress (aka \"feedback\")! </p> </li> <li> <p>To finish off, close down the action server in TERMINAL 2 and the headless Gazebo process in TERMINAL 1 by entering Ctrl+C in each terminal. </p> </li> </ol>"},{"location":"course/assignment1/part5/#what-is-a-ros-action","title":"What is a ROS Action?","text":"<p>In this Exercise we launched an action server and then called it from the command-line using the <code>ros2 action send_goal</code> sub-command. Using the <code>-f</code> flag we were able to ask the server to provide us with real-time feedback on how it was getting on (in TERMINAL 3). In the same way as a ROS Service, the action also provided us with a result once the task had been completed. Feedback is one of the key features that differentiates a ROS Action from a ROS Service: An Action Server provides feedback messages at regular intervals whilst performing an action and working towards its goal. Another feature of ROS Actions is that they can be cancelled part-way through (which we'll play around with shortly).</p> <p>Ultimately, Actions use a combination of both Topic- and Service-based communication, to create a more advanced messaging protocol. Due to the provision of feedback and the ability to cancel a process part-way through, Actions are designed to be used for longer running tasks. You can read more about Actions in the official ROS 2 documentation here (which also includes a nice animation to explain how they work).</p>"},{"location":"course/assignment1/part5/#the-format-of-action-interfaces","title":"The Format of Action Interfaces","text":"<p>Like Services, Action Interfaces have multiple parts to them, and we need to know what format these action messages take in order to be able to use them.</p> <p>We ran <code>ros2 interface show</code> in the previous exercise, to interrogate the action interface used by the <code>/camera_sweep</code> action server:</p> <pre><code>$ ros2 interface show tuos_interfaces/action/CameraSweep\n\n#goal\nfloat32 sweep_angle    # the angular sweep (in degrees) over which to capture images\nint32 image_count      # the number of images to capture during the sweep\n---\n#result\nstring image_path      # The filesystem location of the captured images\n---\n#feedback\nint32 current_image    # the number of images taken\nfloat32 current_angle  # the current angular position of the robot (in degrees)\n</code></pre> <p>As we know from Exercise 1, in order to call this action server, we need to send a goal, and (as we know from Exercise 1) there are two goal parameters that must be provided:</p> <ol> <li><code>sweep_angle</code>: a 32-bit floating-point value</li> <li><code>image_count</code>: a 32-bit integer</li> </ol> <p>Questions</p> <ul> <li>What are the names of the result and feedback interface parameters? (There are three in total.)</li> <li>What data types do these parameters use?</li> </ul> <p>You'll learn how we use this information to develop Python Action Server &amp; Client nodes in the following exercises.</p>"},{"location":"course/assignment1/part5/#creating-python-action-clients","title":"Creating Python Action Clients","text":"<p>In the previous exercise we called a pre-existing Action Server from the command-line, by sending a goal to it. Let's look at how we can do this from within a Python ROS node now.</p>"},{"location":"course/assignment1/part5/#ex2","title":"Exercise 2: Building a Python Action Client Node","text":"<ol> <li> <p>In TERMINAL 1 launch the mystery world simulation again, but this time with an additional argument:</p> <p>TERMINAL 1: <pre><code>ros2 launch tuos_simulations mystery_world.launch.py with_gui:=true\n</code></pre></p> <p>Which will launch Gazebo in full now, with the GUI attached.</p> </li> <li> <p>Then, in TERMINAL 2, launch the Camera Sweep Action Server again: </p> <p>TERMINAL 2: <pre><code>ros2 run tuos_examples camera_sweep_action_server.py\n</code></pre></p> </li> </ol>"},{"location":"course/assignment1/part5/#part-1-a-minimal-action-client","title":"Part 1: A Minimal Action Client","text":"<ol> <li> <p>Now, in TERMINAL 3, create a new package called <code>part5_actions</code> using the <code>create_pkg.sh</code> helper script from the <code>tuos_ros</code> course repo (return here for a reminder on how to do this).</p> </li> <li> <p>Navigate into the <code>scripts</code> folder of your package using the <code>cd</code> command:</p> <pre><code>cd ~/ros2_ws/src/part5_actions/scripts/\n</code></pre> </li> <li> <p>In here, create a new Python file called <code>camera_sweep_action_client.py</code> (using the <code>touch</code> command) and make it executable (using <code>chmod</code>). </p> </li> <li> <p>Then, declare this as an executable in the package's <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/camera_sweep_action_client.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Also modify the <code>package.xml</code> file (below the <code>&lt;test_depend&gt;ament_lint_common&lt;/test_depend&gt;</code> line) to include following <code>msg</code> dependency:</p> package.xml<pre><code>&lt;depend&gt;action_msgs&lt;/depend&gt;\n</code></pre> </li> <li> <p>At an absolute minimum, the Action Client can be constructed as follows:</p> camera_sweep_action_client.py<pre><code>#! /usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient # (1)!\n\nfrom tuos_interfaces.action import CameraSweep # (2)!\n\nclass CameraSweepActionClient(Node):\n\n    def __init__(self):\n        super().__init__(\"camera_sweep_action_client\") # (3)!\n        self.actionclient = ActionClient(\n            node=self, \n            action_type=CameraSweep, \n            action_name=\"camera_sweep\"\n        ) # (4)!\n\n    def send_goal(self, images=0, angle=0): # (5)!\n        goal = CameraSweep.Goal()\n        goal.sweep_angle = float(angle)\n        goal.image_count = images\n\n        self.actionclient.wait_for_server()\n\n        # send the goal to the action server:\n        return self.actionclient.send_goal_async(goal)\n\ndef main(args=None): # (6)!\n    rclpy.init(args=args)\n    action_client = CameraSweepActionClient()\n    future = action_client.send_goal()\n    rclpy.spin_until_future_complete(action_client, future)\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>As you know by now, in order to develop ROS nodes using Python we need to import the <code>rclpy</code> client library, and the <code>Node</code> class to base our node upon. In addition, here we're also importing an <code>ActionClient</code> class too.  </p> </li> <li> <p>We know that the <code>/camera_sweep</code> Action server uses the <code>CameraSweep</code> <code>action</code> interface from the <code>tuos_interfaces</code> package, so we import that here too (which we use to make a call to the server). </p> </li> <li> <p>Standard practice when we initialise ROS nodes: we must give them a name</p> </li> <li> <p>Here, we instantiate an <code>ActionClient</code> class object. In doing this we define the <code>node</code> to add the action client too (in our case <code>self</code>, i.e. our <code>CameraSweepActionClient</code> class). We then also define the interface type used by the server (<code>CameraSweep</code>), and the name of the action that we want to call (<code>action_name=\"camera_sweep\"</code>).</p> </li> <li> <p>Here we define a class method to construct and deliver a goal to the server. </p> <p>As we know from earlier, a <code>CameraSweep.Goal()</code> contains two parameters that we can assign values to: <code>sweep_angle</code> and <code>image_count</code>.</p> <p>The goal is sent to the server using the <code>send_goal_async()</code> method, which returns a future: i.e. something that will happen in the future, that we can wait on.</p> <p>Tip</p> <p>Both goal parameters are set to <code>0</code> by default!</p> </li> <li> <p>In our <code>main</code> method we initialise <code>rclpy</code> and our <code>CameraSweepActionClient</code> class (nothing new here), but then we call the <code>send_goal()</code> method of our class (as discussed above), which returns a future. We can then use the <code>rclpy.spin_until_future_complete()</code> method to spin up our node only until this future object has finished.</p> <p>Warning</p> <p>When the <code>send_goal()</code> method is called, no additional arguments are provided, which means default values will be applied... which were defined above!</p> </li> </ol> </li> <li> <p>Let's build the Node now, so that we can run it. Head back to TERMINAL 3 and use Colcon to build the package: </p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-select part5_actions --symlink-install\n</code></pre> </li> <li> <p>Re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Run your node with <code>ros2 run</code>...</p> <p>As you have hopefully just observed, the node that we've created here makes a call to the action server, waits for the action to take place and then stops. The only way that you'd know what was happening however, is if you were to keep an eye on TERMINAL 2, to see the action server respond to the goal that was sent to it... The client itself provides no feedback during the action, nor the result at the end. Let's look to incorporate that now...</p> </li> </ol>"},{"location":"course/assignment1/part5/#part-2-handling-a-result","title":"Part 2: Handling a Result","text":"<ol> <li> <p>Go back to the <code>camera_sweep_action_client.py</code> file in VS Code.</p> </li> <li> <p>In order to be able to handle the result that is sent from an action server, we first need to handle the response that the server sends to the goal itself.</p> <p>Within the <code>send_goal()</code> method of the <code>CameraSweepActionClient()</code> class, find the line that reads:</p> <pre><code>return self.actionclient.send_goal_async(goal)\n</code></pre> <p>and change this to:</p> <pre><code>self.send_goal_future = self.actionclient.send_goal_async(goal)\nself.send_goal_future.add_done_callback(self.goal_response_callback)\n</code></pre> <p>This method is no longer returning the future that is sent from <code>send_goal_async()</code>, but is now handling this and adding a callback to it: <code>goal_response_callback</code>. This callback will be executed to inform the client of whether the server has accepted the goal or not.</p> </li> <li> <p>Define this as a new class method of the <code>CameraSweepActionClient()</code> class (i.e. underneath the <code>send_goal()</code> class method that has already been defined)...</p> <pre><code>def goal_response_callback(self, future):\n    goal_handle = future.result()\n    if not goal_handle.accepted:\n        self.get_logger().warn(\"The goal was rejected by the server.\")\n        return\n\n    self.get_logger().info(\"The goal was accepted by the server.\")\n\n    self.get_result_future = goal_handle.get_result_async()\n    self.get_result_future.add_done_callback(self.get_result_callback)\n</code></pre> <p>The input to this method will be the future that is created by the <code>send_goal_async()</code> call. We assign this to <code>goal_handle</code> here, and can then use this for two purposes:</p> <ol> <li>To check if the goal that we sent was accepted by the server</li> <li>If it was accepted, then we can get the result (using <code>get_result_async()</code>) and we can attach another callback to this to actually process that result: <code>get_result_callback</code>.</li> </ol> </li> <li> <p>Now, define <code>get_result_callback</code> as another new method of the <code>CameraSweepActionClient()</code> class (i.e. underneath the <code>goal_response_callback()</code> class method that we have just defined)...</p> <pre><code>def get_result_callback(self, future):\n    result = future.result().result\n    self.get_logger().info(\n        f\"The action has completed.\\n\"\n        f\"Result:\\n\"\n        f\"  - Image Path = {result.image_path}\"\n    )\n    rclpy.shutdown()\n</code></pre> <p>The input to this class method is another future object which contains the actual result sent from the server. We assign this to <code>result</code> and use a <code>get_logger().info()</code> call to print this to the terminal when the action has finished.</p> <p>As we know from our work earlier, the <code>CameraSweep</code> interface contains one <code>result</code> parameter called <code>image_path</code>.</p> </li> <li> <p>Finally, in the <code>main</code> method, change this:</p> <pre><code>rclpy.spin_until_future_complete(action_client, future)\n</code></pre> <p>to this:</p> <pre><code>rclpy.spin(action_client)\n</code></pre> </li> <li> <p>Save all your changes!</p> </li> <li> <p>Run this node again now (with the <code>ros2 run</code> command) and observe the changes in action.</p> <p>Our client node now presents us with the result that is sent by the server on completion of the action, but wouldn't it be nice if we could see the real-time feedback as the action takes place? Let's add this in now...</p> </li> </ol>"},{"location":"course/assignment1/part5/#part-3-handling-feedback","title":"Part 3: Handling Feedback","text":"<ol> <li> <p>Go back to the <code>camera_sweep_action_client.py</code> file in VS Code.</p> </li> <li> <p>In order to be able to handle the feedback that is sent from an action server, we need to add yet another callback! Go back to the <code>send_goal()</code> method and the line where we are actually sending the goal to the server:</p> <pre><code>self.send_goal_future = self.actionclient.send_goal_async(goal)\n</code></pre> <p>As it stands, all we're doing here is sending the goal, but we can also add a feedback callback to this too:</p> <pre><code>self.send_goal_future = self.actionclient.send_goal_async(\n    goal=goal, \n    feedback_callback=self.feedback_callback\n)\n</code></pre> <p>The <code>feedback_callback</code> will be executed every time a new feedback message is received from the server.</p> </li> <li> <p>In order to define what we want to do with these feedback messages we need to define this yet another new method of the <code>CameraSweepActionClient()</code> class. Underneath the <code>get_result_callback()</code> class method that we defined earlier, add this new one as well:</p> <pre><code>def feedback_callback(self, feedback_msg):\n    feedback = feedback_msg.feedback\n    fdbk_current_angle = feedback.current_angle\n    fdbk_current_image = feedback.current_image\n    self.get_logger().info(\n        f\"\\nFEEDBACK:\\n\"\n        f\"  - Current angular position = {fdbk_current_angle:.1f} degrees.\\n\"\n        f\"  - Image(s) captured so far = {fdbk_current_image}.\"\n    )\n</code></pre> <p>As we know from our work earlier, the <code>CameraSweep</code> interface contains two feedback parameters: <code>fdbk_current_angle</code> and <code>fdbk_current_image</code>.</p> </li> <li> <p>Save all your changes once again, run the node again with the <code>ros2 run</code> command and observe the changes in action.</p> <p>The node we've now built can send a goal to an action server, process the feedback sent form the server as the action is in progress, and present the result to us once everything is complete.</p> <p>As discussed earlier though, the other key feature of Actions is the ability to cancel them part-way through. So let's look at how to incorporate this now as well.</p> </li> </ol>"},{"location":"course/assignment1/part5/#part-4-cancelling-an-action","title":"Part 4: Cancelling an Action","text":"<ol> <li> <p>First, create a copy of your <code>camera_sweep_action_client.py</code> node and call it <code>camera_sweep_action_client_cancel.py</code> (make sure you're still in the <code>scripts</code> directory of your <code>part5_actions</code> package before you run this command):</p> <p>TERMINAL 3: <pre><code>cp camera_sweep_action_client.py camera_sweep_cancel_client.py\n</code></pre></p> </li> <li> <p>Don't forget to declare this as an additional executable in the package's <code>CMakeLists.txt</code>. You'll then also need to re-build the package with <code>colcon build</code> (go back for a reminder).</p> </li> <li> <p>Open the <code>camera_sweep_cancel_client.py</code> file in VS Code.</p> </li> <li> <p>We want this client to be able to cancel the goal under two different circumstances:</p> <ol> <li>The client itself is shutdown by the user (via a Ctrl+C in the terminal)</li> <li>A conditional event that happens as the action is underway.</li> </ol> <p>In order to address item 1 first, we need to draw upon some of the work we did in Part 2 in the implementation of safe shutdown procedures...</p> </li> <li> <p>As you hopefully recall, we first need to import <code>SignalHandlerOptions</code> into our node, so add this as an additional import at the start of the code:</p> <pre><code>from rclpy.signals import SignalHandlerOptions\n</code></pre> <p>Then, in the <code>main()</code> node function, modify the <code>rclpy.init()</code> call:</p> <pre><code>rclpy.init(\n    args=args,\n    signal_handler_options=SignalHandlerOptions.NO\n)\n</code></pre> </li> <li> <p>Inside the <code>__init__()</code> method of our <code>CameraSweepActionClient()</code> class we now need to add some additional flags:</p> <pre><code>self.goal_succeeded = False\nself.goal_cancelled = False\nself.stop = False\n</code></pre> <p>In the <code>get_result_callback()</code> class method, we can then ensure that the <code>self.goal_succeeded</code> flag is sent to <code>True</code> when a result is received. In this class methods, locate the <code>rclpy.shutdown()</code> line and add the following additional line just above it:</p> <pre><code>self.goal_succeeded = True\n</code></pre> </li> <li> <p>Actions can be cancelled using a <code>cancel_goal_async()</code> method of the <code>goal_handle</code> that is obtained from the <code>goal_response_callback()</code>. As such, we need to make this accessible across our entire <code>CameraSweepActionClient()</code> class. Locate the <code>goal_response_callback()</code> class method, and add this line at the bottom as the last line of the <code>goal_response_callback()</code> method:</p> <pre><code>self._goal_handle = goal_handle\n</code></pre> <p>This makes <code>goal_handle</code> accessible across the entire <code>CameraSweepActionClient()</code> class as <code>self._goal_handle</code>. </p> </li> <li> <p>We can only attempt to cancel an Action when it's in progress, therefore the feedback callback is the best place to trigger this. Locate the <code>feedback_callback()</code> class method and place the following at the end of it:</p> <pre><code>if self.stop:\n    future = self._goal_handle.cancel_goal_async()\n    future.add_done_callback(self.cancel_goal)\n</code></pre> <p>Here, we call the <code>cancel_goal_async()</code> method from <code>self._goal_handle</code>, and add another new callback (<code>cancel_goal()</code>) to it (i.e. to encapsulate what we want to happen when the action is cancelled).</p> </li> <li> <p>Now, let's define this as another new (and final!) class method:</p> <pre><code>def cancel_goal(self, future):\n    cancel_response = future.result()\n    if len(cancel_response.goals_canceling) &gt; 0:\n        self.get_logger().info('Goal successfully canceled')\n        self.goal_cancelled = True\n    else:\n        self.get_logger().info('Goal failed to cancel')\n</code></pre> <p>The input to this callback is another future, which we can use to determine if the goal has been cancelled (as shown above). If it has, then we set our <code>self.goal_cancelled</code> flag to <code>True</code>.</p> </li> <li> <p>Finally, go back the <code>main()</code> function of the node. We're going to replace the <code>rclpy.spin(action_client)</code> line now, with a <code>rclpy.spin_once()</code>, wrapped inside a <code>try</code> - <code>except</code>, wrapped inside a <code>while</code> loop!</p> <pre><code>while not action_client.goal_succeeded:\n    try:\n        rclpy.spin_once(action_client)\n        if action_client.goal_cancelled:\n            break\n    except KeyboardInterrupt:\n        print(\"Ctrl+C\")\n        action_client.stop = True\n</code></pre> <p>The <code>while</code> loop will execute until the action completes successfully, or until the goal is cancelled, or we shutdown the node with a Ctrl+C interrupt.</p> <p>Look back through the node to see how all this will flow through your class.</p> </li> <li> <p>We may wish to cancel a goal conditionally if - say - too much time has elapsed since the call was made, or the caller has been made aware of something else that has happened in the meantime (perhaps we're running out of storage space on the robot and can't save any more images!). </p> <p>For the purposes of this exercise, we want to modify our node so that the action is always cancelled after a total of 5 images have been captured. This can be done by making a fairly small modification to the <code>feedback_callback()</code>. Have a go at implementing this now. </p> <ul> <li>Have a go now at introducing a conditional call to the <code>cancel_goal()</code> method once a total of 5 images have been captured.</li> <li>You could use the <code>captured_images</code> attribute from the <code>CameraSweepFeedback</code> message to trigger this.</li> </ul> </li> </ol>"},{"location":"course/assignment1/part5/#a-summary-of-ros-actions","title":"A Summary of ROS Actions","text":"<p>ROS Actions work a lot like ROS Services, but they have the following key differences:</p> <ol> <li>They can be cancelled: If something is taking too long, or if something else has happened, then an Action Client can cancel an Action whenever it needs to.</li> <li>They provide feedback: so that a client can monitor what is happening and act accordingly (i.e. cancel the action, if necessary).</li> </ol> <p>This mechanism is therefore useful for operations that may take a long time to execute, and where intervention might be necessary.</p>"},{"location":"course/assignment1/part5/#creating-your-own-action-servers-clients-and-interfaces","title":"Creating Your Own Action Servers, Clients and Interfaces","text":"<p>Important</p> <p>Cancel all active processes that you may have running before moving on.</p> <p>So far we have looked at how to call a pre-existing action server, but what about if we actually want to set up our own, and use our own custom Action Interfaces too? </p> <p>To start with, have a look at the Action Server that you've been working with in the previous exercises. You've been launching this with the following command:</p> <pre><code>ros2 run tuos_examples camera_sweep_action_server.py\n</code></pre> <p>Questions</p> <ul> <li>Which package does the action server node belong to?</li> <li>Where (in that package directory) is this node likely to be located?</li> </ul> <p>Once you've identified the name and the location of the source code, open it up in VS Code and have a look through it to see how it all works. Don't worry too much about all the content associated with obtaining and manipulating camera images in there, we'll learn more about this in the next Part of this course. Instead, focus on the general overall structure of the code and the way that the action server is implemented.</p> <p>Some things to review:</p> <ol> <li> <p>The way the server is initialised and the numerous callbacks that are attached to it:</p> <pre><code>self.actionserver = ActionServer(\n    node=self, \n    action_type=CameraSweep,\n    action_name=\"camera_sweep\",\n    execute_callback=self.server_execution_callback, # (1)!\n    callback_group=ReentrantCallbackGroup(), # (2)!\n    goal_callback=self.goal_callback, # (3)!\n    cancel_callback=self.cancel_callback # (4)!\n)\n</code></pre> <ol> <li>This callback contains all the code that will be executed by the server once a valid goal is sent to it (i.e. the core functionality of the Action)</li> <li>This server node is set up as a Multi-threaded Executor (see the setup in <code>main()</code>), to control the execution of the various callbacks that we need. Here, we're assigning the Action Server to a Reentrant Callback Group, allowing all its callbacks to run in parallel, as well as other subscriber callbacks too. </li> <li>This callback is used to check the goal parameters that have been sent to the server, to decide whether to accept or reject a request. </li> <li>This callback contains everything that needs to happen in the event that the Action is cancelled part-way through.</li> </ol> </li> <li> <p>Take a look at the various Action callbacks to see what's happening in each:</p> <ol> <li>How are goal parameters checked, and subsequently accepted or rejected</li> <li>How cancellations are implemented and how this is monitored in the main <code>server_execution_callback()</code></li> <li>How feedback is handled and published</li> <li>How a result is handled and published too</li> </ol> </li> <li> <p>Finally, consider the shutdown operations.</p> </li> </ol>"},{"location":"course/assignment1/part5/#explore","title":"Implementing an \"Exploration\" strategy using the Action Framework","text":"<p>An exploration strategy allows a robot to autonomously navigate an unknown environment while simultaneously avoiding crashing into things! One way to achieve this is to utilise two distinct motion states: moving forwards and turning on the spot, and repeatedly switching between them. Brownian Motion and Levy Flight are examples of this kind of approach. Randomising the time spent in either, or both, of these two states will result in a navigation strategy that allows a robot to slowly and randomly explore an environment. Forward motion could be performed until - say - a certain distance has been travelled, a set time has elapsed or something gets in the way. Likewise, the direction, speed and/or duration of turning could also be randomised to achieve this.</p> <p>Over the next few exercises we'll construct our own Action Interface, Server and Client nodes with the aim of creating the basis for a basic exploration behaviour. Having completed these exercises you'll have something that - with further development - could be turned into an exploration type behaviour such as the above.</p>"},{"location":"course/assignment1/part5/#ex3","title":"Exercise 3: Creating an Action Interface","text":"<p>In Exercise 2 we created the <code>part5_actions</code> package. Inside this package we will now define an Action Interface to be used by the subsequent Action Server and Client that we will create in the later exercises. </p> <ol> <li> <p>Action interfaces must be defined within an <code>action</code> folder at the root of the package directory, so let's create this now in TERMINAL 1:</p> <ol> <li> <p>First, navigate into your package:</p> <pre><code>cd ~/ros2_ws/src/part5_actions\n</code></pre> </li> <li> <p>Then use <code>mkdir</code> to make a new directory:</p> <pre><code>mkdir action\n</code></pre> </li> </ol> </li> <li> <p>In here we'll now define an Action Interface called <code>ExploreForward</code>:</p> <pre><code>touch action/ExploreForward.action\n</code></pre> </li> <li> <p>Open this up in VS Code and define the data structure of the Interface as follows:</p> ExploreForward.action<pre><code>#goal\nfloat32 fwd_velocity               # The speed at which the robot should move forwards (m/s)\nfloat32 stopping_distance          # Minimum distance of an approaching obstacle before the robot stops (meters) \n---\n#result\nfloat32 total_distance_travelled   # Total distance travelled during the action (meters)\nfloat32 closest_obstacle           # LaserScan distance to the closest detected obstacle up ahead (meters)\n---\n#feedback\nfloat32 current_distance_travelled # Distance travelled so far during the action (meters)\n</code></pre> <p>This interface therefore has two goal parameters, one feedback parameter, and two result parameters:</p> <p>Goal:</p> <ol> <li><code>fwd_velocity</code>: The speed (in m/s) at which the robot should move forwards when the action server is called. </li> <li><code>stopping_distance</code>: The distance (in meters) at which the robot should stop ahead of any objects or boundary walls that are in front of it (this data will come from <code>LaserScan</code> data from the robot's LiDAR sensor).</li> </ol> <p>Feedback:</p> <ol> <li><code>current_distance_travelled</code>: The distance travelled (in meters) since the current action was initiated (based on <code>Odometry</code> data).</li> </ol> <p>Result:</p> <ol> <li><code>total_distance_travelled</code>: The total distance travelled (in meters) over the course of the action (based on <code>Odometry</code> data).</li> <li><code>closest_obstacle</code>: The distance to the closest obstacle up ahead of the robot when the action completes.</li> </ol> </li> <li> <p>Now, declare this interface in the package's <code>CMakeLists.txt</code> file, so that the necessary Python code can be created:</p> CMakeLists.txt<pre><code>find_package(rosidl_default_generators REQUIRED)\nrosidl_generate_interfaces(${PROJECT_NAME}\n  \"action/ExploreForward.action\" \n)\n</code></pre> </li> <li> <p>Also modify the <code>package.xml</code> file (above the <code>&lt;export&gt;</code> line) to include the necessary <code>rosidl</code> dependencies:</p> package.xml<pre><code>&lt;buildtool_depend&gt;rosidl_default_generators&lt;/buildtool_depend&gt;\n&lt;exec_depend&gt;rosidl_default_runtime&lt;/exec_depend&gt;\n&lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;\n</code></pre> <p>Also, add the following <code>msg</code> dependencies too (below the <code>&lt;depend&gt;action_msgs&lt;/depend&gt;</code> line):</p> package.xml<pre><code>&lt;depend&gt;geometry_msgs&lt;/depend&gt;\n&lt;depend&gt;nav_msgs&lt;/depend&gt;\n&lt;depend&gt;sensor_msgs&lt;/depend&gt;\n</code></pre> </li> <li> <p>Now run <code>colcon</code> to generate the necessary source code for this new interface:</p> <ol> <li> <p>First, always make sure you're in the root of the Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Then run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part5_actions --symlink-install \n</code></pre> </li> <li> <p>And finally re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>We can now verify that it worked. </p> <ol> <li> <p>Use <code>ros2 interface</code> to list all available interface types, but use the <code>-a</code> option to display only action-type interfaces:</p> <pre><code>ros2 interface list -a\n</code></pre> <p>Look for a message with the prefix \"<code>part5_actions</code>\".</p> </li> <li> <p>Next, show the data structure of the interface:</p> <pre><code>ros2 interface show part5_actions/action/ExploreForward\n</code></pre> <p>This should match the content of the <code>part5_actions/action/ExploreForward.action</code> file that we created above.</p> </li> </ol> </li> </ol>"},{"location":"course/assignment1/part5/#ex4","title":"Exercise 4: Building the \"ExploreForward\" Action Server","text":"<ol> <li> <p>In TERMINAL 1 navigate to the <code>scripts</code> folder of your <code>part5_actions</code> package then:</p> <ol> <li>Create a Python script called <code>explore_server.py</code></li> <li>Make it executable</li> <li>Declare it as an executable in <code>CMakeLists.txt</code></li> </ol> </li> <li> <p>Open up the file in VS Code.</p> </li> <li> <p>The job of the Server node is as follows:</p> <ul> <li>The action server should make the robot move forwards until it detects an obstacle up ahead.</li> <li>It should use the <code>ExploreForward.action</code> Interface that we created in the previous exercise. </li> <li> <p>The Server will therefore need to be configured to accept two goal parameters:</p> <ol> <li>The speed (in m/s) at which the robot should move forwards when the action server is called. </li> <li> <p>The distance (in meters) at which the robot should stop ahead of any objects or boundary walls that are in front of it.</p> <p>To do this you'll need to subscribe to the <code>/scan</code> topic. Be aware that an object won't necessarily be directly in front of the robot, so you may need to monitor a range of <code>LaserScan</code> data points (within the <code>ranges</code> array) to make the collision avoidance effective (recall the LaserScan callback example from Part 3 (TODO)).</p> </li> </ol> </li> <li> <p>Be sure to do some error checking on the goal parameters to ensure that a valid request is made. This is done by attaching a <code>goal_callback</code> to the Action Server. </p> <ul> <li><code>fwd_velocity</code>: Should be a velocity that is moderate, and within the robot's velocity limits.</li> <li><code>stopping_distance</code>: Should be greater than the minimum detectable limit of the LiDAR Sensor (TODO), large enough to safely avoid collisions. </li> </ul> </li> <li> <p>Whilst your server performs its task it should provide the following feedback to a Client:</p> <ol> <li> <p>The distance travelled (in meters) since the current action was initiated.</p> <p>To do this you'll need to subscribe to the <code>/odom</code> topic. Remember the work that you did in Part 2 on this. </p> <p>Tips</p> <ul> <li>The robot's orientation shouldn't change over the course of a single action call, only its <code>linear.x</code> and <code>linear.y</code> positions should vary.</li> <li>Bear in mind however that the robot won't necessarily be moving along the <code>X</code> or <code>Y</code> axis, so you will need to consider the total distance travelled in the <code>X-Y</code> plane.</li> <li>We did this in the Part 2 <code>move_square</code> exercise, so refer to this if you need a reminder.</li> </ul> </li> </ol> </li> <li> <p>Finally, on completion of the action, your server should be configured to provide the following two result parameters:</p> <ol> <li>The total distance travelled (in meters) over the course of the action.</li> <li>The distance to the obstacle that made the robot stop (if the action server has done its job properly, then this should be very similar to the <code>stopping_distance</code> that was provided by the Action Client in the goal).</li> </ol> </li> </ul> </li> <li> <p>You should refer to the <code>camera_sweep_action_server.py</code> code from the earlier exercises to help you construct this: a lot of the techniques used here will be similar (excluding all the camera related stuff).</p> </li> </ol> <p>Testing</p> <p>There's a good simulation environment that you can use as you're developing your action server here. When you want to test things out, launch the simulation with the following command: </p> <p>TERMINAL 1: <pre><code>ros2 launch turtlebot3_gazebo turtlebot3_dqn_stage4.launch.py\n</code></pre></p> <p>Don't forget, that in order to launch the server, you'll need to have built everything with <code>colcon</code>:</p> <ol> <li> <p>In TERMINAL 2, make sure you're in the root of the Workspace:</p> <p>TERMINAL 2: <pre><code>cd ~/ros2_ws/\n</code></pre></p> </li> <li> <p>Run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part5_actions --symlink-install \n</code></pre> </li> <li> <p>And finally re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> <p>Once you've done this, you'll then be able to run it:</p> <p>TERMINAL 2: <pre><code>ros2 run part5_actions explore_server.py\n</code></pre></p> <p>Don't forget that you don't need to have developed a Python Client Node in order to test the server. Use the <code>ros2 action send_goal</code> CLI tool to make calls to the server (like we did in Exercise 1).</p>"},{"location":"course/assignment1/part5/#ex5","title":"Exercise 5: Building a Basic \"ExploreForward\" Client","text":"<ol> <li> <p>In TERMINAL 3 navigate to the <code>scripts</code> folder of your <code>part5_actions</code> package, create a Python script called <code>explore_client.py</code>, make it executable, and add this to your <code>CMakeLists.txt</code>.</p> </li> <li> <p>Run <code>colcon build</code> on it now, so you don't have to worry about it later:</p> <p>TERMINAL 3: <pre><code>cd ~/ros2_ws\n</code></pre></p> <pre><code>colcon build --packages-select part5_actions --symlink-install\n</code></pre> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Open up the <code>explore_client.py</code> file in VS Code.</p> </li> <li> <p>The job of the Action Client is as follows:</p> <ul> <li>The client needs to issue a correctly formatted goal to the server.</li> <li>The client should be programmed to monitor the feedback data from the Server.  If it detects (from the feedback) that the robot has travelled a distance greater than 2 meters without detecting an obstacle, then it should cancel the current action call.</li> </ul> </li> <li> <p>Use the techniques that we used in the Client node from Exercise 2 as a guide to help you with this. </p> </li> <li> <p>Once you have everything in place, launch the action client with <code>ros2 run</code> as below:</p> <p>TERMINAL 3: <pre><code>ros2 run part5_actions explore_client.py\n</code></pre></p> <p>If all is good, then this client node should call the action server, which will (in turn) make the robot move forwards until it reaches a certain distance from an obstacle up ahead, at which point the robot will stop, and your client node will stop too. Once this happens, reorient your robot (using the <code>teleop_keyboard</code> node) and launch the client node again to make sure that it is robustly stopping in front of obstacles repeatedly, and when approaching them from a range of different angles. </p> <p>Important</p> <p>Make sure that your cancellation functionality works correctly too, ensuring that:</p> <ol> <li>The robot never moves any further than 2 meters during a given action call</li> <li>An action is aborted mid-way through if the client node is shut down with Ctrl+C</li> </ol> </li> </ol>"},{"location":"course/assignment1/part5/#ex6","title":"Exercise 6 (Advanced): Implementing an Exploration Strategy","text":"<p>Up to now, your Action Client node should have the capability to call the <code>ExploreForward.action</code> server to make the robot move forwards by 2 meters, or until it reaches an obstacle (whichever occurs first), but you could build on this now and turn it into a full exploration behaviour:</p> <ul> <li>Between action calls, your client node could make the robot turn on the spot to face a different direction and then issue a further action call to make the robot move forwards once again.</li> <li>The turning process could be done at random (ideally), or by a fixed amount every time.</li> <li>By programming your client node to repeat this process over and over again, the robot would (somewhat randomly) travel around its environment safely, stopping before it crashes into any obstacles and reorienting itself every time it stops moving forwards. </li> </ul>"},{"location":"course/assignment1/part5/#wrapping-up","title":"Wrapping Up","text":"<p>In Part 5 of this course you've learnt:</p> <ul> <li>How ROS Actions work and why they might be useful.</li> <li>How to develop Action Client Nodes in Python which can monitor the action in real-time (via feedback), and which can also cancel the requested action, if required.</li> <li>How to use standard ROS tools to interrogate action interfaces, thus allowing us to build clients to call them</li> <li>How to build custom Action servers, clients and interfaces.</li> <li>How to harness this framework to implement an exploration strategy. </li> </ul>"},{"location":"course/assignment1/part5/#topics-services-or-actions-which-to-choose","title":"Topics, Services or Actions: Which to Choose?","text":"<p>You should now have developed a good understanding of the three communication methods that are available within ROS to facilitate communication between ROS Nodes:</p> <ol> <li>Topic-based messaging.</li> <li>ROS Services.</li> <li>ROS Actions.</li> </ol> <p>Through this course you've gained some practical experience using all three of these, but you may still be wondering how to select the appropriate one for a certain robot task... </p> <p>This ROS.org webpage summarises all of this very nicely (and briefly), so you should have a read through this to make sure you know what's what. In summary though:</p> <ul> <li>Topics: Are most appropriate for broadcasting continuous data-streams such as sensor data and robot state information, and for publishing data that is likely to be required by a range of Nodes across a ROS network.</li> <li>Services: Are most appropriate for very short procedures like quick calculations (inverse kinematics etc.) and performing short discrete actions that are unlikely to go wrong or will not need intervention (e.g. turning on a warning LED when a battery is low).</li> <li>Actions: Are most appropriate for longer running tasks (like moving a robot), or for operations where we might need to change our mind and do something different or cancel an invoked behaviour part way through.</li> </ul>"},{"location":"course/assignment1/part5/#backup","title":"WSL-ROS Managed Desktop Users: Save your work!","text":"<p>Remember, to save the work you have done in WSL-ROS during this session so that you can restore it on a different machine at a later date. Run the following script in any idle WSL-ROS Terminal Instance now:</p> <pre><code>wsl_ros backup\n</code></pre> <p>You'll then be able to restore it to a fresh WSL-ROS environment next time you fire one up (<code>wsl_ros restore</code>).  </p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment1/part6/","title":"Part 6: Cameras, Machine Vision & OpenCV","text":""},{"location":"course/assignment1/part6/#introduction","title":"Introduction","text":"<p> Exercises: 4 Estimated Completion Time: 2 hours</p>"},{"location":"course/assignment1/part6/#aims","title":"Aims","text":"<p>In this part of the course we'll make use of the Waffle's camera, and look at how to work with images in ROS. Here we'll look at how to build ROS nodes that capture images and process them. We'll explore some ways in which this data can be used to inform decision-making in robotic applications.  </p>"},{"location":"course/assignment1/part6/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Use a range of ROS tools to interrogate camera image topics on a ROS Network and view the images being streamed to them.</li> <li>Use the computer vision library OpenCV with ROS, to obtain camera images and process them in real-time.  </li> <li>Apply filtering processes to isolate objects of interest within an image.</li> <li>Develop object detection nodes and harness the information generated by these processes to control a robot's position.</li> <li>Use camera data as a feedback signal to implement a line following behaviour using proportional control.</li> </ol>"},{"location":"course/assignment1/part6/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Using the <code>rqt_image_view</code> node whilst changing the robot's viewpoint</li> <li>Exercise 2: Object Detection</li> <li>Exercise 3: Using Image Moments for Robot Control</li> <li>Exercise 4: Line following</li> </ul>"},{"location":"course/assignment1/part6/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Initial Object Detection Code (for Exercise 2)</li> <li>A Complete Worked Example of the <code>object_detection.py</code> Node</li> <li>The <code>line_follower</code> Template (for Exercise 4)</li> </ul>"},{"location":"course/assignment1/part6/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>Launch your ROS environment now so that you have access to a Linux terminal instance (aka TERMINAL 1).</p> <p>Step 2: Restore your work (WSL-ROS Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS Environment will not be preserved between sessions or across different University computers, and so you should be backing up your work to your <code>U:\\</code> drive regularly. When prompted (on first launch of WSL-ROS in TERMINAL 1) enter <code>Y</code> to restore this<sup>1</sup>.</p> <p>Step 3: Launch VS Code </p> <p>WSL users remember to check for this (TODO).</p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Check that the Course Repo is up-to-date before you start on these exercises. See here for how to install and/or update (TODO). </p> <p>Step 5: Launch the Robot Simulation</p> <p>In this session we'll start by working with the same mystery world environment from Part 5. In TERMINAL 1, use the following command to load it:</p> <p>TERMINAL 1: <pre><code>ros2 launch tuos_simulations coloured_pillars.launch.py\n</code></pre> ...and then wait for the Gazebo window to open:</p> <p></p>"},{"location":"course/assignment1/part6/#working-with-cameras-and-images-in-ros","title":"Working with Cameras and Images in ROS","text":""},{"location":"course/assignment1/part6/#camera-topics-and-data","title":"Camera Topics and Data","text":"<p>There are a number of tools that we can use to view the live images that are being captured by a robot's camera in ROS. As with all robot data, these streams are published to topics, so we firstly need to identify those topics.</p> <p>In a new terminal instance (TERMINAL 2), run <code>ros2 topic list</code> to see the full list of topics that are currently active on our system. Conveniently, all the topics related to our robot's camera are prefixed with <code>/camera</code>! Filter the <code>ros2 topic list</code> output using <code>grep</code> (a Linux command), to only show topics with this prefix:</p> <p>TERMINAL 2: <pre><code>ros2 topic list | grep /camera\n</code></pre></p> <p>This should provide the following filtered list:</p> <pre><code>/camera/camera_info\n/camera/image_raw\n/camera/image_raw/compressed\n/camera/image_raw/compressedDepth\n/camera/image_raw/theora\n</code></pre> <p>The main item that we're interested in here is the raw image data, and the key topic that we'll therefore be using here is:</p> <pre><code>/camera/image_raw\n</code></pre> <p>Run <code>ros2 topic info</code> on this to identify the interface used by this topic.</p> <p>Then, run <code>ros2 interface show</code> on the interface to learn about the data format. You should end up with an output that looks like this (simplified slightly here):</p> <pre><code># This message contains an uncompressed image\n# (0, 0) is at top-left corner of image\n\nstd_msgs/Header header # Header timestamp should be acquisition time of image\n        builtin_interfaces/Time stamp\n                int32 sec\n                uint32 nanosec\n        string frame_id\n\nuint32 height                # image height, that is, number of rows\nuint32 width                 # image width, that is, number of columns\n\nstring encoding       # Encoding of pixels -- channel meaning, ordering, size\n                      # taken from the list of strings in include/sensor_msgs/image_encodings.hpp\n\nuint8 is_bigendian    # is this data bigendian?\nuint32 step           # Full row length in bytes\nuint8[] data          # actual matrix data, size is (step * rows)\n</code></pre> <p></p> <p>Questions</p> <ol> <li>What type of interface is used by this topic, and which package is this derived from?</li> <li>Using <code>ros2 topic echo</code> and the information about the topic message (as shown above) determine the size of the images that our robot's camera will capture (i.e. its dimensions, in pixels).  It will be quite important to know this when we start manipulating these camera images later on. </li> <li>Finally, considering the list above again, which part of the message do you think contains the actual image data?</li> </ol>"},{"location":"course/assignment1/part6/#viz","title":"Visualising Camera Streams","text":"<p>We can view the images being streamed to the above camera topic (in real-time) in a variety of different ways, and we'll explore a couple of these now.</p> <p>One way is to use RViz, which can be launched using the following <code>ros2 launch</code> command:</p> <p>TERMINAL 2: <pre><code>ros2 launch tuos_simulations rviz.launch.py\n</code></pre></p> <p>Once RViz launches, you should see a camera panel in the bottom-left corner with a live stream of the images being obtained from the robot's camera.</p> <p></p> <p>Close down RViz by entering Ctrl+C in TERMINAL 2.  </p>"},{"location":"course/assignment1/part6/#ex1","title":"Exercise 1: Using RQT whilst changing the robot's viewpoint","text":"<p>Another tool we can use to view camera data-streams is the RQT.</p> <ol> <li> <p>Enter the following command to launch it:</p> <p>TERMINAL 2: <pre><code>rqt\n</code></pre> </p> </li> <li> <p>From the top menu select <code>Plugins</code> &gt; <code>Vizualisation</code> &gt; <code>Image View</code>.</p> <p> </p> <p>This allows us to easily view images that are being published to any camera topic on the ROS network. Another useful feature is the ability to save these images (as <code>.jpg</code> files) to the filesystem: See the \"Save as image\" button highlighted in the figure above. This might be useful later on...</p> </li> <li> <p>Click the drop-down box in the top left of the window to select an image topic to display.  Select <code>/camera/image_raw</code> (if it's not already selected).</p> </li> <li> <p>Keep this window open now, and launch a new terminal instance (TERMINAL 3).</p> </li> <li> <p>Launch the <code>teleop_keyboard</code> node. Rotate the robot on the spot, keeping an eye on the RQT Image View window as you do this. Stop the robot once one of the coloured pillars in the arena is roughly in the centre of the robot's field of vision, then close the <code>teleop_keyboard</code> node and RQT Image View by entering Ctrl+C in TERMINAL 3 and TERMINAL 2 respectively.</p> </li> </ol>"},{"location":"course/assignment1/part6/#opencv","title":"OpenCV and ROS","text":"<p>OpenCV is a mature and powerful computer vision library designed for performing real-time image analysis, and it is therefore extremely useful for robotic applications.  The library is cross-platform and there is a Python API (<code>cv2</code>), which we'll be using to do some computer vision tasks of our own during this lab session. While we can work with OpenCV using Python straight away (via the API), the library can't directly interpret the native image format used by the ROS, so there is an interface that we need to use.  The interface is called CvBridge, which is a ROS package that handles the conversion between ROS and OpenCV image formats.  We'll therefore need to use these two libraries (OpenCV and CvBridge) hand-in-hand when developing ROS nodes to perform computer vision related tasks.</p>"},{"location":"course/assignment1/part6/#object-detection","title":"Object Detection","text":"<p>One common job that we often want a robot to perform is object detection, and we will illustrate how this can be achieved using OpenCV tools for colour filtering, to detect the coloured pillar that your robot should now be looking at.  </p>"},{"location":"course/assignment1/part6/#ex2","title":"Exercise 2: Object Detection","text":"<p>In this exercise you will learn how to use OpenCV to capture images, filter them and perform other analysis to confirm the presence and location of features that we might be interested in.</p>"},{"location":"course/assignment1/part6/#step-1-getting-started","title":"Step 1: Getting Started","text":"<ol> <li> <p>First create a new package called <code>part6_vision</code> using the <code>create_pkg.sh</code> helper script (from the <code>tuos_ros</code> course repo). </p> </li> <li> <p>Navigate into the <code>scripts</code> directory of this new package (using <code>cd</code>), create a new file called <code>object_detection.py</code> (using <code>touch</code>), make this executable (<code>chmod</code>) and declare this as an executable in the package's <code>CMakeLists.txt</code> (you've done all this lots of times now, but check back through previous parts of the course if you need a reminder on how to do any of it).</p> </li> <li> <p>Modify the <code>package.xml</code> file (below the <code>&lt;buildtool_depend&gt;ament_cmake_python&lt;/buildtool_depend&gt;</code> line) to include following dependencies:</p> package.xml<pre><code>&lt;depend&gt;opencv2&lt;/depend&gt;\n&lt;depend&gt;cv_bridge&lt;/depend&gt;\n&lt;depend&gt;sensor_msgs&lt;/depend&gt;\n&lt;depend&gt;geometry_msgs&lt;/depend&gt;\n</code></pre> </li> <li> <p>Build the package with <code>colcon</code>:</p> <ol> <li> <p>In TERMINAL 2, make sure you're in the root of the Workspace:</p> <p>TERMINAL 2: <pre><code>cd ~/ros2_ws/\n</code></pre></p> </li> <li> <p>Run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part6_vision --symlink-install \n</code></pre> </li> <li> <p>And finally re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Open up the <code>object_detection.py</code> Python file in VS Code.</p> </li> <li> <p>Copy the code here into the empty <code>object_detection.py</code> file, save it, then read the annotations so that you understand how the node works and what should happen when you run it. </p> </li> <li> <p>Run the node using <code>ros2 run</code>.</p> <p>Warning</p> <p>This node will capture an image and display it in a pop-up window. Once you've viewed the image in this pop-up window MAKE SURE YOU CLOSE THE POP-UP WINDOW DOWN so that execution can complete!</p> </li> <li> <p>As you should know from reading the explainer, the node has just obtained an image and saved it to a location on the filesystem.  Navigate to this filesystem location and view the image using <code>eog</code>.</p> <p>What you may have noticed from the terminal output when you ran the <code>object_detection.py</code> node is that the robot's camera captures images at a native size of 1080x1920 pixels (you should already know this from interrogating the <code>/camera/rgb/image_raw/width</code> and <code>/height</code> messages using <code>rostopic echo</code> earlier, right?!).  That's over 2 million pixels in total in a single image (2,073,600 pixels per image, to be exact), each pixel having a blue, green and red value associated with it - so that's a lot of data in a single image file! </p> <p>Question</p> <p>The size of the image file (in bytes) was actually printed to the terminal when you ran the <code>object_detection.py</code> node. Did you notice how big it was exactly?</p> </li> </ol> <p>Processing an image of this size is therefore hard work for a robot: any analysis we do will be slow and any raw images that we capture will occupy a considerable amount of storage space. The next step then is to reduce this down by cropping the image to a more manageable size.</p>"},{"location":"course/assignment1/part6/#step-2-cropping","title":"Step 2: Cropping","text":"<p>We're going to modify the <code>object_detection.py</code> node now to:</p> <ul> <li>Capture a new image in its native size</li> <li>Crop it down to focus in on a particular area of interest</li> <li>Save both of the images (the cropped one should be much smaller than the original)  </li> </ul> <p></p> <ol> <li> <p>In your <code>object_detection.py</code> node locate the line:</p> <pre><code>self.show_image(img=cv_img, img_name=\"step1_original\")\n</code></pre> </li> <li> <p>Underneath this, add the following additional lines of code:</p> <pre><code>crop_width = width - 400\ncrop_height = 400\ncrop_y0 = int((width / 2) - (crop_width / 2))\ncrop_z0 = int((height / 2) - (crop_height / 2))\ncropped_img = cv_img[crop_z0:crop_z0+crop_height, crop_y0:crop_y0+crop_width]\n\nself.show_image(img=cropped_img, img_name=\"step2_cropping\")\n</code></pre> </li> <li> <p>Run the node again.  </p> <p>Remember</p> <p>Make sure you close all of these pop-up windows down after viewing them to ensure that all your images are saved to the filesystem and the node completes all of its tasks successfully.</p> <p>The code that you have just added here has created a new image object called <code>cropped_img</code>, from a subset of the original by specifying a desired <code>crop_height</code> and <code>crop_width</code> relative to the original image dimensions.  Additionally, we have also specified where in the original image (in terms of pixel coordinates) we want this subset to start, using <code>crop_y0</code> and <code>crop_z0</code>. This process is illustrated in the figure below:</p> <p> </p> <p>The original image (<code>cv_img</code>) is cropped using a process called \"slicing\":</p> <p><pre><code>cropped_img = cv_img[\n    crop_z0:crop_z0+crop_height,\n    crop_y0:crop_y0+crop_width\n]\n</code></pre> This may seem quite confusing, but hopefully the figure below illustrates what's going on here:</p> <p> </p> </li> </ol>"},{"location":"course/assignment1/part6/#step-3-masking","title":"Step 3: Masking","text":"<p>As discussed above, an image is essentially a series of pixels each with a blue, green and red value associated with it to represent the actual image colours. From the original image that we have just obtained and cropped, we then want to get rid of any colours other than those associated with the pillar that we want the robot to detect.  We therefore need to apply a filter to the pixels, which we will ultimately use to discard any pixel data that isn't related to the coloured pillar, whilst retaining data that is.  </p> <p>This process is called masking and, to achieve this, we need to set some colour thresholds. This can be difficult to do in a standard Blue-Green-Red (BGR) or Red-Green-Blue (RGB) colour space, and you can see a good example of this in this article from RealPython.com.  We will apply some steps discussed in this article to convert our cropped image into a Hue-Saturation-Value (HSV) colour space instead, which makes the process of colour masking a bit easier.</p> <ol> <li> <p>First, analyse the Hue and Saturation values of the cropped image. To do this, first navigate to the <code>~/myrosdata/object_detection</code> directory, where the raw image has been saved:</p> <p>TERMINAL 2: <pre><code>cd ~/myrosdata/object_detection\n</code></pre></p> <p>Then, run the following ROS Node (from the <code>tuos_examples</code> package), supplying the name of the cropped image as an additional argument:</p> <pre><code>ros2 run tuos_examples image_colours.py step2_cropping.jpg\n</code></pre> </li> <li> <p>The node should produce a scatter plot, illustrating the Hue and Saturation values of each of the pixels in the image. Each data point in the plot represents a single image pixel and each is coloured to match its RGB value:</p> <p> </p> </li> <li> <p>You should see from the image that all the pixels related to the coloured pillar that we want to detect are clustered together.  We can use this information to specify a range of Hue and Saturation values that can be used to mask our image: filtering out any colours that sit outside this range and thus allowing us to isolate the pillar itself. The pixels also have a Value (or \"Brightness\"), which isn't shown in this plot. As a rule of thumb, a range of brightness values between 100 and 255 generally works quite well.</p> <p> </p> <p>In this case then, we select upper and lower HSV thresholds as follows:</p> <pre><code>lower_threshold = (115, 225, 100)\nupper_threshold = (130, 255, 255)\n</code></pre> <p>Use the plot that you have generated yourself to determine your own upper and lower thresholds. </p> <p>OpenCV contains a built-in function to detect which pixels of an image fall within a specified HSV range: <code>cv2.inRange()</code>.  This outputs a matrix, the same size and shape as the number of pixels in the image, but containing only <code>True</code> (<code>1</code>) or <code>False</code> (<code>0</code>) values, illustrating which pixels do have a value within the specified range and which don't.  This is known as a Boolean Mask (essentially, a series of ones or zeroes).  We can then apply this mask to the image, using a Bitwise AND operation, to get rid of any image pixels whose mask value is <code>False</code> and keep any flagged as <code>True</code> (or in range).</p> </li> <li> <p>To do this, first locate the following line in your <code>object_detection.py</code> node:</p> <pre><code>self.show_image(img=cropped_img, img_name=\"step2_cropping\")\n</code></pre> </li> <li> <p>Underneath this, add the following:</p> <pre><code>hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\nlower_threshold = (115, 225, 100)\nupper_threshold = (130, 255, 255)\nimg_mask = cv2.inRange(hsv_img, lower_threshold, upper_threshold)\n\nself.show_image(img=img_mask, img_name=\"step3_image_mask\")\n</code></pre> </li> <li> <p>Now, run the node again. A third image should also be generated now. </p> <p>As shown in the figure below, the third image should simply be a black and white representation of the cropped image, where the white regions should indicate the areas of the image where pixel values fall within the HSV range specified earlier. </p> <p>Notice (from the text printed to the terminal) that the cropped image and the image mask have the same dimensions, but the image mask file has a significantly smaller file size.  While the mask contains the same number of pixels, these pixels only have a value of <code>1</code> or <code>0</code>, whereas - in the cropped image of the same pixel size - each pixel has a Red, Green and Blue value: each ranging between <code>0</code> and <code>255</code>, which represents significantly more data.</p> <p> </p> </li> </ol>"},{"location":"course/assignment1/part6/#bitwise_and","title":"Step 4: Filtering","text":"<p>Finally, we can apply this mask to the cropped image, generating a final version of it where only pixels marked as <code>True</code> in the mask retain their RGB values, and the rest are simply removed.  As discussed earlier, we use a Bitwise AND operation to do this and, once again, OpenCV has a built-in function to do this: <code>cv2.bitwise_and()</code>.</p> <ol> <li> <p>Locate the following line in your <code>object_detection.py</code> node:</p> <pre><code>self.show_image(img=img_mask, img_name=\"step3_image_mask\")\n</code></pre> </li> <li> <p>And, underneath this, add the following:</p> <pre><code>filtered_img = cv2.bitwise_and(cropped_img, cropped_img, mask = img_mask)\n\nself.show_image(img=filtered_img, img_name=\"step4_filtered_image\")\n</code></pre> </li> <li> <p>Run this node again, and a fourth image should also be generated now, this time showing the cropped image taken from the robot's camera, but only containing data related to coloured pillar, with all other background image data removed (and rendered black):</p> <p> </p> </li> </ol>"},{"location":"course/assignment1/part6/#image-moments","title":"Image Moments","text":"<p>You have now successfully isolated an object of interest within your robot's field of vision, but perhaps we want to make our robot move towards it, or - conversely - make our robot navigate around it and avoid crashing into it!  We therefore also need to know the position of the object in relation to the robot's viewpoint, and we can do this using image moments.</p> <p>The work we have just done above led to us obtaining what is referred to as a colour blob.  OpenCV also has built-in tools to allow us to calculate the centroid of a colour blob like this, allowing us to determine where exactly within an image the object of interest is located (in terms of pixels).  This is done using the principle of image moments: essentially statistical parameters related to an image, telling us how a collection of pixels (i.e. the blob of colour that we have just isolated) are distributed within it.  You can read more about Image Moments here, which tells us that the central coordinates of a colour blob can be obtained by considering some key moments of the image mask that we obtained from thresholding earlier:</p> <ul> <li>\\(M_{00}\\): the sum of all non-zero pixels in the image mask (i.e. the size of the colour blob, in pixels)</li> <li>\\(M_{10}\\): the sum of all the non-zero pixels in the horizontal (y) axis, weighted by row number</li> <li>\\(M_{01}\\): the sum of all the non-zero pixels in the vertical (z) axis, weighted by column number</li> </ul> <p>Remember</p> <p>We refer to the horizontal as the y-axis and the vertical as the z-axis here, to match the terminology that we have used previously to define our robot's principal axes.</p> <p>We don't really need to worry about the derivation of these moments too much though.  OpenCV has a built-in <code>moments()</code> function that we can use to obtain this information from an image mask (such as the one that we generated earlier):</p> <pre><code>m = cv2.moments(img_mask)\n</code></pre> <p>So, using this we can obtain the <code>y</code> and <code>z</code> coordinates of the blob centroid quite simply:</p> <pre><code>cy = m['m10']/(m['m00']+1e-5)\ncz = m['m01']/(m['m00']+1e-5) \n</code></pre> <p>Question</p> <p>We're adding a very small number to the \\(M_{00}\\) moment here to make sure that the divisor in the above equations is never zero and thus ensuring that we never get caught out by any \"divide-by-zero\" errors. Why might this be necessary?</p> <p></p> <p>Once again, there is a built-in OpenCV tool that we can use to add a circle onto an image to illustrate the centroid location within the robot's viewpoint: <code>cv2.circle()</code>.  This is how we produced the red circle that you can see in the figure above.  You can see how this is implemented in a complete worked example of the <code>object_detection.py</code> node from the previous exercise. </p> <p>In our case, we can't actually change the position of our robot in the z axis, so the <code>cz</code> centroid component here might not be that important to us for navigation purposes.  We may however want to use the centroid coordinate <code>cy</code> to understand where a feature is located horizontally in our robot's field of vision, and use this information to turn towards it (or away from it, depending on what we are trying to achieve).  We can then use this as the basis for some real closed-loop control.</p>"},{"location":"course/assignment1/part6/#ex3","title":"Exercise 3: Using Image Moments for Robot Control","text":"<p>Inside the <code>tuos_examples</code> package there is a node that has been developed to illustrate how all the OpenCV tools that you have explored so far could be used to search an environment and stop a robot when it is looking directly at an object of interest. All the tools that are used in this node should be familiar to you by now, and in this exercise you're going to make a copy of this node and modify it to enhance its functionality.</p> <ol> <li> <p>The node is called <code>colour_search.py</code>, and it is located in the <code>scripts</code> folder of the <code>tuos_examples</code> package. Copy this into the <code>scripts</code> folder of your own <code>part6_vision</code> package by first ensuring that you are located in the desired destination folder:</p> <p>TERMINAL 2: <pre><code>cd ~/ros2_ws/src/part6_vision/scripts\n</code></pre></p> </li> <li> <p>Then, copy the <code>colour_search.py</code> node using <code>cp</code> as follows:</p> <p>TERMINAL 2: <pre><code>cp ~/ros2_ws/src/tuos_ros/tuos_examples/scripts/colour_search.py ./\n</code></pre></p> </li> <li> <p>Open up the <code>colour_search.py</code> file in VS Code to view the content.  Have a look through it and see if you can make sense of how it works.  The overall structure should be fairly familiar to you by now: we have a Python class structure, a Subscriber with a callback function, a timer with a callback containing all the robot control code, and a lot of the OpenCV tools that you have explored so far in this part of the course.  Essentially this node functions as follows:</p> <ol> <li>The robot turns on the spot whilst obtaining images from its camera (by subscribing to the <code>/camera/image_raw</code> topic).</li> <li>Camera images are obtained, cropped, then a threshold is applied to the cropped images to detect the blue pillar in the simulated environment.</li> <li>If the robot can't see a blue pillar then it turns on the spot quickly.</li> <li>Once detected, the centroid of the blue blob representing the pillar is calculated to obtain its current location in the robot's viewpoint.</li> <li>As soon as the blue pillar comes into view the robot starts to turn more slowly instead.</li> <li>The robot stops turning as soon as it determines that the pillar is situated directly in front of it (determined using the <code>cy</code> component of the blue blob centroid).</li> <li>The robot then waits for a while and then starts to turn again.</li> <li>The whole process repeats until it finds the blue pillar once again.</li> </ol> </li> <li>Run the node as it is to see this in action.  Observe the log messages as they are printed to the terminal throughout execution.</li> <li>Your task is to then modify the node so that it stops in front of every coloured pillar in the arena (there are four in total, each of a different colour, as you know). For this, you may need to use some of the methods that you have explored in the previous exercises.<ol> <li>You might first want to use some of the methods that we used to obtain and analyse some images from the robot's camera:<ol> <li>Use the <code>teleop_keyboard</code> node to manually move the robot, making it look at every coloured pillar in the arena individually.</li> <li>Run the <code>object_detection.py</code> node that you developed in the previous exercise to capture an image, crop it, save it to the filesystem and then feed this cropped image into the <code>image_colours.py</code> node from the <code>tuos_examples</code> package (as you did earlier)</li> <li>From the plot that is generated by the <code>image_colours.py</code> node, determine some appropriate HSV thresholds to apply for each coloured pillar in the arena.</li> </ol> </li> <li>Once you have the right thresholds, then you can add these to your <code>colour_search.py</code> node so that it has the ability to detect every pillar in the same way that it currently detects the blue one.</li> </ol> </li> </ol>"},{"location":"course/assignment1/part6/#pid","title":"PID Control and Line Following","text":"<p>Line following is a handy skill for a robot to have! We can achieve this on our TurtleBot3 using its camera system and the image processing techniques that have been covered so far in this session.</p> <p>COM2009 Lecture 6 introduces a well established algorithm for closed-loop control known as PID Control, and this can be used to achieve such line following behaviour.</p> <p>At the heart of this is the principle of Negative-Feedback control, which considers a Reference Input, a Feedback Signal and the Error between these.</p> <p></p> <p> </p>      Negative-Feedback Control     Adapted from Arturo Urquizo, CC BY-SA 3.0, via Wikimedia Commons <p>The Reference Input represents a desired state that we would like our system to maintain. If we want our TurtleBot3 to successfully follow a coloured line on the floor, we will need it to keep the colour blob that represents that coloured line in the centre of its view point at all times. The desired state would therefore be to maintain the <code>cy</code> centroid of the colour blob in the centre of its vision.</p> <p>A Feedback Signal informs us of what the current state of the system actually is. In our case, this feedback signal would be the real-time location of the coloured line in the live camera images, i.e. its <code>cy</code> centroid (obtained using processing methods such as those covered in Exercise 3 above). </p> <p>The difference between these two things is the Error, and the PID control algorithm provides us with a means to control this error and minimise it, so that our robot's actual state matches the desired state. i.e.: the coloured line is always in the centre of its viewpoint.</p> <p></p> <p></p> <p></p> <p>The PID algorithm is as follows:</p> \\[ u(t)=K_{P} e(t) + K_{I}\\int e(t)dt + K_{D}\\dfrac{d}{dt}e(t) \\] <p>Where \\(u(t)\\) is the Controlled Output, \\(e(t)\\) is the Error (as illustrated in the figure above) and \\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\) are Proportional, Integral and Differential Gains respectively. These three gains are constants that must be established for any given system through a process called tuning. This tuning process is discussed in COM2009 Lecture 6, but you will also explore this in the practical exercise that follows.</p>"},{"location":"course/assignment1/part6/#ex4","title":"Exercise 4: Line Following","text":""},{"location":"course/assignment1/part6/#ex4a","title":"Part A: Setup","text":"<ol> <li>Make sure that all ROS processes from the previous exercise are shut down now, including the <code>colour_search.py</code> node, and the Gazebo simulation in TERMINAL 1.</li> <li> <p>In TERMINAL 1 launch a new simulation from the <code>tuos_simulations</code> package:</p> <p>TERMINAL 1: <pre><code>ros2 launch tuos_simulations line_following_setup.launch.py\n</code></pre></p> <p>Your robot should be launched onto a long thin track with a straight pink line painted down the middle of the floor:</p> <p> </p> </li> <li> <p>In TERMINAL 2 you should still be located in your <code>part6_vision/scripts</code> directory, but if not then go there now:</p> <p>TERMINAL 2: <pre><code>cd ~/ros2_ws/src/part6_vision/scripts\n</code></pre></p> </li> <li> <p>Perform the necessary steps to create a new empty Python file called <code>line_follower.py</code> and prepare it for execution as a node within your package.</p> </li> <li> <p>Once that's done open up the empty file in VS Code.</p> <p></p> </li> <li> <p>Start with the code template provided here. This template contains three \"TODOs\" that you need to complete, all of which are explained in detail in the code annotations, so read these carefully. Ultimately, you did all of this in Exercise 2, so go back here if you need a reminder on how any of this works. </p> <p>Your aim here is to get the code to generate a cropped image, with the coloured line isolated and located within it, like this:</p> <p> </p> </li> </ol>"},{"location":"course/assignment1/part6/#ex4b","title":"Part B: Implementing and Tuning a Proportional Controller","text":"<p>Referring back to the equation for the PID algorithm as discussed above, the Proportional, Integral and Differential components all have different effects on a system in terms of its ability to maintain the desired state (the reference input). The gain terms associated with each of these components (\\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\)) must be tuned appropriately for any given system in order to achieve stability of control.</p> <p>A PID Controller can actually take three different forms:</p> <ol> <li>\"P\" Control: Only a Proportional gain (\\(K_{P}\\)) is used, all other gains are set to zero.</li> <li>\"PI\" Control: Proportional and Integral gains (\\(K_{P}\\) and \\(K_{I}\\)) are applied, the Differential gain is set to zero. </li> <li>\"PID\" Control: The controller makes use of all three gain terms (\\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\))</li> </ol> <p>In order to allow our TurtleBot3 to follow a line, we actually only really need a \"P\" Controller, so our control equation becomes quite simple, reducing to:</p> \\[ u(t)=K_{P} e(t) \\] <p>The next task then is to adapt our <code>line_follower.py</code> node to implement this control algorithm and find a proportional gain that is appropriate for our system.</p> <ol> <li> <p>Return to your <code>line_follower.py</code> file. Underneath the line that reads:</p> <pre><code>cv2.waitKey(1)\n</code></pre> <p>Paste the following additional code:</p> <pre><code>kp = 0.01\nreference_input = ?\nfeedback_signal = cy\nerror = feedback_signal - reference_input \n\nang_vel = kp * error\nself.get_logger().info(\n    f\"Error = {error:.1f} pixels | Control Signal = {ang_vel:.2f} rad/s\"\n)\n</code></pre> <p></p> <p>Fill in the Blank!</p> <p>What is the Reference Input to the control system (<code>reference_input</code>)? Refer to this figure from earlier. </p> <p>Here we have implemented our \"P\" Controller. The Control Signal that is being calculated here is the angular velocity that will be applied to our robot (the code won't make the robot move just yet, but we'll get to that bit shortly!) The Controlled Output will therefore be the angular position (i.e. the yaw) of the robot.  </p> </li> <li> <p>Run the code as it is, and consider the following:</p> <ol> <li>What proportional gain (\\(K_{P}\\)) are we applying?</li> <li>What is the maximum angular velocity that can be applied to our robot? Is the angular velocity that has been calculated actually appropriate?</li> <li>Is the angular velocity that has been calculated positive or negative? Will this make the robot turn in the right direction and move towards the line?  </li> </ol> </li> <li> <p>Let's address the third question (c) first...</p> <p>A positive angular velocity should make the robot turn anti-clockwise (i.e. to the left), and a negative angular velocity should make the robot turn clockwise (to the right). The line should currently be to the left of the robot, which means a positive angular velocity would be required in order to make the robot turn towards it. If the value of the Control Signal that is being calculated by our proportional controller (as printed to the terminal) is negative, then this isn't correct, so we need to change the sign of our proportional gain (\\(K_{P}\\)) in order to correct this:</p> <pre><code>kp = -0.01\n</code></pre> </li> <li> <p>Next, let's address the second of the above questions (b)...</p> <p>The maximum angular velocity that can be applied to our robot is \u00b11.82 rad/s. If our proportional controller is calculating a value for the Control Signal that is greater than 1.82, or less than -1.82 then this needs to be limited. In between the following two lines of code:</p> <pre><code>ang_vel = kp * error\nself.get_logger().info(\n    f\"Error = {error:.1f} pixels | Control Signal = {ang_vel:.2f} rad/s\"\n)\n</code></pre> <p>Insert the following: <pre><code>if ang_vel &lt; -1.82:\n    ang_vel = -1.82\nelif ang_vel &gt; 1.82:\n    ang_vel = 1.82\n</code></pre></p> </li> <li> <p>Finally, we need to think about the actual proportional gain that is being applied. This is where we need to actually tune our system by finding a proportional gain value that controls our system appropriately.</p> <p>Return to your <code>line_follower.py</code> file. Underneath the line that reads:</p> <pre><code>self.get_logger().info(\n    f\"Error = {error:.1f} pixels | Control Signal = {ang_vel:.2f} rad/s\"\n)\n</code></pre> <p>Paste the following:</p> <pre><code>self.vel_cmd.linear.x = 0.1\nself.vel_cmd.angular.z = ang_vel\nself.vel_pub.publish(self.vel_cmd)\n</code></pre> <p>The code should now make the robot move with a constant linear velocity of 0.1 m/s at all times, while its angular velocity will be determined by our proportional controller, based on the controller error and the proportional gain parameter <code>kp</code>.</p> <p>The figure below illustrates the effects different values of proportional gain can have on a system.</p> <p>      Courtesy of Prof. Roger Moore     Taken from COM2009 Lecture 6: PID Control    </p> <p>Run the code and see what happens. You should find that the robot behaves quite erratically, indicating that <code>kp</code> (at an absolute value of 0.01) is probably too large.</p> </li> <li> <p>Try reducing <code>kp</code> by a factor of 100: <code>kp = -0.0001</code>. Before you run the code again, you can reset the Gazebo simulation by pressing Ctrl+Shift+R so that the robot returns to the starting position.</p> <p>You should find that the robot now gradually approaches the line, but it can take a while for it to do so.</p> </li> <li> <p>Next, increase <code>kp</code> by a factor of 10: <code>kp = -0.001</code>. Once again, reset the robot back to its starting position in Gazebo by using Ctrl+Shift+R to reset the simulation.</p> <p>The robot should now reach the line much quicker, and follow the line well once it reaches it.</p> </li> <li> <p>Could <code>kp</code> be modified any more to improve the control further? Play around a bit more and see what happens. We'll but this to the test on a more challenging track in the next part of this exercise.</p> </li> </ol>"},{"location":"course/assignment1/part6/#ex4c","title":"Part C: Advanced Line Following","text":"<ol> <li> <p>Now, in TERMINAL 1 run a new simulation:</p> <p>TERMINAL 1: <pre><code>ros2 launch tuos_simulations line_following.launch.py\n</code></pre></p> <p>Your robot should be launched into an environment with a more interesting line to follow:</p> <p> </p> </li> <li> <p>In TERMINAL 2, run your <code>line_follower.py</code> node and see how it performs. Does your proportional gain need to be adjusted further to optimise the performance?</p> </li> <li> <p>Next, think about conditions where the line can't initially be seen...</p> <p>As you know, the angular velocity is determined by considering the <code>cy</code> component of a colour blob representing the line. What happens in situations where the blob of colour isn't there though?  What influence would this have on the Control Signals that are calculated by the proportional controller? To consider this further, try launching the robot in the same arena but in a different location instead, and think about how you might approach this situation:</p> <p>TERMINAL 1: <pre><code>ros2 launch tuos_simulations line_following.launch.py x_pos:=3 y_pos:=-3 yaw:=0\n</code></pre></p> </li> <li> <p>Finally, what happens when the robot reaches the finish line? How could you add additional functionality to ensure that the robot stops when it reaches this point? What features of the arena could you use to trigger this?</p> </li> </ol>"},{"location":"course/assignment1/part6/#wrapping-up","title":"Wrapping Up","text":"<p>In this session you have learnt how to use data from a robot's camera to extract further information about its environment.  The camera allows our robot to \"see\" and the information that we obtain from this device can allow us to develop more advanced robotic behaviours such as searching for objects, follow things or - conversely - moving away or avoiding them.  You have learnt how to do some basic tasks with OpenCV, but this is a huge and very capable library of computer vision tools, and we encourage you to explore this further yourselves to enhance some of the basic principles that we have shown you today.</p>"},{"location":"course/assignment1/part6/#backup","title":"WSL-ROS Managed Desktop Users: Save your work!","text":"<p>Remember, to save the work you have done in WSL-ROS during this session so that you can restore it on a different machine at a later date. Run the following script in any idle WSL-ROS Terminal Instance now:</p> <pre><code>wsl_ros backup\n</code></pre> <p>You'll then be able to restore it to a fresh WSL-ROS environment whenever you need it again (<code>wsl_ros restore</code>).  </p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment1/part1/publisher/","title":"A Simple Publisher Node","text":""},{"location":"course/assignment1/part1/publisher/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>publisher.py</code> file and review the annotations to understand how it all works.</p> <p>Tip</p> <p>Don't forget the Shebang! See below for further details...</p> publisher.py<pre><code>#!/usr/bin/env python3\n# A simple ROS2 Publisher\n\nimport rclpy # (1)!\nfrom rclpy.node import Node\n\nfrom example_interfaces.msg import String # (2)!\n\nclass SimplePublisher(Node): # (3)!\n\n    def __init__(self):\n        super().__init__(\"simple_publisher\") # (4)!\n\n        self.my_publisher = self.create_publisher(\n            msg_type=String,\n            topic=\"my_topic\",\n            qos_profile=10,\n        ) # (5)!\n\n        publish_rate = 1 # Hz\n        self.timer = self.create_timer(\n            timer_period_sec=1/publish_rate, \n            callback=self.timer_callback\n        ) # (6)!\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\" # (7)!\n        )\n\n    def timer_callback(self): # (8)!\n        ros_time = self.get_clock().now().seconds_nanoseconds()\n\n        topic_msg = String()\n        topic_msg.data = f\"The ROS time is {ros_time[0]} (seconds).\"\n        self.my_publisher.publish(topic_msg)\n        self.get_logger().info(f\"Publishing: '{topic_msg.data}'\")\n\ndef main(args=None): # (9)!\n    rclpy.init(args=args)\n    my_simple_publisher = SimplePublisher()\n    rclpy.spin(my_simple_publisher)\n    my_simple_publisher.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__': # (10)!\n    main()\n</code></pre> <ol> <li> <p><code>rclpy</code> is the ROS Client Library for Python. </p> <p>This is a vital import that allows us to create ROS nodes and initialise them on the ROS network.</p> <p>We also import the <code>Node</code> class from the <code>rclpy.node</code> library. This is a ready-made Python Class that contains all the necessary functionality that a Python ROS Node might need, so we'll use this as the basis for our own node (which we'll create shortly).</p> </li> <li> <p>We also need to import the <code>String</code> message type from the <code>example_interfaces.msg</code> library for publishing our messages.</p> </li> <li> <p>We create a Python class called <code>SimplePublisher()</code>, which we'll use to encapsulate all the functionality of our node.</p> <p>The vast majority of the functionality of this node is inherited from the <code>rclpy.node</code>, <code>Node()</code> Class which we imported above. </p> </li> <li> <p>Using the <code>super()</code> method we call the <code>__init__()</code> method from the parent Node class that our <code>SimplePublisher</code> class is derived from.</p> <p>We provide a name here, which is the name that will be used to register our node on the ROS network (we can call the node anything that we want, but it's a good idea to call it something meaningful).</p> </li> <li> <p>We then use the <code>create_publisher()</code> method (inherited from the <code>Node</code> class) in order to provide our node with the ability to publish messages to a ROS Topic. When calling this we provide 3 key bits of information:</p> <ol> <li> <p><code>msg_type</code>: The type of message that we want to publish.</p> <p>In our case, a <code>String</code> message from the <code>example_interfaces.msg</code> module.</p> </li> <li> <p><code>topic</code>: The name of the topic that we want to publish these messages to.</p> <p>This could be an existing topic (in which case, we'd need to make sure we used the correct message type), or a new topic (in which case, the name can be anything we want it to be).</p> <p>In our case, we want to create a new topic on the ROS network called <code>\"my_topic\"</code>.</p> </li> <li> <p><code>qos_profile</code>: A queue size, which is a \"Quality of Service\" (QoS) setting which limits the amount of messages that are queued in a buffer. </p> <p>In our case, we're setting this to <code>10</code>, which is generally appropriate for most of the applications that we'll be working on.</p> </li> </ol> </li> <li> <p>Here, we're calling the <code>create_timer()</code> method, which we'll use to control the rate at which messages are published to our topic. Here we define 2 things:</p> <ol> <li> <p><code>timer_period_sec</code>: The rate at which we want the timer to run. This must be provided as a period, in seconds. In the line above, we have specified a publishing frequency (in Hz):</p> <p><code>publish_rate = 1 # Hz</code></p> <p>So the associated time period (in seconds) is: </p> <p>\\(T = \\frac{1}{f}\\)</p> </li> <li> <p><code>callback</code>: This is a function that will be executed every time the timer elapses at the desired rate (1 Hz). We're specifying a function called <code>timer_callback</code>, which we'll define later on in the code...</p> </li> </ol> </li> <li> <p>Finally, we use the <code>get_logger().info()</code> method to send a Log message to the terminal to inform us that the initialisation of our node is complete.</p> </li> <li> <p>Here we define the timer callback function. Anything in here will execute at the rate that we specified when we created the <code>create_timer()</code> instance before. In our case:</p> <ol> <li>Use the <code>get_clock()</code> method to get the current ROS Time.</li> <li>Instantiate a <code>String()</code> message (defined as <code>topic_msg</code>).</li> <li>Populate this message with data. In our case, a statement that includes the ROS Time, as obtained above.</li> <li>Call the <code>publish()</code> method of our <code>my_publisher</code> object, to actually publish this message to the <code>\"my_topic\"</code> topic.</li> <li>Send the message data to the terminal as a log message as well, so that we can see what it is when our Node is actually running.</li> </ol> </li> <li> <p>With the functionality of our <code>SimplePublisher</code> class now established, we define a <code>main()</code> function for the Node. This will be fairly common to most Python Nodes that we create, with the following 5 key processes:</p> <ol> <li>Initialise the <code>rclpy</code> library.</li> <li>Create an instance of our <code>SimplePublisher()</code> node.</li> <li>\"Spin\" the node to keep it alive so that any callbacks can execute as required (in our case here, just the <code>timer_callback()</code>). </li> <li>Destroy the node once termination is requested (triggered by entering Ctrl+C in the terminal).</li> <li>Shutdown the <code>rclpy</code> library.</li> </ol> </li> <li> <p>Finally, we call the <code>main()</code> function to set everything going. We do this inside an <code>if</code> statement, to ensure that our node is the main executable (i.e. it has been executed directly (via <code>ros2 run</code>), and hasn't been called by another script)</p> </li> </ol>"},{"location":"course/assignment1/part1/publisher/#defining-package-dependencies","title":"Defining Package Dependencies","text":"<p>We're importing a couple of Python libraries into our node here, which means that our package has two dependencies: <code>rclpy</code> and <code>example_interfaces</code>:</p> <pre><code>import rclpy \nfrom rclpy.node import Node\n\nfrom example_interfaces.msg import String\n</code></pre> <p>Its good practice to add these dependencies to your <code>package.xml</code> file. Locate this file (<code>ros2_ws/src/part1_pubsub/package.xml</code>), open it up and find the following line:</p> <pre><code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;\n</code></pre> <p><code>rclpy</code> is therefore already defined as an execution dependency (which means that our package needs this library in order to execute our code), but we need to add <code>example_interfaces</code> as well, so add the following additional line underneath:</p> <pre><code>&lt;exec_depend&gt;example_interfaces&lt;/exec_depend&gt;\n</code></pre> <p>Job done. Save the file and close it.</p>"},{"location":"course/assignment1/part1/publisher/#shebang","title":"The Shebang","text":"<p>The very first line of code looks like a comment, but it is actually a very crucial part of the script:</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>This is called the Shebang, and it tells the operating system which interpreter to use to execute the code. In our case here, it tells the operating system where to find the right Python interpreter that should be used to actually run the code.</p> <p> \u2190 Back to Part 1 </p>"},{"location":"course/assignment1/part1/subscriber/","title":"A Simple Subscriber Node","text":""},{"location":"course/assignment1/part1/subscriber/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>subscriber.py</code> file and (again) make sure you read the annotations to understand how it all works!</p> subscriber.py<pre><code>#!/usr/bin/env python3\n# A simple ROS2 Subscriber\n\nimport rclpy # (1)!\nfrom rclpy.node import Node\n\nfrom example_interfaces.msg import String\n\nclass SimpleSubscriber(Node): # (2)! \n\n    def __init__(self): \n        super().__init__(\"simple_subscriber\") # (3)!\n\n        self.my_subscriber = self.create_subscription(\n            msg_type=String,\n            topic=\"{BLANK}\",\n            callback=self.msg_callback,\n            qos_profile=10,\n        ) # (4)!\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\"\n        ) # (5)!\n\n    def msg_callback(self, topic_message: String): # (6)!\n        # (7)!\n        self.get_logger().info(f\"The '{self.get_name()}' node heard:\") \n        self.get_logger().info(f\"'{topic_message.data}'\")\n\ndef main(args=None): # (8)!\n    rclpy.init(args=args)\n    my_simple_subscriber = SimpleSubscriber()\n    rclpy.spin(my_simple_subscriber)\n    my_simple_subscriber.destroy_node()\n    rclpy.shutdown() \n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>As with our publisher node, we need to import the <code>rclpy</code> client library and the <code>String</code> message type from the <code>example_interfaces.msg</code> library in order to write a Python ROS Node and use the relevant ROS messages:</p> </li> <li> <p>This time, we create a Python Class called <code>SimpleSubscriber()</code> instead, but which still inherits the <code>Node</code> class from <code>rclpy</code> as we did with the Publisher before.</p> </li> <li> <p>Once again, using the <code>super()</code> method we call the <code>__init__()</code> method from the parent Node class that our <code>SimpleSubscriber</code> class is derived from, and provide a name to use to register in on the network.</p> </li> <li> <p>We're now using the <code>create_subscription()</code> method here, which will allow this node to subscribe to messages on a ROS Topic. When calling this we provide 4 key bits of information:</p> <ol> <li> <p><code>msg_type</code>: The type of message that the topic uses (which we could obtain by running the <code>ros2 topic info</code> command).</p> <p>We know (having just created the publisher), that our topic uses <code>String</code> messages (from <code>example_interfaces</code>).</p> </li> <li> <p><code>topic</code>: The name of the topic that we want to listen (or subscribe) to.</p> <p>Fill in the Blank!</p> <p>Replace the <code>{BLANK}</code> in the code above with the name of the topic that our <code>publisher.py</code> node was set up to publish to!</p> </li> <li> <p><code>callback</code>: When building a subscriber, we need a callback function, which is a function that will execute every time a new message is received from the topic.</p> <p>At this stage, we define what this callback function is called (<code>self.msg_callback</code>), and we'll actually define the function itself further down within the Class.</p> </li> <li> <p><code>qos_profile</code>: As before, a queue size to limit the amount of messages that are queued in a buffer. </p> </li> </ol> </li> <li> <p>Print a Log message to the terminal to indicate that the initialisation process has taken place.</p> </li> <li> <p>Here we're defining what will happen each time our subscriber receives a new message. This callback function must have only one argument (other than <code>self</code>), which will contain the message data that has been received:</p> <p>We're also using a Python Type Annotation here too, which informs the interpreter that the <code>topic_message</code> that is received by the <code>msg_callback</code> function will be of the <code>String</code> data type.</p> <p>(All this really does is allow autocomplete functionality to work within our text editor, so that whenever we want to pull an attribute from the <code>toic_message</code> object it will tell us what attributes actually exist within the object.)</p> </li> <li> <p>In this simple example, all we're going to do on receipt of a message is to print a couple of log messages to the terminal, to include: </p> <ol> <li> <p>The name of this node (using the <code>self.get_name()</code> method)</p> </li> <li> <p>The message that has been received (<code>topic_mesage.data</code>)</p> </li> </ol> </li> <li> <p>The rest of this is exactly the same as before with our publisher.</p> </li> </ol>"},{"location":"course/assignment1/part1/subscriber/#dfts","title":"Don't Forget the Shebang!","text":"<p>Remember: don't forget the shebang, it's very important!</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p> \u2190 Back to Part 1 </p>"},{"location":"course/assignment1/part2/move_circle/","title":"A Simple Velocity Control Node (Move Circle)","text":""},{"location":"course/assignment1/part2/move_circle/#the-initial-code","title":"The Initial Code","text":"<p>Start by returning to the <code>publisher.py</code> file from your <code>part1_pubsub</code> package (or return here), and copy the contents into your new <code>move_circle.py</code> file. Then, adapt the code as follows.</p>"},{"location":"course/assignment1/part2/move_circle/#creating-the-move-circle-node","title":"Creating the \"Move Circle\" Node","text":""},{"location":"course/assignment1/part2/move_circle/#imports","title":"Imports","text":"<p>Once again, <code>rclpy</code> and the <code>Node</code> class from the <code>rclpy.node</code> library are vital for any node we create, so the first two imports will remain the same.</p> <p>We need to import the message type used by the <code>/cmd_vel</code> topic here though, in order to be able to format velocity command messages appropriately. We can find all the necessary information about this message type by using the <code>ros2 topic info</code> command:</p> <pre><code>$ ros2 topic info /cmd_vel\nType: geometry_msgs/msg/Twist\n...\n</code></pre> <p>And so our message import becomes:</p> <pre><code>from geometry_msgs.msg import Twist # (1)!\n</code></pre> <ol> <li>In place of: <code>from example_interfaces.msg import String</code></li> </ol>"},{"location":"course/assignment1/part2/move_circle/#change-the-class-name","title":"Change the Class Name","text":"<p>Previously our publisher class was called <code>SimplePublisher()</code>, change this to <code>Circle</code>:</p> <pre><code>class Circle(Node):\n</code></pre>"},{"location":"course/assignment1/part2/move_circle/#initialising-the-class","title":"Initialising the Class","text":"<ol> <li> <p>Initialise the node with an appropriate name:</p> <pre><code>super().__init__(\"move_circle\")\n</code></pre> </li> <li> <p>Change the <code>create_publisher()</code> parameters:</p> <pre><code>self.my_publisher = self.create_publisher(\n    msg_type=??, # (1)!\n    topic=??, # (2)!\n    qos_profile=10,\n)\n</code></pre> <ol> <li>What message type are we using here?</li> <li>What's the topic name?</li> </ol> </li> <li> <p>We'll need to publish velocity commands at a rate of at least 10 Hz, so set this here, and then set up a timer accordingly:</p> <pre><code>publish_rate = 10 # Hz\nself.timer = self.create_timer(\n    timer_period_sec=1/publish_rate, \n    callback=self.timer_callback\n)\n</code></pre> </li> </ol>"},{"location":"course/assignment1/part2/move_circle/#modifying-the-timer-callback","title":"Modifying the Timer Callback","text":"<p>Here, we'll publish our velocity commands:</p> <pre><code>def timer_callback(self):\n    radius = 0.5 # meters\n    linear_velocity = 0.1 # meters per second [m/s]\n    angular_velocity = ?? # radians per second [rad/s] # (1)!\n\n    topic_msg = Twist() # (2)!\n    topic_msg.linear.x = linear_velocity\n    topic_msg.angular.z = angular_velocity\n    self.my_publisher.publish(topic_msg) # (3)!\n\n    self.get_logger().info( # (4)!\n        f\"Linear Velocity: {topic_msg.linear.x:.2f} [m/s], \"\n        f\"Angular Velocity: {topic_msg.angular.z:.2f} [rad/s].\",\n        throttle_duration_sec=1, # (5)!\n    )\n</code></pre> <ol> <li> <p>Having defined the radius of the circle, and the linear velocity that we want the robot to move at, how would we calculate the angular velocity that should be applied?</p> <p>Consider the equation for angular velocity:</p> <p> </p> \\[ \\omega=\\frac{v}{r} \\] </li> <li> <p><code>/cmd_vel</code> uses <code>Twist</code> messages, so we instantiate one here, and assign the linear and angular velocity values (as set above) to the relevant message fields. Remember, we talked about all this here.</p> </li> <li> <p>Once the appropriate velocity fields have been set, publish the message.</p> </li> <li> <p>Publish a ROS Log Message to inform us (in the terminal) of the velocity control values that are being published by the node.</p> </li> <li> <p>Remember in the Odometry Subscriber how we used a counter (<code>self.counter</code>) and an <code>if()</code> statement to control the rate at which these log messages were generated?</p> <p>We can actually achieve exactly the same thing by simply supplying a <code>throttle_duration_sec</code> argument to the <code>get_logger().info()</code> call. Much easier, right?</p> </li> </ol>"},{"location":"course/assignment1/part2/move_circle/#updating-main","title":"Updating \"Main\"","text":"<p>Once again, don't forget to update any relevant parts of the <code>main</code> function to ensure that we're instantiating the <code>Circle()</code> class, spinning and shutting it down correctly.</p> <p> \u2190 Back to Part 2 </p>"},{"location":"course/assignment1/part2/move_square/","title":"Odometry-based Navigation (Move Square)","text":"<p>A combined publisher-subscriber node to achieve odometry-based control...</p> <p>Below you will find a template Python script to show you how you can both publish to <code>/cmd_vel</code> and subscribe to <code>/odom</code> in the same node.  This will help you build a closed-loop controller to make your robot follow a square motion path of size: 1m x 1m. </p> <p>You can publish velocity commands to <code>/cmd_vel</code> to make the robot move, monitor the robot's position and orientation in real-time, determine when the desired movement has been completed, and then update the velocity commands accordingly.  </p>"},{"location":"course/assignment1/part2/move_square/#suggested-approach","title":"Suggested Approach","text":"<p>Moving in a square can be achieved by switching between two different movement states sequentially: Moving forwards and turning on the spot. At the start of each movement step we can read the robot's current odometry, and then use this as a reference to compare to, and to tell us when the robot's position/orientation has changed by the required amount, e.g.:</p> <ol> <li>With the robot stationary, read the odometry to determine its current X and Y position in the environment.</li> <li>Move forwards until the robot's X and Y position indicate that it has moved linearly by 0.5m.</li> <li>Stop moving forwards.</li> <li>Read the robot's odometry to determine its current orientation (\"yaw\"/<code>\u03b8<sub>z</sub></code>).</li> <li>Turn on the spot until the robot's orientation changes by 90\u00b0.</li> <li>Stop turning.</li> <li>Repeat.  </li> </ol>"},{"location":"course/assignment1/part2/move_square/#the-code","title":"The Code","text":"move_square.py <ol> <li>Import the <code>Twist</code> message for publishing velocity commands to <code>/cmd_vel</code>.</li> <li>Import the <code>Odometry</code> message, for use when subscribing to the <code>/odom</code> topic.</li> <li>Import the <code>quaternion_to_euler</code> function from <code>tb3_tools.py</code> to convert orientation from quaternions to Euler angles (about the principal axes).</li> <li> <p>Finally, import some useful mathematical operations (and <code>pi</code>), which may prove useful for this task:</p> <p> Mathematical Operation Python Implementation \\(\\sqrt{a+b}\\) <code>sqrt(a+b)</code> \\(a^{2}+(bc)^{3}\\) <code>pow(a, 2) + pow(b*c, 3)</code> \\(\\pi r^2\\) <code>pi * pow(r, 2)</code> <p></p> <li> <p>Here we establish a <code>Twist</code> message, which we can populate with velocities and then publish to <code>/cmd_vel</code> within the <code>timer_callback()</code> method (in order to make the robot move).</p> </li> <li> <p>Here, we define some variables that we can use to store relevant bits of odometry data while our node is running (and read it back to implement feedback control):</p> <ul> <li><code>self.x</code>, <code>self.y</code> and <code>self.theta_z</code> will be used by the <code>odom_callback()</code> to store the robot's current pose</li> <li><code>self.x0</code>, <code>self.y0</code> and <code>self.theta_z0</code> can be used in the <code>timer_callback()</code> method to keep a record of where the robot was at a given moment in time (and determine how far it has moved since that point)</li> </ul> </li> <li> <p>We'll also need to keep track of how far the robot has travelled (or turned) in order to determine when sufficient movement has taken place to trigger a switch to the alternative state, i.e.:</p> <ul> <li><code>if</code> travelled 1 meter, <code>then</code>: turn</li> <li><code>if</code> turned 90\u00b0, <code>then</code>: move forward</li> </ul> </li> <li> <p>Here we obtain the robot's current orientation (in quaternions) and convert it to Euler angles (in radians) about the principal axes, where:</p> <ul> <li>\"roll\" = <code>\u03b8<sub>x</sub></code></li> <li>\"pitch\" = <code>\u03b8<sub>y</sub></code></li> <li>\"yaw\" = <code>\u03b8<sub>z</sub></code></li> </ul> </li> <li> <p>We're only interested in <code>x</code>, <code>y</code> and <code>\u03b8<sub>z</sub></code>, so we assign these to class variables <code>self.x</code>, <code>self.y</code> and <code>self.theta_z</code>, so that we can access them elsewhere within our <code>Square()</code> class.</p> </li> <li> <p>Sometimes, it can take a few moments for the first topic message to come through, and it's useful to know when that's happened so that you know you are dealing with actual topic data! Here, we're just setting a flag to <code>True</code> once the callback function has executed for the first time (i.e. the first topic message has been received).</p> </li>"},{"location":"course/assignment1/part2/move_square/#alternative-approach-waypoint-tracking","title":"Alternative Approach: Waypoint Tracking","text":"<p>A square motion path can be fully defined by the coordinates of its four corners, and we can make the robot move to each of these corners one-by-one, using its odometry system to monitor its real-time position, and adapting linear and angular velocities accordingly.</p> <p>This is slightly more complicated, and you might want to wait until you have a bit more experience with ROS before tackling it this way.</p> <p> \u2190 Back to Part 2 </p>"},{"location":"course/assignment1/part2/odom_subscriber/","title":"An Odometry Subscriber Node","text":""},{"location":"course/assignment1/part2/odom_subscriber/#the-initial-code","title":"The Initial Code","text":"<p>Having copied the <code>subscriber.py</code> file from your <code>part1_pubsub</code> package, you'll start out with the code discussed here.</p> <p>Let's look at what we need to change now.</p>"},{"location":"course/assignment1/part2/odom_subscriber/#from-simple-subscriber-to-odom-subscriber","title":"From Simple Subscriber to Odom Subscriber","text":""},{"location":"course/assignment1/part2/odom_subscriber/#imports","title":"Imports","text":"<p>We will generally rely on <code>rclpy</code> and the <code>Node</code> class from the <code>rclpy.node</code> library, for most nodes that we will create, so our first two imports will remain the same. </p> <p>We won't be working with <code>String</code> type messages any more however, so we need to replace this line in order to import the correct message type. As we know from earlier in Part 2, the <code>/odom</code> topic uses messages of the type <code>nav_msgs/msg/Odometry</code>:</p> <pre><code>$ ros2 topic info /odom\nType: nav_msgs/msg/Odometry\n...\n</code></pre> <p>This tells us everything we need to know to construct the Python import statement correctly:</p> <pre><code>from nav_msgs.msg import Odometry\n</code></pre> <p>We'll also need to import a handy function that should already exist as an importable module in your <code>part2_navigation</code> package called <code>tb3_tools</code>:</p> <pre><code>from part2_navigation.tb3_tools import quaternion_to_euler\n</code></pre> <p>As the name suggests, we'll use this to convert the raw orientation values from <code>/odom</code> into their Euler Angle representation.</p> Info <p>This module can be found here: <code>part2_navigation/part2_navigation/tb3_tools.py</code>, if you want to have a look.</p>"},{"location":"course/assignment1/part2/odom_subscriber/#change-the-class-name","title":"Change the Class Name","text":"<p>Previously our class was called <code>SimpleSubscriber()</code>, change this to something more appropriate now, e.g.: <code>OdomSubscriber()</code>:</p> <pre><code>class OdomSubscriber(Node):\n</code></pre>"},{"location":"course/assignment1/part2/odom_subscriber/#initialising-the-class","title":"Initialising the Class","text":"<p>The structure of this remains largely the same, we just need to modify a few things: </p> <ol> <li> <p>Change the name that is used to register the node on the ROS Network:</p> <pre><code>super().__init__(\"odom_subscriber\")\n</code></pre> </li> <li> <p>Change the subscription parameters:</p> <pre><code>self.my_subscriber = self.create_subscription(\n    msg_type=Odometry, # (1)!\n    topic=\"odom\", # (2)!\n    callback=self.msg_callback, \n    qos_profile=10,\n)\n</code></pre> <ol> <li><code>/odom</code> uses the Odometry message type (as imported above)</li> <li>The topic name is <code>\"odom\"</code>, of course!</li> </ol> </li> <li> <p>The final thing we'll do inside our class' <code>__init__</code> method (after we've set up the subscriber) is initialise a counter:</p> <pre><code>self.counter = 0 \n</code></pre> <p>The reason for this will be explained shortly...</p> </li> </ol>"},{"location":"course/assignment1/part2/odom_subscriber/#modifying-the-message-callback","title":"Modifying the Message Callback","text":"<p>This is where the changes are a bit more significant:</p> <pre><code>def msg_subscriber(self, topic_message: Odometry): # (1)!\n\n    pose = topic_data.pose.pose # (2)!\n\n    # (3)!\n    pos_x = pose.position.x\n    pos_y = pose.position.y\n    pos_z = pose.position.z\n\n    roll, pitch, yaw = quaternion_to_euler(pose.orientation) # (4)!\n\n    if self.counter &gt; 10: # (5)!\n        self.counter = 0\n        self.get_logger().info(\n            f\"x = {pos_x:.3f} (m), y = ? (m), theta_z = ? (radians)\"\n        ) # (6)!\n    else:\n        self.counter += 1\n</code></pre> <ol> <li>When defining the message callback, modify the type annotation for the <code>topic_message</code> input.</li> <li> <p>There are two key parts to an odometry message: Pose and Twist.</p> <p>We're only really interested in the Pose part of the message here, so grab this first.</p> </li> <li> <p>As we know by now, Pose contains information about both the \"position\" and \"orientation\" of the robot, we extract the position values first and assign them to the variables <code>pos_x</code>, <code>pos_y</code> and <code>pos_z</code>.</p> <p>Position data is provided in meters, so we don't need to do any conversion on this and can use the data directly.</p> </li> <li> <p>Orientation data is in quaternions, so we convert this by passing it to the <code>quaternion_to_euler</code> function that we imported from <code>tb3_tools</code> earlier.</p> <p>This function provides us with the orientation of the robot about its 3 principal axes:</p> <ul> <li><code>\u03b8<sub>x</sub></code>: \"Roll\"</li> <li><code>\u03b8<sub>y</sub></code>: \"Pitch\"</li> <li><code>\u03b8<sub>z</sub></code>: \"Yaw\"</li> </ul> </li> <li> <p>Here we print out the values that we're interested in to the terminal.</p> <p>This callback function will execute every time a new message is published to the <code>odom</code> topic, which occurs at a rate of around 20 times per second (20 Hz).</p> Tip <p>We can use he <code>ros2 topic hz</code> function to tell us this:</p> <pre><code>$ ros2 topic hz /odom\naverage rate: 18.358\nmin: 0.037s max: 0.088s std dev: 0.01444s window: 20\n</code></pre> <p>That's a lot of messages to be printed to the terminal every second! We therefore use an <code>if</code> statement and a <code>counter</code> to ensure that our <code>print</code> statement only executes for 1 in every 10 topic messages instead.</p> </li> <li> <p>Task: Continue formatting the <code>print</code> message to display the three odometry values that are relevant to our robot!  </p> </li> </ol>"},{"location":"course/assignment1/part2/odom_subscriber/#updating-main","title":"Updating \"Main\"","text":"<p>The only thing left to do now is update any relevant parts of the <code>main</code> function to ensure that you are instantiating, spinning and shutting down your node correctly.</p> <p> \u2190 Back to Part 2 </p>"},{"location":"course/assignment1/part3/lidar_subscriber/","title":"Building a Basic LaserScan Subscriber Node","text":"<p>Copy all the code below into your <code>lidar_subscriber.py</code> file and then review the annotations to understand how it all works.</p> lidar_subscriber.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.signals import SignalHandlerOptions # (1)!\n\nfrom sensor_msgs.msg import LaserScan # (2)!\n\nimport numpy as np # (3)!\n\nclass LidarSubscriber(Node): \n\n    def __init__(self): \n        super().__init__(\"lidar_subscriber\")\n\n        self.lidar_sub = self.create_subscription(\n            msg_type=LaserScan,\n            topic=\"/scan\",\n            callback=self.lidar_callback,\n            qos_profile=10,\n        ) # (4)!\n\n        self.get_logger().info(f\"The '{self.get_name()}' node is initialised.\")\n\n    def lidar_callback(self, scan_data: LaserScan): \n        left = scan_data.ranges[0:21] \n        right = scan_data.ranges[-20:] # (5)!\n        front = np.array(right + left) # (6)!\n\n        valid_data = front[front != float(\"inf\")] # (7)!\n        if np.shape(valid_data)[0] &gt; 0: # (8)!\n            single_point_average = valid_data.mean() # (9)!\n        else:\n            single_point_average = float(\"nan\") # (10)!\n\n        self.get_logger().info(\n            f\"LiDAR Reading (front): {single_point_average:.3f} meters.\",\n            throttle_duration_sec = 1,\n        ) # (11)!\n\ndef main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    )\n    node = LidarSubscriber()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print(\"Shutdown request (Ctrl+C) detected...\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown() \n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li>None of this should be new to you by now. Remember from Part 2 that we're using <code>SignalHandlerOptions</code> to handle shutdown requests (triggered by Ctrl+C).</li> <li>We're building a <code>/scan</code> subscriber here, and we know that this topic uses the <code>sensor_msgs/msg/LaserScan</code> interface type, so we import this here.</li> <li><code>numpy</code> is a Python library that allows us to work with numeric data, very useful for big arrays like <code>ranges</code>.</li> <li>We construct a subscriber in much the same way as we have done in Parts 1 and 2, this time targetting the <code>/scan</code> topic though.</li> <li> <p>From the front of the robot, we obtain a 20\u00b0 arc of scan data either side of the x-axis (see the figure below).</p> </li> <li> <p>Then, we combine the <code>left</code> and <code>right</code> data arrays, and convert this from a Python list to a <code>numpy</code> array (see the figure below).</p> </li> <li> <p>This illustrates one of the great features of <code>numpy</code> arrays: we can filter them.</p> <p>Remember that <code>front</code> is now a <code>numpy</code> array containing 40 data points.</p> <p>Remember also, that there will typically be several <code>inf</code> values scattered around the LaserScan array, resulting from sensor readings that are outside the sensor's measurement range (i.e. greater than <code>range_max</code> or less than <code>range_min</code>). We need to get rid of these, so we ask <code>numpy</code> to filter our array as follows:</p> <ol> <li> <p>Of all the values in the <code>front</code> array, determine which ones are not equal to <code>inf</code>: </p> <p><code>front != float(\"inf\")</code></p> </li> <li> <p>Use this filter to remove these <code>inf</code> values from the <code>front</code> array:</p> <p><code>front[front != float(\"inf\")]</code></p> </li> <li> <p>Return this as a new <code>numpy</code> array called <code>valid_data</code>:</p> <p><code>valid_data = front[front != float(\"inf\")]</code></p> </li> </ol> </li> <li> <p>In certain situations (i.e. in very sparse environments) all values could be equal to<code>inf</code> (imagine the \"empty world\" simulation). Here we're checking the size of the <code>valid_data</code> array to make sure that we haven't just removed all values through the above filtering process!</p> </li> <li> <p>If the array is not empty, then use the <code>mean()</code> method to determine the average of all readings within the dataset</p> </li> <li>If the array is empty, then return \"not a number\" (aka \"nan\") instead </li> <li> <p>Print this value to the terminal, but throttle the messages so that only one is displayed every second</p> <p>Question</p> <p>If we didn't throttle this, what rate would the messages be printed at?</p> </li> </ol> <p>The data processing is illustrated in the figure below:</p> <p></p> <p> \u2190 Back to Part 3 </p>"},{"location":"course/assignment1/part4/move_client/","title":"Part 4 Move Service-Client","text":"<p>Copy all the code below into your <code>move_client.py</code> file and review the annotations to understand how it all works.</p> <p>DFTS!!</p> <p>(Don't forget the shebang!) <pre><code>#!/usr/bin/env python3\n</code></pre></p> move_client.py<pre><code>\n</code></pre>"},{"location":"course/assignment1/part4/move_server/","title":"Part 4 Move Service-Server","text":"<p>Copy all the code below into your <code>move_server.py</code> file and review the annotations to understand how it all works.</p> <p>Remember</p> <p>Don't forget the shebang!</p> <pre><code>#!/usr/bin/env python3\n</code></pre> move_server.py<pre><code>\n</code></pre>"},{"location":"course/assignment1/part6/line_follower/","title":"Part 6 Line Following (Setup)","text":"<p>Use this code as a starting point for Part A of the Line Following exercise.</p> line_follower.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.signals import SignalHandlerOptions\n\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Twist\n\nclass LineFollower(Node):\n\n    def __init__(self):\n        super().__init__(\"line_follower\")\n\n        self.camera_sub = self.create_subscription(\n            msg_type=Image,\n            topic=\"/camera/image_raw\",\n            callback=self.camera_callback,\n            qos_profile=10,\n        )\n\n        self.vel_pub = self.create_publisher(\n            msg_type=Twist,\n            topic=\"/cmd_vel\",\n            qos_profile=10\n        )\n\n        self.vel_cmd = Twist()\n        self.shutdown = False\n\n    def shutdown_ops(self):\n        self.get_logger().info(\n            \"Shutting down...\"\n        )\n        cv2.destroyAllWindows()\n        for i in range(5):\n            self.vel_pub.publish(Twist())\n        self.shutdown = True\n\n    def camera_callback(self, img_data):\n        cvbridge_interface = CvBridge()\n        try:\n            cv_img = cvbridge_interface.imgmsg_to_cv2(\n                img_data, desired_encoding=\"bgr8\")\n        except CvBridgeError as e:\n            self.get_logger().warn(f\"{e}\")\n\n        cv2.imshow(\"camera image\", cv_img)\n\n        height, width, _ = cv_img.shape\n        ## TODO 1 (1)\n\n        ## TODO 2 (2)\n\n        ## TODO 3 (3)\n\n        cv2.waitKey(1)\n\ndef main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    )\n    node = LineFollower()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\n            f\"{node.get_name()} received a shutdown request (Ctrl+C)\"\n        )\n    finally:\n        node.shutdown_ops()\n        while not node.shutdown:\n            continue\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>Image Cropping </p> <p>Apply some cropping to the raw camera image (<code>cv_img</code>). </p> <p>Crop it to around 1/5 of its original height, and to a width so that the pink line is just visible at the edge of the image. </p> <p>Call your new cropped image something like <code>cropped_img</code>. You could then use the <code>cv2.imshow()</code> method to display this in an additional pop-up window when the node is run: </p> <pre><code>cv2.imshow(\"cropped_image\", cropped_img)\n</code></pre> <p> </p> </li> <li> <p>Colour Detection</p> <p>Filter the cropped image by selecting appropriate HSV values so that the pink line can be isolated from the rest of the image.</p> <p>You may need to use the <code>tuos_examples\\image_colours.py</code> node again to help you identify the correct Hue and Saturation value range.</p> <p>Use <code>cv2.cvtColor()</code> to convert your <code>cropped_img</code> into an HSV colour representation:</p> <pre><code>hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\n</code></pre> <p>Use <code>cv2.inRange()</code> to create a mask with the HSV value range that you have determined:</p> <pre><code>line_mask = cv2.inRange(\n    hsv_img, {lower_hsv_values}, {upper_hsv_values}\n)\n</code></pre> <p>And then use <code>cv2.bitwise_and()</code> to create a new image with the mask applied, so that the coloured line is isolated:</p> <pre><code>line_isolated = cv2.bitwise_and(\n    cropped_img, cropped_img, mask = line_mask\n)\n</code></pre> <p> </p> </li> <li> <p>Locating the line</p> <p>Finally, find the horizontal position of the line in the robot's viewpoint.</p> <p>Calculate the image moments of the pink colour blob that represents the line (<code>line_mask</code>) using the <code>cv2.moments()</code> method. Remember that it's the \\(c_{y}\\) component that we're interested in here:</p> \\[ c_{y}=\\dfrac{M_{10}}{M_{00}} \\] <p>Ultimately, this will provide us with the feedback signal that we can use for a proportional controller that we will implement in the next part of the exercise.</p> <p>Once you've obtained the image moments (and <code>cy</code>), use <code>cv2.circle()</code> to mark the centroid of the line on the filtered image (<code>line_isolated</code>) with a circle. For this, you'll also need to calculate the \\(c_{z}\\) component of the centroid:</p> \\[ c_{z}=\\dfrac{M_{01}}{M_{00}} \\] <p>Remember that once you've done all this you can display the filtered image of the isolated line (with the circle to denote the centroid location) using <code>cv2.imshow()</code> again:</p> <pre><code>cv2.imshow(\"filtered line\", line_isolated)\n</code></pre> <p> </p> </li> </ol> <p> \u2190 Back to Part 6 - Exercise 4 (Part A) </p>"},{"location":"course/assignment1/part6/object_detection/","title":"Part 6 Object Detection Node","text":"<p>Copy all the code below into your <code>object_detection.py</code> file, and make sure you read the annotations!</p> object_detection.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy \nfrom rclpy.node import Node # (1)!\n\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError # (2)!\n\nfrom sensor_msgs.msg import Image # (3)!\n\nfrom pathlib import Path # (4)!\n\nclass ObjectDetection(Node):\n\n    def __init__(self): # (5)!\n        super().__init__(\"object_detection\")\n\n        self.camera_sub = self.create_subscription(\n            msg_type=Image,\n            topic=\"/camera/image_raw\",\n            callback=self.camera_callback,\n            qos_profile=10\n        )\n\n        self.waiting_for_image = True # (6)!\n\n    def camera_callback(self, img_data): # (7)!\n        cvbridge_interface = CvBridge() # (8)!\n        try:\n            cv_img = cvbridge_interface.imgmsg_to_cv2(\n                img_data, desired_encoding=\"bgr8\"\n            ) # (9)!\n        except CvBridgeError as e:\n            self.get_logger().warning(f\"{e}\")\n\n        if self.waiting_for_image: # (10)!\n            height, width, channels = cv_img.shape\n\n            self.get_logger().info(\n                f\"Obtained an image of height {height}px and width {width}px.\"\n            )\n\n            self.show_image(img=cv_img, img_name=\"step1_original\")\n\n            self.waiting_for_image = False # (15)!\n            cv2.destroyAllWindows() # (16)!\n\n    def show_image(self, img, img_name, save_img=True): # (11)!\n\n        self.get_logger().info(\"Opening the image in a new window...\")\n        cv2.imshow(img_name, img) # (12)!\n\n        if save_img: # (13)!\n            self.save_image(img, img_name)\n\n        self.get_logger().info(\n            \"IMPORTANT: Close the image pop-up window to exit.\"\n        )\n\n        cv2.waitKey(0) # (14)!\n\n    def save_image(self, img, img_name): # (17)!\n        self.get_logger().info(f\"Saving the image...\")\n\n        base_image_path = Path.home().joinpath(\"myrosdata/object_detection/\")\n        base_image_path.mkdir(parents=True, exist_ok=True) # (18)!\n        full_image_path = base_image_path.joinpath(\n            f\"{img_name}.jpg\") # (19)!\n\n        cv2.imwrite(str(full_image_path), img) # (20)!\n\n        self.get_logger().info(\n            f\"\\nSaved an image to '{full_image_path}'\\n\"\n            f\"  - image dims: {img.shape[0]}x{img.shape[1]}px\\n\"\n            f\"  - file size: {full_image_path.stat().st_size} bytes\"\n        ) # (21)!\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetection()\n    while node.waiting_for_image:\n        rclpy.spin_once(node) # (22)!\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>Nothing new here, moving on...</p> </li> <li> <p>We're importing the OpenCV library for Python (remember the Python API that we talked about earlier), which is called <code>cv2</code>, and also that ROS-to-OpenCV bridge interface that we talked about earlier too: <code>cv_bridge</code>.</p> <p>From <code>cv_bridge</code> we're importing the <code>CvBridge</code> and <code>CvBridgeError</code> classes from the <code>cv_bridge</code> library specifically.</p> </li> <li> <p>We need to subscribe to an image topic in order to obtain the data being published to it. You should've already identified the type of interface that is published to the <code>/camera/image_raw</code> topic, so we import that interface type here (from the <code>sensor_msgs</code> package) so that we can build a subscriber to the topic later.</p> </li> <li> <p>We're also importing the Python <code>Path</code> class from the <code>pathlib</code> module. A very handy tool for doing file operations.</p> </li> <li> <p>Initialising our <code>ObjectDetection()</code> Class (should be very familiar to you by now):</p> <ol> <li>Giving our node a name.</li> <li>Creating a subscriber to the <code>/camera/image_raw</code> topic, providing the interface type used by the topic (<code>sensor_msgs/msg/Image</code> - as imported above), and pointing it to a callback function (<code>camera_callback</code>, in this case), to define the processes that should be performed every time a message is obtained on this topic (in this case, the messages will be our camera images)</li> </ol> </li> <li> <p>We're creating a flag to indicate whether the node has obtained an image yet or not. For this exercise, we only want to obtain a single image, so we will set the <code>waiting_for_image</code> flag to <code>False</code> once an image has been obtained and processed, to avoid capturing any more. </p> <p>This flag will then be used to shut down the node when it's done its job.</p> </li> <li> <p>Here, we're defining a callback function for our <code>self.camera_sub</code> subscriber...</p> </li> <li> <p>Here, we create an instance of the <code>CvBridge</code> class that we imported earlier, and which we'll use later on to convert ROS image data into a format that OpenCV can understand.</p> </li> <li> <p>We're using the CvBridge interface to take our ROS image data and convert it to a format that OpenCV will be able to understand.  In this case we are specifying conversion (or \"encoding\") to an 8-bit BGR (Blue-Green-Red) image format: <code>\"bgr8\"</code>.</p> <p>We contain this within a <code>try</code>-<code>except</code> block though, which is the recommended procedure when doing this.  Here we try to convert an image using the desired encoding, and if a <code>CvBridgeError</code> is raised then we print a warning message to the terminal.  Should this happen, this particular execution of the camera callback function will stop.</p> </li> <li> <p>Then we check the <code>waiting_for_image</code> flag to see if this is the first image that has been received by the node.  If so, then:</p> <ol> <li>Obtain the height and width of the image (in pixels), as well as the number of colour channels.</li> <li>Print a log message containing the image dimensions.</li> <li>Pass the image data to the <code>show_image()</code> function (defined below). We also pass a descriptive name for the image to this function too (<code>img_name</code>).</li> </ol> </li> <li> <p>This class method presents the image to us in a pop-up window and also calls another method which saves the image to file for us.    </p> </li> <li> <p>Display the actual image in a pop-up window:</p> <ol> <li>The image data is passed into the function via the <code>img</code> argument,</li> <li>We need to give the pop-up window a name, so in this case we are using the <code>img_name</code> argument that is passed into this class method.</li> </ol> </li> <li> <p>The <code>show_image()</code> class method has a <code>save_img</code> argument, which is set to <code>True</code> by default, so that this <code>if</code> condition is triggered, and another class method is called to save the image to file.    </p> </li> <li> <p>We're supplying a value of <code>0</code> here, which tells this function to keep this window open indefinitely and wait until it is closed manually before allowing our <code>show_image()</code> class method to complete.</p> <p>If we had supplied a value here (say: <code>1</code>) then the function would simply wait for 1 millisecond and then close the pop-up window down. In our case however, we want some time to actually look at the image and then close the window down ourselves, manually. </p> <p>Once the window has been closed, the execution of our code is able to continue...    </p> </li> <li> <p>We then set the <code>waiting_for_image</code> flag to <code>False</code> so that we only ever perform these processing steps once (we only want to capture a single image).  This will then trigger the main <code>while</code> loop to stop (see below), thus causing the overall execution of the node to stop too.</p> </li> <li> <p><code>cv2.destroyAllWindows()</code> ensures that any OpenCV image pop-up windows that may still be active or in memory are destroyed before the class method exits (and the node shuts down).     </p> </li> <li> <p>This class method handles the saving of the image to a file using <code>cv2</code> tools and <code>pathlib</code>.</p> </li> <li> <p>Here, we define a filesystem location to save images to. </p> <p>We want this to exist in a folder called \"<code>myrosdata/object_detection</code>\" in the home directory, so we can use Pathlib's <code>Path.home().joinpath(...)</code> to define it (a handy way to access the User's home directory, without needing to know the Users name).</p> <p>Then, we use the Pathlib <code>Path.mkdir()</code> method to create this directory if it doesn't exist already.    </p> </li> <li> <p>A full file path is constructed for the image here (using the <code>Path.joinpath()</code> method), based on:</p> <ol> <li>The <code>base_image_path</code> that we defined above </li> <li>An image name that is passed into this class method via the <code>img_name</code> argument.</li> </ol> </li> <li> <p>This saves the image to a <code>.jpg</code> file.  We're supplying the <code>full_image_path</code> that was created above, and also the actual image data (<code>self.cv_img</code>) so that the function knows what image we want to save.</p> </li> <li> <p>We're printing a log message to the terminal to inform us of:</p> <ol> <li>Where the image has been saved to</li> <li>How big the image is (in terms of its pixel dimensions)</li> <li>How big the image file is (in bytes).</li> </ol> </li> <li> <p>We're using <code>spin_once()</code> inside a <code>while</code> loop here so that we can keep an eye on the value of the <code>wait_for_image</code> flag, and stop spinning (i.e. break out of the <code>while</code> loop) once it turns <code>False</code>.</p> </li> </ol> <p> \u2190 Back to Part 6 - Exercise 2 </p>"},{"location":"course/assignment1/part6/object_detection_complete/","title":"Part 6 Object Detection Node (Complete)","text":"<p>Here's a full example of the <code>object_detection.py</code> node that you should have developed during Part 6 Exercise 2.  Also included here is an illustration of how to use the <code>cv2.circle()</code> method to create a marker on an image illustrating the centroid of the detected feature, as discussed here.</p> object_detection_complete.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy \nfrom rclpy.node import Node\n\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError\n\nfrom sensor_msgs.msg import Image\n\nfrom pathlib import Path\n\nclass ObjectDetection(Node):\n\n    def __init__(self):\n        super().__init__(\"object_detection\")\n\n        self.camera_sub = self.create_subscription(\n            msg_type=Image,\n            topic=\"/camera/image_raw\",\n            callback=self.camera_callback,\n            qos_profile=10\n        )\n\n        self.waiting_for_image = True\n\n    def camera_callback(self, img_data):\n        cvbridge_interface = CvBridge()\n        try:\n            cv_img = cvbridge_interface.imgmsg_to_cv2(\n                img_data, desired_encoding=\"bgr8\"\n            )\n        except CvBridgeError as e:\n            self.get_logger().warning(f\"{e}\")\n\n        if self.waiting_for_image:\n            height, width, channels = cv_img.shape\n\n            self.get_logger().info(\n                f\"Obtained an image of height {height}px and width {width}px.\"\n            )\n\n            self.show_image(img=cv_img, img_name=\"step1_original\")\n\n            crop_width = width - 400\n            crop_height = 400\n            crop_y0 = int((width / 2) - (crop_width / 2))\n            crop_z0 = int((height / 2) - (crop_height / 2))\n            cropped_img = cv_img[\n                crop_z0:crop_z0+crop_height, \n                crop_y0:crop_y0+crop_width\n            ]\n\n            self.show_image(img=cropped_img, img_name=\"step2_cropping\")\n\n            hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\n            lower_threshold = (115, 225, 100)\n            upper_threshold = (130, 255, 255)\n            img_mask = cv2.inRange(hsv_img, lower_threshold, upper_threshold)\n\n            self.show_image(img=img_mask, img_name=\"step3_image_mask\")\n\n            filtered_img = cv2.bitwise_and(cropped_img, cropped_img, mask = img_mask)\n\n            # Finding the Image Centroid: (1) \n            m = cv2.moments(img_mask) # (2)!\n            cy = m['m10'] / (m['m00'] + 1e-5)\n            cz = m['m01'] / (m['m00'] + 1e-5) # (3)!\n            cv2.circle(\n                filtered_img,\n                (int(cy), int(cz)),\n                10, (0, 0, 255), 2\n            ) # (4)!\n\n            self.show_image(img=filtered_img, img_name=\"step4_filtered_image\")\n\n            self.waiting_for_image = False\n            cv2.destroyAllWindows()\n\n    def show_image(self, img, img_name, save_img=True):\n\n        self.get_logger().info(\"Opening the image in a new window...\")\n        cv2.imshow(img_name, img)\n\n        if save_img:\n            self.save_image(img, img_name)\n\n        self.get_logger().info(\n            \"IMPORTANT: Close the image pop-up window to exit.\"\n        )\n\n        cv2.waitKey(0)\n\n    def save_image(self, img, img_name):\n        self.get_logger().info(f\"Saving the image...\")\n\n        base_image_path = Path.home().joinpath(\"myrosdata/object_detection/\")\n        base_image_path.mkdir(parents=True, exist_ok=True)\n        full_image_path = base_image_path.joinpath(\n            f\"{img_name}.jpg\")\n\n        cv2.imwrite(str(full_image_path), img)\n\n        self.get_logger().info(\n            f\"\\nSaved an image to '{full_image_path}'\\n\"\n            f\"  - image dims: {img.shape[0]}x{img.shape[1]}px\\n\"\n            f\"  - file size: {full_image_path.stat().st_size} bytes\"\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetection()\n    while node.waiting_for_image:\n        rclpy.spin_once(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>Everything here should be familiar to you from earlier in this exercise, except for this section...</p> </li> <li> <p>Here, we obtain the moments of our colour blob by providing the boolean representation of it (i.e. the <code>img_mask</code>) to the <code>cv2.moments()</code> function.</p> </li> <li> <p>Then, we are determining where the central point of this colour blob is located by calculating the <code>cy</code> and <code>cz</code> coordinates of it.  This provides us with pixel coordinates relative to the top left-hand corner of the image.</p> </li> <li> <p>Finally, this function allows us to draw a circle on our image at the centroid location so that we can visualise it.  Into this function we pass:</p> <ol> <li>The image that we want the circle to be drawn on.  In this case: <code>filtered_img</code>.</li> <li>The location that we want the circle to be placed, specifying the horizontal and vertical pixel coordinates respectively: <code>(int(cy), int(cz))</code>.</li> <li>How big we want the circle to be: here we specify a radius of 10 pixels.</li> <li>The colour of the circle, specifying this using a Blue-Green-Red colour space: <code>(0, 0, 255)</code> (i.e.: pure red in this case)</li> <li>Finally, the thickness of the line that will be used to draw the circle, in pixels.</li> </ol> </li> </ol> <p> \u2190 Back to Part 6 - Exercise 2 </p>"},{"location":"course/assignment2/","title":"Assignment #2: Team Robotics Project","text":""},{"location":"course/assignment2/#overview","title":"Overview","text":"<p>In Assignment #2 you will put into practice everything that you are learning about ROS in Assignment #1, and explore the capabilities of the framework further.</p> <p>You will attend a 2-hour lab session per week in Diamond Computer Room 5 for the full 12-week semester. You will work in teams to develop ROS Nodes for our TurtleBot3 Waffles that allow them to successfully complete a number of robotics tasks in a real-world environment. </p> <p>Assignment #2 is split into two parts: Part A and Part B. You will complete Part A in the first half of the semester (Weeks 1-6) and then move on to Part B in the second half of the semester (Weeks 7-12). </p>"},{"location":"course/assignment2/#the-tasks","title":"The Tasks","text":"<p> Part Tasks Marks(/100) Submission A Task 1: Velocity ControlTask 2: Avoiding Obstacles 2020 Week 6 B Tasks 3 &amp; 4:More Details Coming Soon... 60 Week 12 <p></p> <p>As shown above, there are four tasks in total that you must complete for Assignment #2, worth a total of 100 marks overall. Exact submission deadlines will be stated on Blackboard.</p> <p>Tasks 1, 2 &amp; 3 are programming tasks, requiring you to develop ROS nodes for the Waffles in order for them to complete certain real-world objectives in the robot arena in Computer Room 5. All three of these tasks will be marked based on how well the robot completes each of the objectives. Task 4 will require you to produce some documentation to describe your approach to Task 3.</p>"},{"location":"course/assignment2/#assessment","title":"Assessment","text":"<p>This assignment is worth 30% of the overall mark for COM2009. As a team you will be assessed on a ROS package that you develop to satisfy the above tasks.</p> <p>For Tasks 1, 2 &amp; 3: Each submission will be assessed by deploying your ROS package on one of the robotics laptops used extensively throughout the lab sessions. Nodes within your package will then be executed on the laptop to control a real robot in the Diamond Computer Room 5 Robot Arena.</p> <p>For Task 4: Your package must contain a <code>README.md</code> file in the root of the package directory. Further details on the structure of this, and the marking criteria will be made available in due course... </p>"},{"location":"course/assignment2/#submissions","title":"Submissions","text":"<p>Before you get started on any of the tasks, as a team you'll need to create a single ROS package and host this on GitHub (further details on the next page). You can then add all the necessary functionality for each task as you go along. On each of the submission deadlines (as summarised above and detailed on Blackboard) we will clone your teams GitHub repo. </p> <p>Note</p> <p>You should work on each task as a team, and create only one ROS package/GitHub repo per team for this assignment.</p>"},{"location":"course/assignment2/#your-ros-package","title":"Your ROS Package","text":""},{"location":"course/assignment2/#launching-your-code","title":"Launching Your Code","text":"<p>In order to launch the necessary functionality within your package for a given task you will need to include correctly named launch files: <code>task1.launch.py</code>, <code>task2.launch.py</code>, etc. This will allow you to ensure that all the required functionality is executed when your submission is assessed, and also ensures that we know exactly how to launch this functionality in order to assess it. Full details of the requirements for each launch file are provided on the associated task page.</p> <p>Warning</p> <p>It's up to you to ensure that your code launches as intended for a given task. If it doesn't, then you'll be awarded zero marks, so make sure you test it all out prior to submission!</p> <p>For more information on how to create <code>.launch.py</code> files, refer to the following resources:</p> <ol> <li>Assignment #1 Part 3</li> <li>The Launch Files Extras page </li> </ol>"},{"location":"course/assignment2/#key-requirements-penalties","title":"Key Requirements (Penalties...)","text":"<p>In order to be awarded any marks for any task outlined in the table above, you must ensure that the following key requirements are met in regard to the ROS package that you create:</p> <p>TODO</p> <p>Warning</p> <p>Failure to follow these requirements could result in you being awarded zero marks!</p>"},{"location":"course/assignment2/getting-started/","title":"Getting Started (Week 1)","text":"<p>Before you get started on Assignment #2 (as detailed in the pages that follow), you should work through the following tasks in your teams during the first lab in Week 1. </p> <ul> <li> Set Up Your Team's ROS Package</li> <li> Get to Know the Robots</li> </ul> <p>The instructions below will guide you through these key first steps.</p>"},{"location":"course/assignment2/getting-started/#set-up-your-teams-ros-package","title":"Set Up Your Team's ROS Package","text":"<p>As discussed on the Assignment #2 Overview, everything that your team submit for this lab assignment must be contained within a single ROS package. Inside this you will develop all the necessary nodes to make a TurtleBot3 Waffle complete each of the assignment tasks, as well as some documentation to describe your approach. Each task will be assessed by the Teaching Team via launch files that you must also provide within your package. </p> <p>The first step however is to create your team's ROS Package.</p> <p>Your team's package will need to be hosted on GitHub, so each team member will need a GitHub account. Head to GitHub and set up an account if you don't already have one. Use your Sheffield email address (ending in <code>@sheffield.ac.uk</code>) to register<sup>1</sup>.</p>"},{"location":"course/assignment2/getting-started/#creating-your-teams-package-repo-on-github","title":"Creating Your Team's Package Repo (on GitHub)","text":"<p>Nominate only one member of your team to do this bit.</p> <ol> <li>Ensure that you are signed in to your account on GitHub, then go to the <code>ros2_pkg_template</code> Repo. </li> <li> <p>Click on the green <code>Use this template</code> button, and then select <code>Create a new repository</code> from the dropdown menu. </p> <p> </p> <p>You should then be presented with a Create a new repository screen.</p> </li> <li> <p>The name for your repository must be as follows:</p> <pre><code>com2009_teamXX_2025\n</code></pre> <p>... where <code>XX</code> should be replaced with your COM2009 Assignment #2 Team Number. Enter this in the <code>Repository name</code> box.</p> <p>If your team number is less than 10: put a zero before the number, so that the team number is always 2 digits long, e.g.: </p> <ul> <li><code>com2009_team03_2025</code> for Team 3</li> <li><code>com2009_team08_2025</code> for Team 8</li> <li><code>com2009_team15_2025</code> for Team 15</li> </ul> </li> <li> <p>Select <code>Private</code> to make the repository private, then click the green <code>Create repository</code> button. </p> <p> </p> </li> <li> <p>You'll then be directed to your main repository page. From here, click on <code>Settings</code>, then under <code>Access</code> click <code>Collaborators</code>:</p> <p> </p> <p>(You may be prompted for 2FA.)</p> </li> <li> <p>In the <code>Manage access</code> area, click the green <code>Add people</code> button and add <code>tom-howard</code>: </p> <p> </p> </li> <li> <p>Finally, click on the <code>Add people</code> button and add the rest of your team members as collaborators to this repo too.</p> </li> </ol>"},{"location":"course/assignment2/getting-started/#register-your-ros-package-url-with-the-teaching-team","title":"Register Your ROS Package URL with the Teaching Team","text":"<p>Having created your package, you'll need to tell us your GitHub username and the URL to your team's GitHub repository, so that we can access it and pull download your work when the submissions are due.</p> <p>There is a form that you must complete (as a team), to register your ROS package with us for Assignment #2. </p> <p>Access the form here (also available on Blackboard). You must be signed in to your university email account (<code>...@sheffield.ac.uk</code>) to access this. </p> <p>The team member who created the Repo (in the step above) should fill in this form now.</p> <p>Warning</p> <p>Failure to do this (and do it properly) could result in you receiving 0 marks for the assignment tasks!</p>"},{"location":"course/assignment2/getting-started/#initialising-your-teams-ros-package-locally","title":"Initialising Your Team's ROS Package (Locally)","text":"<p>Nominate only one member of your team to do this bit too.</p> <p>You should do this from within your own ROS installation (or WSL-ROS2), rather than on the robotics laptop that you will use to work with the real robots in the lab. Select a team member who has access to their own ROS installation in the lab now (i.e. via a personal laptop), or access WSL-ROS2 using one of the \"WSL-ROS laptops\" that are also available in the lab.</p> <ol> <li> <p>On GitHub, go back to your repository's main page by clicking the <code>&lt;&gt; Code</code> tab at the top-left.</p> </li> <li> <p>Click the green <code>Code</code> button and then, from the dropdown menu, click the  button to copy the remote HTTPS URL of your repo. </p> <p> </p> </li> <li> <p>From your local ROS installation, open a terminal instance and navigate to the <code>src</code> directory of the ROS Workspace:</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> </li> <li> <p>Clone your repo here using the remote HTTPS URL:</p> <pre><code>git clone REMOTE_HTTPS_URL\n</code></pre> </li> <li> <p>Navigate into the package directory using the <code>cd</code> command:</p> <pre><code>cd com2009_teamXX_2025\n</code></pre> <p>(...replacing <code>XX</code> with your COM2009 Assignment #2 Team Number.)</p> </li> <li> <p>Then, run an initialisation script to configure your ROS package appropriately:</p> <pre><code>./init_pkg.sh\n</code></pre> </li> </ol>"},{"location":"course/assignment2/getting-started/#git","title":"Configure Git","text":"<p>Next, you'll need to make sure Git is configured properly in your local ROS installation before you do anything else.</p> <ol> <li> <p>From the same terminal instance as above run the following commands to update your personal details in the global Git config file on your machine:</p> <p><pre><code>git config --global user.name \"your_name\"\n</code></pre> ...replacing <code>your_name</code> with your actual name! E.g.: <code>git config --global user.name \"John Smith\"</code></p> <p><pre><code>git config --global user.email \"your_email_address\"\n</code></pre> ...replacing <code>your_email_address</code> with your actual email address!</p> </li> <li> <p>If you're working in WSL-ROS2 on a University machine, don't forget to run <code>wsl_ros backup</code> to save these changes to your external WSL-ROS2 backup file, so that they will always be restored whenever you run <code>wsl_ros restore</code> in a fresh WSL-ROS2 instance on another machine. </p> <p>Note</p> <p>All team members will actually need to do this bit before interacting with Git!</p> <p>Regardless of which team member is setting up your team's ROS package to begin with, you'll all need to interact with Git for this assignment, and you should therefore each set up your own individual Git configurations (via the steps above) before working individually on your team's ROS package.</p> </li> </ol>"},{"location":"course/assignment2/getting-started/#git-push","title":"Push Your Local ROS Package Back to GitHub","text":"<p>Again, only one member of your team needs to do this bit.</p> <p>Having initialised your team's ROS package, it's now ready for you to start populating with code for the Assignment #2 Tasks! The first step though is to push the changes made in the initialisation step (above) back to GitHub, so that everyone in your team is working from the right starting point. </p> <ol> <li> <p>From the same terminal as above, use the <code>git status</code> command to show you all the changes that have been made to the repo in the initialisation process:</p> <pre><code>git status\n</code></pre> </li> <li> <p>Use <code>git add</code> to stage all these changes for an initial commit:</p> <pre><code>git add .\n</code></pre> <p>Warning</p> <p>Don't forget the <code>.</code> at the end there!</p> </li> <li> <p>Then commit them:</p> <pre><code>git commit -m \"ROS package initialisations complete.\"\n</code></pre> </li> <li> <p>Finally, push the local changes back up the \"remote\" repository on GitHub:</p> <pre><code>git push origin main\n</code></pre> <p>You'll then be asked to enter your GitHub username, followed by a password. This password is not your GitHub account password!  </p> <p>Warning</p> <p>Your GitHub account password won't work here! You'll need to generate a personal access token (classic) and use this instead!</p> </li> <li> <p>All team members should then be able to pull the remote repo into their own ROS Workspaces (<code>cd ~/ros2_ws/src/ &amp;&amp; git clone REMOTE_HTTPS_URL</code>), make contributions and push these back to the remote repo as required (using their own GitHub account credentials and personal access tokens).</p> </li> </ol> <p>You'll need to copy your ROS package onto the Robot Laptops when working on the Real-Robot based tasks, which we'll cover in more detail later. </p>"},{"location":"course/assignment2/getting-started/#getting-to-know-the-real-robots","title":"Getting to Know the Real Robots","text":"<p>Assignment #2 involves extensive work with our real robots, and you'll therefore have access to the robots for every lab session so that you can work on these tasks as you wish. All the details on how the robots work, how to get them up and running and start programming them can be found in the \"Waffles\" section of this course site. You should proceed now as follows (in your teams):</p> <ol> <li>Everyone must complete a health and safety quiz (on Blackboard) before you (or your team) work with the real robots for the first time. Head to Blackboard and do this now, if you haven't already.</li> <li>Each team has been assigned a specific robot (there's a list on Blackboard). When you're ready, speak to a member of the teaching team who will provide you with the robot that has been assigned to you.</li> <li> <p>Work through each page of the \"Waffles\" section of this site (in order):</p> <ul> <li> Read about the hardware.</li> <li> Learn how to launch ROS and get the robots up and running.</li> <li> Work through the Waffle (&amp; ROS) Basics, which will help to get you started and understand how ROS and the robots work.</li> <li> <p> There is also some Essential Information that you must all be aware of when working with the real robots. Work through the further exercises on this page now.</p> </li> <li> <p> Finally, review the Shutdown Procedures and follow the steps here to shut down the robot and power off the robotics laptop at the end of each lab session.</p> </li> </ul> </li> </ol> <ol> <li> <p>As a University of Sheffield student, you can apply for the GitHub Student Developer Pack, which gives you access to a range of developer tools including GitHub Pro. GitHub Pro allows you to have unlimited collaborators on your repositories, which might help you to collaborate on your ROS package with your team.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment2/ros-pkg-tips/","title":"Working with your ROS Package (in the Lab)","text":"<p>Having followed the instructions on the Getting Started page in Week 1, your team's ROS package will be hosted on GitHub, which makes it much easier to collaborate, and transfer between simulation (e.g. WSL-ROS2, for example) and the real hardware in the lab. </p> <p>You'll need to transfer your ROS package to a robot laptop whenever you want to work on a real robot during the labs. </p>"},{"location":"course/assignment2/ros-pkg-tips/#working-with-git-and-github","title":"Working with Git and GitHub","text":"<p>You'll be working with Git and GitHub quite extensively throughout Assignment #2. Hopefully a lot of you will already be quite familiar with these tools, but if not, we would strongly recommend that you have a look at This Course by the University of Sheffield's Research Software Engineering (RSE) Team.</p>"},{"location":"course/assignment2/ros-pkg-tips/#setting-up-ssh-keys","title":"Setting Up SSH Keys","text":"<p>Using SSH keys, you can clone your team's ROS package to the robot laptops, make commits and push these back up to GitHub during the labs, without needing to provide your GitHub username and a personal access token every time. This makes life a lot easier! The following steps describe the process you should follow to achieve this (adapted from GitHub Docs).</p>"},{"location":"course/assignment2/ros-pkg-tips/#step-0-check-if-you-already-have-an-ssh-key-on-the-laptop","title":"Step 0: Check if you already have an SSH Key on the Laptop","text":"<p>These instructions are adapted from this GitHub Docs page.</p> <p>If you're generating an SSH key for the first time, then you can skip this step and go straight to the next section: \"Step 1: Generating an SSH key (on the Laptop)\". If, however, you've already generated an SSH Key on the laptop previously then check it's still there before you go any further by following these steps...</p> <ol> <li> <p>Open a terminal on the laptop.</p> Tip <p>You can use the Ctrl+Alt+T keyboard combination to open a terminal!</p> </li> <li> <p>Use the <code>ls</code> command as follows, to see if your SSH key already exists on the laptop:</p> <pre><code>ls -al ~/.ssh\n</code></pre> <p>This will provide you with a list of all the SSH keys on the laptop. Your team's key should have the same name as your ROS package (if you followed the steps correctly when you created the key previously), and so you should see your key in the list, i.e.: <code>com2009_teamXX_2025.pub</code>.</p> </li> <li> <p>If your key is there, then you're good to go... Either clone your ROS package onto the Laptop if you deleted it the last time you were in the lab, or just navigate back to it and pull down any updates (<code>git pull</code>) if you left it there.</p> <p>Warning</p> <p>We strongly recommend that you delete your team's package from the laptop at the end of each lab session.</p> </li> <li> <p>If you can't see your key in the list, then you'll need to follow all the steps on this page, starting with Step 1: Generating an SSH key (on the Laptop).</p> </li> </ol>"},{"location":"course/assignment2/ros-pkg-tips/#ssh-keygen","title":"Step 1: Generating an SSH key (on the Laptop)","text":"<ol> <li> <p>From a terminal instance on the laptop navigate to the <code>~/.ssh</code> folder:</p> <pre><code>cd ~/.ssh\n</code></pre> </li> <li> <p>Create a new SSH key on the laptop, using your GitHub email address:</p> <pre><code>ssh-keygen -t ed25519 -C \"your.email@sheffield.ac.uk\"\n</code></pre> <p>Replacing <code>your.email@sheffield.ac.uk</code> with your GitHub email address.</p> <p></p> </li> <li> <p>You'll then be asked to \"Enter a file in which to save the key\". This needs to be unique, so enter the name of your ROS package. For the purposes of this example, let's assume yours is called <code>com2009_team99_2025</code>.</p> </li> <li> <p>You'll then be asked to enter a passphrase. This is how you make your SSH key secure, so that no other teams using the same laptop can access and make changes to your team's package/GitHub repo. You'll be asked to enter this whenever you try to commit/push new changes to your ROS package on GitHub. Decide on a passphrase and share this ONLY with your fellow team members. </p> </li> <li> <p>Next, start the laptop's ssh-agent:</p> <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> </li> <li> <p>Add your SSH private key to the laptop's ssh-agent. You'll need to enter the name of the SSH key file that you created in the earlier step (e.g.: <code>com2009_team99_2025</code>)</p> <pre><code>ssh-add ~/.ssh/com2009_team99_2025\n</code></pre> <p>Replacing <code>com2009_team99_2025</code> with the name of your own SSH key file, of course!</p> </li> <li> <p>Then, you'll need to add the SSH key to your account on GitHub...</p> </li> </ol>"},{"location":"course/assignment2/ros-pkg-tips/#step-2-adding-an-ssh-key-to-your-github-account","title":"Step 2: Adding an SSH key to your GitHub account","text":"<p>These instructions are replicated from this GitHub Docs page.</p> <ol> <li> <p>On the laptop, copy the SSH public key that you created in the previous steps to your clipboard.</p> <p>Do this from a terminal on the laptop, using <code>cat</code>:</p> <pre><code>cat ~/.ssh/com2009_team99_2025.pub\n</code></pre> <p>...replacing <code>com2009_team99_2025</code> with the name of your SSH key file.</p> <p>The content of the file will then be displayed in the terminal... copy it from here.</p> <p>Tips</p> <ol> <li>To copy text from inside a terminal window use Ctrl+Shift+C</li> <li> <p>You could also open the file in VS Code and copy it from there:</p> <pre><code>code ~/.ssh/com2009_team99_2025.pub\n</code></pre> </li> </ol> </li> <li> <p>Go to your GitHub account in a web browser. In the upper-right corner of any page, click your profile photo, then click Settings.</p> </li> <li> <p>In the \"Access\" section of the sidebar, click SSH and GPG keys.</p> </li> <li> <p>Click New SSH key.</p> </li> <li> <p>Enter a descriptive name for the key in the \"Title\" field, e.g. <code>com2009_dia-laptop1</code>.</p> </li> <li> <p>Select <code>Authentication Key</code> as the \"Key Type.\"</p> </li> <li> <p>Paste the text from your SSH Public Key file into the \"Key\" field.</p> </li> <li> <p>Finally, click the \"Add SSH Key\" button.</p> </li> </ol>"},{"location":"course/assignment2/ros-pkg-tips/#ssh-clone","title":"Cloning your ROS package onto the Laptop","text":"<p>With your SSH keys all set up, you can now clone your ROS package onto the laptop. </p> <p>There's a ROS Workspace on each of the robot laptops and (much the same as in your own local ROS environment) your package must reside within this workspace!</p> <ol> <li> <p>From a terminal on the laptop, navigate to the ROS Workspace <code>src</code> directory:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Go to your ROS package on GitHub. Click the Code button and then select the SSH option to reveal the SSH address of your repo. Copy this. </p> </li> <li> <p>Head back to the terminal instance on the laptop to then clone your package into the <code>ros2_ws/src/</code> directory using <code>git</code>:</p> <pre><code>git clone REMOTE_SSH_ADDRESS\n</code></pre> <p>Where <code>REMOTE_SSH_ADDRESS</code> is the SSH address that you have just copied from GitHub.</p> </li> <li> <p>Run Colcon to build your package, which is a three-step process:</p> <ol> <li> <p>Navigate into the root of the ROS Workspace:</p> <pre><code>cd ~/ros2_ws\n</code></pre> </li> <li> <p>Run the <code>colcon build</code> command, targetting your package only:</p> <p><pre><code>colcon build --packages-select com2009_team99_2025 --symlink-install\n</code></pre> (...again, replacing <code>com2009_team99_2025</code> with your team's package name.)</p> </li> <li> <p>Then, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Navigate into your package and run the following commands to set your identity, to allow you to make commits to your package repo:</p> <p><pre><code>cd com2009_team99_2025/\n</code></pre> <pre><code>git config user.name \"your name\"\n</code></pre> <pre><code>git config user.email \"your email address\"\n</code></pre></p> </li> </ol> <p>You should then be able to commit and push any updates that you make to your ROS package while working on the laptop, back to your remote repository using the secret passphrase that you defined earlier!</p>"},{"location":"course/assignment2/ros-pkg-tips/#deleting-your-ros-package-after-a-lab-session","title":"Deleting your ROS package after a lab session","text":"<p>Remember that the Robotics Laptops use an account that everyone in the class has access to. You might therefore want to delete your package from the laptop at the end of each lab session. It's very easy to clone it back onto the laptop again by following the steps above at the start of each lab session. Deleting your package (by following the instructions below) won't delete your SSH key from the laptop though, so you won't need to do all that again, and your SSH key will still be protected with the secret passphrase that you set up when generating the SSH Key to begin with (assuming that you are working on the same laptop, of course!) </p> <p>Warning</p> <p>Make sure you've pushed any changes to GitHub before deleting your package!</p> <p>Delete your package by simply running the following command from any terminal on the laptop:</p> <pre><code>rm -rf ~/ros2_ws/src/com2009_teamXX_2025\n</code></pre> <p>... replacing <code>XX</code> with your own team's number!</p>"},{"location":"course/assignment2/ros-pkg-tips/#returning-in-a-subsequent-lab-session","title":"Returning in a Subsequent Lab Session","text":"<p>Your team will be provided with the same Robotics Laptop for each lab session. Having completed all the steps above in a previous lab session, you should be able to return to the laptop, re-clone your package and continue working with relative ease...</p> <ol> <li> <p>The private SSH key that you created in a previous lab session (and secured with a passphrase) should still be saved on the laptop. Check that this is the case by first running the following command:</p> <p><pre><code>ls -al ~/.ssh\n</code></pre> If you can see your team's ssh key in the list then you're good to go. If not you'll need to go back here and follow the steps to create it again.</p> </li> <li> <p>Next, start the laptop's ssh-agent and re-add your team's private key:</p> <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> <pre><code>ssh-add ~/.ssh/com2009_teamXX_2025\n</code></pre> <p>Replacing <code>XX</code> with your team number.</p> </li> <li> <p>Then, navigate to the ROS Workspace <code>src</code> directory:</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> </li> <li> <p>Clone your package into here, using the SSH address of your package on GitHub:</p> <pre><code>git clone REMOTE_SSH_ADDRESS\n</code></pre> <p>You'll be asked for your secret passphrase, hopefully you remember it!</p> </li> </ol>"},{"location":"course/assignment2/part-a/","title":"Assignment #2 Part A","text":"<p>Total Marks: 40/100</p> <p>Submission: Week 6 </p> <p>Tasks:</p> <ul> <li>Task 1: Velocity Control</li> <li>Task 2: Avoiding Obstacles</li> </ul>"},{"location":"course/assignment2/part-a/deadline/","title":"Preparing Your ROS Package for the Week 6 Deadline","text":"<p>This page TODO</p> <ul> <li>make sure the code is on <code>main</code> branch</li> <li>code will be pulled at on the deadline</li> <li>no extensions</li> <li>we'll push a file to your main branch called XX.txt to indicate when your work has been pulled</li> <li> <p>task 1 &amp; 2 launch files - make sure they're there</p> </li> <li> <p>make sure the nodes are defined in CMakeLists.txt</p> </li> <li>make sure everything is executable</li> </ul>"},{"location":"course/assignment2/part-a/deadline/#deductions","title":"Deductions","text":"<ul> <li>package name is correct</li> <li>colcon build fails</li> <li>authors in package.xml</li> <li>description</li> </ul>"},{"location":"course/assignment2/part-a/task1/","title":"Task 1: Velocity Control","text":"<p>Develop a working ROS application to making a real TurtleBot3 Waffle follow a prescribed motion profile, whilst printing key information to the terminal.</p> <p>Course Checkpoints</p> <p>You should aim to have completed the following additional parts of the COM2009 ROS Course to support your work on this task: </p> <ul> <li>Assignment #1: Part 2, up to (and including) Exercise 5.</li> <li>Real Waffle Essential Exercises: Exercise 1 (Publishing Velocity Commands).</li> </ul>"},{"location":"course/assignment2/part-a/task1/#summary","title":"Summary","text":"<p>The main objective of this task is to create a ROS node (or multiple nodes) that make your robot follow a figure-of-eight pattern on the robot arena floor. The figure-of-eight trajectory should be generated by following two loops, both 1 meter in diameter, as shown below. </p> <p> </p> The figure-of-eight path for Task 1. <p>Whilst doing this, you will also need to print some robot odometry data to the terminal at regular intervals (see below for the specifics). In order to get the terminal message formatting right, you might want to have a look at the documentation on Python String Formatting, and refer to any of the code examples that involve printing messages to the terminal in Assignment #1.</p>"},{"location":"course/assignment2/part-a/task1/#details","title":"Details","text":"<ol> <li>The robot must start by moving anti-clockwise, following a circular motion path of 1 m diameter (\"Loop 1,\" as shown in the figure above).</li> <li>Once complete, the robot must then turn clockwise to follow a second circular path, again of 1 m diameter (\"Loop 2\").</li> <li>After Loop 2 the robot must stop, at which point it should be located back at its starting point.</li> <li> <p>The velocity of the robot should be defined to ensure that the whole sequence takes 60 seconds to complete (5 seconds).</p> <p>Note: The timer will start as soon as the robot starts moving.</p> </li> <li> <p>The robot's real-time pose should be printed to the terminal throughout, where messages should be of the following format (exactly): </p> <pre><code>x={x} [m], y={y} [m], yaw={yaw} [degrees].\n</code></pre> <p>Where <code>{x}</code>, <code>{y}</code> and <code>{yaw}</code> should be replaced with the correct real-time odometry data as follows:</p> <ol> <li><code>{x}</code>: the robot's linear position in the X axis, quoted in meters to two decimal places.</li> <li><code>{y}</code>: the robot's linear position in the Y axis, quoted in meters to two decimal places.</li> <li><code>{yaw}</code>: the robot's orientation about the Z axis, quoted in degrees to one decimal place.</li> </ol> <p>The data should be quoted relative to its starting position at the beginning of the task, e.g. at the start of the task (before the robot has moved) the terminal messages should read:</p> <pre><code>x=0.00 [m], y=0.00 [m], yaw=0.0 [degrees].\n</code></pre> <p>These message should be printed to the terminal at a rate of 1Hz. It doesn't matter if the messages continue to be printed to the terminal after the robot has stopped (i.e. after the figure-of-eight has been completed).</p> </li> <li> <p>The ROS package that you submit must contain a launch file called <code>task1.launch.py</code>. When assessing your team's package, the teaching team will use the following command to execute all the necessary functionality from within your package: </p> <pre><code>ros2 launch com2009_teamXX_2025 task1.launch.py\n</code></pre> <p>... where <code>XX</code> will be replaced with your team number.</p> <p>Note: ROS will already be running on the robot before we attempt to execute your launch file. </p> </li> </ol>"},{"location":"course/assignment2/part-a/task1/#a-note-on-odometry","title":"A note on Odometry","text":"<p>When the robot is placed in the arena at the start of the task its odometry may not necessarily read zero, so you will need to compensate for this. You'll therefore need to grab the robot pose from the <code>/odom</code> topic before your robot starts moving, and then use that as the zero-reference to convert all the subsequent odometry readings that you obtain throughout the task.</p> <p>Odometry and keeping track of the robot's pose is discussed in detail in Assignment #1 Part 2.</p>"},{"location":"course/assignment2/part-a/task1/#simulation-resources","title":"Simulation Resources","text":"<p>It's easier to develop your node(s) in simulation before testing things out on a real robot. You can use the standard \"Empty World\" environment to do this, which can be launched in using the following command:</p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>For the real task, there will be cylindrical objects placed at the centre of each of the figure-of-eight loops, so your robot will need to move around these as it completes the task. We have therefore also created a simulation environment that is representative of the real world environment during the assessment. This is available in a package called <code>com2009_simulations</code>, which is part of the <code>tuos_ros</code> Course Repo. The instructions for downloading and installing this within your own local ROS installation are available here.</p> <p>If you've already installed this (as part of Assignment #1 perhaps), then it's worth making sure that you have the most up-to-date version (as discussed here).</p> <p>Once you've done all this, then you should be able to launch the Task 1 development arena with the following <code>ros2 launch</code> command:</p> <pre><code>ros2 launch com2009_simulations task1.launch.py\n</code></pre> <p> </p> The Task 1 development arena. <p>Note</p> <p>There won't be any loop markers on the real robot arena floor during the assessment.</p>"},{"location":"course/assignment2/part-a/task1/#marking","title":"Marking","text":"<p>This task will be assessed by the teaching team as part of Part A (i.e. along with Task 2). This will be assessed during the Easter Holiday period, with feedback returned to you before the semester resumes.</p> <p>There are 20 marks available for this task in total, summarised as follows:</p> <p> Criteria Marks Details A: The Motion Path 10/20 How closely the real robot follows a true figure-of-eight path in the robot arena, based on the criteria table below. B: Terminal Messages 10/20 The correct formatting of your odometry messages, and the validity of the data that is presented in the terminal as the robot performs the task, based on the criteria table below. <p></p>"},{"location":"course/assignment2/part-a/task1/#criterion-a-the-motion-path","title":"Criterion A: The Motion Path","text":"<p>Marks: 10/20</p> <p> Criteria Details Marks A1: Direction of travel The robot must move anticlockwise for the first loop (\"Loop 1\") and then clockwise for the second (\"Loop 2\"). 2 A2: Loop 1 The loop must be ~1 m in diameter, centred about the red beacon. 2 A3: Loop 2 The loop must be ~1 m in diameter, centred about the blue beacon. 2 A4: Stopping Once the robot completes its figure of eight, it must stop with both wheels within 10 cm of the start line. 2 A5: Timing The robot must complete the full figure of eight and stop in 55-65 seconds. 2 <p></p>"},{"location":"course/assignment2/part-a/task1/#criterion-b-terminal-messages","title":"Criterion B: Terminal Messages","text":"<p>Marks: 10/20</p> <p> Criteria Details Marks B1: Rate Messages should be printed to the terminal at a rate of 1 Hz. 2 B2: Format The messages printed to the terminal should be formatted exactly as detailed above. 2 B3: Data Each message value (<code>x</code>, <code>y</code> and <code>yaw</code>) should be plausible, that is: they represent the actual pose of the robot, based on all readings being set to zero at the start/finish point (as illustrated above). In addition, each value must be quoted in the correct units (meters or degrees, as appropriate). 6 <p></p>"},{"location":"course/assignment2/part-a/task2/","title":"Task 2: Avoiding Obstacles","text":"<p>Develop the ROS node(s) to allow a TurtleBot3 Waffle to autonomously explore an environment containing various obstacles. The robot must explore as much of the environment as possible in 90 seconds without crashing into anything!</p> <p>Course Checkpoints</p> <p>You should aim to have completed the following additional parts of the COM2009 ROS Course to support your work on this task: </p> <ul> <li>Assignment #1: Up to and including Part 5 (in full).</li> <li>Real Waffle Essential Exercises:<ul> <li>Exercise 1 (Publishing Velocity Commands),</li> <li>Exercise 2 (Out of Range LiDAR Data).</li> </ul> </li> </ul>"},{"location":"course/assignment2/part-a/task2/#summary","title":"Summary","text":"<p>Assignment #1 Part 3 introduces the Waffle's LiDAR sensor. This sensor is very useful, as it tells us the distance to any objects that are present in the robot's environment. In Assignment #1 Part 5 we look at how this data, in combination with the ROS Action framework, can be used as the basis for a basic exploration strategy that would incorporate obstacle avoidance. Building on this in Part 5 Exercise 6, we discuss how this could be developed further by developing an action client that could make successive calls to the action server to keep the robot moving randomly, and indefinitely, around an arena whilst avoiding obstacles.</p> <p>This is one approach that you could use for this task, but there are other (and potentially simpler) ways that this could be achieved too. </p> <p>In COM2009 Lecture 3 (\"Sensing, Actuation &amp; Control\"), for instance, you are introduced to Cybernetic Control Principles and some of Braitenberg's \"Vehicles,\" which are discussed and implemented on a Lego robot during the lecture! In particular, \"Vehicle 3b\" might well be relevant to consider as a simple method to achieve an obstacle avoidance behaviour.</p> <p>Another aspect of this task is exploration: your robot will be awarded more marks for navigating around more of the environment. Consider the search strategies that are discussed in Lecture 8 (\"Local Guidance Strategies\"), such as \"Brownian Motion\" and \"Levy Walks.\" Could something along these lines be implemented on the  Waffle?</p>"},{"location":"course/assignment2/part-a/task2/#details","title":"Details","text":"<p>The environment that your robot will need to explore for this will (again) be the Diamond Computer Room 5 Robot Arena, which is a square arena of 4x4m. For the task, the arena will contain a number of \"obstacles,\" i.e.: short wooden walls and coloured cylinders. Your robot will need to be able to detect these obstacles and navigate around them in order to fully explore the space.</p> <ol> <li>The robot will start in the centre of the arena.</li> <li> <p>It must explore the environment for 90 seconds without touching any of the arena walls or the obstacles within it.</p> <p>Note: The 90-second timer will start as soon as the robot starts moving within the arena.</p> </li> <li> <p>If the robot makes contact with anything before the time has elapsed then the attempt is over.</p> </li> <li>The arena floor will be divided into 16 equal-sized zones and the robot must enter as many of the outer 12 zones as possible during the attempt.</li> <li> <p>The robot must be moving for the entire duration of the task. Simply just turning on the spot for the whole time doesn't count!</p> <p></p> </li> <li> <p>The ROS package that you submit must contain a launch file called <code>task2.launch.py</code>, such that the functionality that you develop for Task 2 can be launched from your package via the command:</p> <pre><code>ros2 launch com2009_teamXX_2025 task2.launch.py\n</code></pre> <p>Test this out before submission to make sure that it works!</p> </li> <li> <p>ROS will already be running on the robot before we attempt to execute your launch file on the laptop that the robot has been paired with. </p> </li> </ol> <p>Note</p> <p>The location, orientation and quantity of obstacles in the arena will not be revealed beforehand, so the ROS package that you develop will need to be able to accommodate an unknown environment. </p>"},{"location":"course/assignment2/part-a/task2/#simulation-resources","title":"Simulation Resources","text":"<p>Within the <code>com2009_simulations</code> package there is an example arena which can be used to develop and test out your team's obstacle avoidance node(s) for this task.</p> <p>Info</p> <p>Make sure you check for updates to the Course Repo to ensure that you have the most up-to-date version of these simulations.</p> <p>The simulation can be launched using the following <code>ros2 launch</code> command:</p> <pre><code>ros2 launch com2009_simulations task2.launch.py\n</code></pre> <p></p> <p> </p> The Obstacle Avoidance development arena. <p>Warning</p> <p>The location, orientation and quantity of obstacles will be different to those in this simulation!</p>"},{"location":"course/assignment2/part-a/task2/#marking","title":"Marking","text":"<p>There are 20 marks available for Task 2 in total, awarded as follows:</p> <p> Criteria Marks Details A: Run Time 8/20 You will be awarded marks for the amount of time that your robot spends exploring the environment before 90 seconds has elapsed, or the robot makes contact with anything in its environment (as per the table below). The robot must leave the central zone (the red zone in the simulation) in order to be eligible for any of these marks. B: Exploration 12/20 You will be awarded 1 mark for each of the outer 12 arena zones that the robot manages to enter (i.e. excluding the four zones in the middle). The robot only needs to enter each of the 12 zones once, but its full body must be inside the zone marking to be awarded the mark. <p></p>"},{"location":"course/assignment2/part-a/task2/#run-time","title":"Criterion A: Run Time","text":"<p>Marks: 8/20</p> <p>Marks will be awarded as follows:</p> <p> Time (Seconds) Marks 0-9 0 10-19 1 20-29 2 30-39 3 40-49 4 50-59 5 60-89 6 The full 90! 8 <p></p>"},{"location":"course/extras/","title":"Additional Resources","text":""},{"location":"course/extras/#todo","title":"TODO","text":""},{"location":"course/extras/course-repo/","title":"The TUoS ROS Course Repo","text":"<p>A ROS Metapackage called <code>tuos_ros</code> has been put together to support these courses. This package is available on GitHub here. This repo contains the following ROS packages:</p> Package Name Description <code>tuos_examples</code> Some example scripts to support certain exercises in COM2009 Assignment #1 <code>tuos_interfaces</code> Some custom ROS interfaces to support certain exercises in COM2009 Assignment #1 <code>tuos_simulations</code> Some Gazebo simulations to support certain exercises in COM2009 Assignment #1 <code>tuos_tb3_tools</code> Scripts that run on the real Waffles, and some RViz configs to support real robot work too <code>com2009_simulations</code> Simulation resources to support your development work in COM2009 Assignment #2"},{"location":"course/extras/course-repo/#installing","title":"Installing","text":"<p>The <code>tuos_ros</code> course repo is already installed and ready to go in WSL-ROS, for those who use it, and on the Robotics Laptops. To install the packages in your own local ROS installation, follow the steps here.</p> <ol> <li> <p>Navigate to your ROS Workspace <code>src</code> directory:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Clone the repo from GitHub:</p> <pre><code>git clone -b humble https://github.com/tom-howard/tuos_ros.git\n</code></pre> </li> <li> <p>Navigate back one directory, into the root* of the ROS workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Run <code>colcon build</code> to compile the packages:</p> <pre><code>colcon build --packages-up-to tuos_ros\n</code></pre> </li> <li> <p>Then re-source your <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol>"},{"location":"course/extras/course-repo/#verify","title":"Verify","text":"<p>Once you've installed it, then you can verify that the build process has worked using the following command:</p> <pre><code>colcon_cd tuos_ros\n</code></pre> <p>Which should take you to a directory within the repo that's also called <code>tuos_ros</code>, i.e.:</p> <pre><code>~/ros2_ws/src/tuos_ros/tuos_ros\n</code></pre>"},{"location":"course/extras/course-repo/#updating","title":"Updating","text":"<p>The course repo may be updated every now and again, so its worth checking regularly that you have the most up-to-date version. You can do this by pulling down the latest updates from GitHub using <code>git pull</code>:</p> <pre><code>cd ~/ros2_ws/src/tuos_ros/ &amp;&amp; git pull\n</code></pre> <p>If you see the following message:</p> <pre><code>-bash: cd: tuos_ros/: No such file or directory\n</code></pre> <p>... then go back and make sure you've installed the repo first!</p> <p>Then, run <code>colcon build</code> and re-source your environment:</p> <pre><code>cd ~/ros2_ws &amp;&amp; colcon build --packages-up-to tuos_ros &amp;&amp; source ~/.bashrc\n</code></pre>"},{"location":"course/extras/launch-files/","title":"Launch Files","text":""},{"location":"course/extras/launch-files/#todo","title":"TODO","text":""},{"location":"ros/","title":"Installing ROS","text":"<p>Installing ROS2 on your own computer, or accessing it on the University of Sheffield Campus (TODO).</p>"},{"location":"waffles/","title":"Working with the Real TurtleBot3 Waffles","text":"<p>This section of the Course Site contains a range of resources to support your work with our real TurtleBot3 Waffles in the Diamond. </p> <p></p>"},{"location":"waffles/basics/","title":"Waffle (&amp; ROS) Basics","text":"<p>Having completed the steps on the previous page, your robot and laptop should now be paired, and ROS should be up and running (on the robot). You're ready to bring the robot to life! </p> <p>On this page are a series of exercises for you to work through in your teams, to explore how the robots work. We'll also talk through some core ROS concepts and use some key ROS tools, in case you haven't had a chance to explore these in simulation yet.</p>"},{"location":"waffles/basics/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Making the Robot Move</li> <li>Exercise 2: Seeing the Sensors in Action</li> <li>Exercise 3: Visualising the ROS Network</li> <li>Exercise 4: Exploring ROS Topics and Interfaces</li> <li>Exercise 5: Creating A Velocity Control Node (with Python)</li> <li>Exercise 6: Using SLAM to create a map of the environment</li> </ul>"},{"location":"waffles/basics/#prerequisite-robot-laptop-bridging","title":"Prerequisite: Robot-Laptop 'Bridging'","text":"<p>The robot and laptop both communicate over the University network via a Zenoh Bridge. The bridge should already be running on the robot after having run the <code>tb3_bringup</code> command on the robot earlier. The next step is to establish a connection to this bridge from the laptop, so that all ROS nodes, topics etc. can communicate as necessary. </p> <p>Open up a new terminal instance on the laptop (either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon) and enter the following command:</p> <p><pre><code>waffle X bridge\n</code></pre> You should now have two terminals active: </p> <ol> <li>The robot terminal where you ran the <code>tb3_bringup</code> command<sup>1</sup></li> <li>The laptop terminal where you just ran the <code>bridge</code> command (above)</li> </ol> <p>Leave both of these terminals alone, but keep them running in the background at all times while working with your robot.</p>"},{"location":"waffles/basics/#manual-control","title":"Manual Control","text":""},{"location":"waffles/basics/#exMove","title":"Exercise 1: Making the Robot Move","text":"<p>There's a very useful ready-made ROS application called <code>teleop_keyboard</code> (from the <code>turtlebot3_teleop</code> package) that we will use to drive a Waffle around. This node works in exactly the same way in both simulation and in the real-world!</p> <ol> <li> <p>Open up a new terminal instance on the laptop either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon, we'll refer to this as TERMINAL 1. In this terminal enter the following <code>ros2 run</code> command to launch the <code>teleop_keyboard</code> node:</p> <p>TERMINAL 1: <pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around using specific buttons on the keyboard:</p> <p> </p> <p>Warning</p> <p>Take care to avoid any obstacles or other people in the lab as you do this!</p> </li> <li> <p>Once you've spent a bit of time on this, close the application down by entering Ctrl+C in TERMINAL 1.</p> </li> </ol>"},{"location":"waffles/basics/#packages-and-nodes","title":"Packages and Nodes","text":"<p>ROS applications are organised into packages. Packages are basically folders containing scripts, configurations and launch files (ways to launch those scripts and configurations).  </p> <p>Scripts tell the robot what to do and how to act. In ROS, these scripts are called nodes. ROS Nodes are executable programs that perform specific robot tasks and operations. These are typically written in C++ or Python, but it's possible to write ROS Nodes using other programming languages too.</p> <p>There are two key ways to launch ROS applications:</p> <ol> <li><code>ros2 launch</code></li> <li><code>ros2 run</code></li> </ol> <p>Recall that we just used the <code>ros2 run</code> command in Exercise 1 to launch the <code>teleop_keyboard</code> node. This command has the following structure:</p> <pre><code>ros2 run {[1] Package name} {[2] Node name}\n</code></pre> <p>Part [1] specifies the name of the ROS package containing the functionality that we want to execute. Part [2] is used to specify a single script within that package that we want to execute. We therefore use <code>ros2 run</code> commands to launch single executables (aka Nodes) onto the ROS network (in Exercise 1 for example, we launched the <code>teleop_keyboard</code> node).</p> <p>The <code>ros2 launch</code> command has a similar structure:</p> <pre><code>ros2 launch {[1] Package name} {[2] Launch file}\n</code></pre> <p>Here, Part [1] is the same as the <code>ros2 run</code> command, but Part [2] is slightly different: <code>{[2] Launch file}</code>. In this case, Part [2] is a file within that package that specifies any number of Nodes that we want to launch onto the ROS network. We can therefore launch multiple nodes at the same time from a single launch file.</p>"},{"location":"waffles/basics/#sensors-visualisation-tools","title":"Sensors &amp; Visualisation Tools","text":"<p>Our Waffles have some pretty sophisticated sensors on them, allowing them to \"see\" the world around them. Let's now see what our robot sees, using some handy ROS tools.</p>"},{"location":"waffles/basics/#exViz","title":"Exercise 2: Seeing the Sensors in Action","text":"<ol> <li> <p>There shouldn't be anything running in TERMINAL 1 now, after you closed down the <code>teleop_keyboard</code> node (using Ctrl+C) at the end of the previous exercise. Return to this terminal and enter the following command:</p> <p>TERMINAL 1: <pre><code>ros2 launch tuos_tb3_tools rviz.launch.py\n</code></pre></p> <p>This will launch an application called RViz, which is a handy tool that allows us to visualise the data from all the sensors on-board our robots. When RViz opens, you should see something similar to the following:</p> <p> </p> <p>In the bottom left-hand corner of the RViz screen there should be a Camera panel, displaying a live image feed from the robot's camera.</p> No camera images? <ol> <li>In the top-left \"Displays\" panel, scroll down to the \"Camera\" item.</li> <li>Under \"Topic\", find the \"Reliability Policy\" option.</li> <li>From the Drop-down box, change this from \"Best Effort\" to \"Reliable\".</li> </ol> <p> </p> <p>In the main RViz panel you should see a digital render of the robot, surrounded by lots of green dots. This is a representation of the laser displacement data coming from the LiDAR sensor (the black device on the top of the robot). The LiDAR sensor spins continuously, sending out laser pulses into the environment as it does so. When a pulse hits an object it is reflected back to the sensor, and the time it takes for this to happen is used to calculate how far away the object is.</p> <p>The LiDAR sensor spins and performs this process continuously, so a full 360\u00b0 scan of the environment can be generated. This data is therefore really useful for things like obstacle avoidance and mapping. </p> </li> <li> <p>Place your hand in front of the robot and see if the position of the green dots change to match your hand's location. Move your hand up and down and consider at what height the LiDAR sensor is able to detect it.</p> </li> <li> <p>Then, move your hand closer and further away and watch how the green dots move to match this. </p> </li> <li> <p>Open up a new terminal instance (TERMINAL 2) and launch the <code>teleop_keyboard</code> node as you did in Exercise 1. Watch how the data in the RViz screen changes as you drive the robot around a bit.</p> </li> </ol>"},{"location":"waffles/basics/#exNet","title":"Exercise 3: Visualising the ROS Network","text":"<p>Using <code>ros2 run</code> and <code>ros2 launch</code>, as we have done so far, it's easy to end up with a lot of different processes or ROS Nodes running on the network, some of which we will interact with, but others may just be running in the background. It is often useful to know exactly what is running on the ROS network, and there are a few ways to do this.</p> <ol> <li> <p>Open up a new terminal instance now (TERMINAL 3) and from here use the <code>ros2 node</code> command to list the nodes that are currently running:</p> <p>TERMINAL 3: <pre><code>ros2 node list\n</code></pre></p> <p>You may notice up to 4 items in the list.</p> </li> <li> <p>We can visualise the connections between the active nodes by using a ROS node called <code>rqt_graph</code>. Launch this as follows:</p> <p>TERMINAL 3: <pre><code>rqt\n</code></pre></p> <p>A window should then open:</p> <p> </p> </li> <li> <p>From here, we then want to load the Node Graph plugin. From the top menu select <code>Plugins</code> &gt; <code>Introspection</code> &gt; <code>Node Graph</code>.</p> </li> <li> <p>In the window that opens, select <code>Nodes/Topics (active)</code> from the dropdown menu in the top left. </p> <p>What you should then see is a map of all the nodes in the list from above (as ovals), and arrows to illustrate the flow of information between them. This is a visual representation of the ROS network!</p> <p>Items that have a rectangular border are ROS Topics. ROS Topics are essentially communication channels, and ROS Nodes can read (subscribe) or write (publish) to these topics to access sensor data, pass information around the network and make things happen.</p> <p>If the <code>teleop_keyboard</code> Node is still active (in TERMINAL 2) then this graph should show us that the node is publishing messages to a topic called <code>/cmd_vel</code>, which in turn is being subscribed to by the <code>zenoh_bridge_ros2dds</code> Node. The Zenoh Bridge node handles all communication between the robot and the laptop, so this node is tunnelling the data from <code>/cmd_vel</code> to the robot to make it move.</p> </li> </ol> <p>A ROS Robot could have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to communicate and share data with the other nodes on the ROS Network.</p>"},{"location":"waffles/basics/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. Data is published to topics via message interfaces.</p> <p>Let's have a look at this in a bit more detail...</p>"},{"location":"waffles/basics/#exTopicMsg","title":"Exercise 4: Exploring ROS Topics and Interfaces","text":"<p>Much like the <code>ros2 node list</code> command, we can use <code>ros2 topic list</code> to list all the topics that are currently active on the ROS network.</p> <ol> <li> <p>Close down the RQT Graph window if you haven't done so already. This will release TERMINAL 3 so that we can enter commands in it again. Return to this terminal window and enter the following:</p> <p>TERMINAL 3: <pre><code>ros2 topic list\n</code></pre></p> <p>A much larger list of items should be printed to the terminal now. See if you can spot the <code>/cmd_vel</code> item in the list.</p> <p>This topic is used to control the velocity of the robot ('command velocity').</p> </li> <li> <p>Let's find out more about this using the <code>ros2 topic info</code> command.</p> <p>TERMINAL 3: <pre><code>ros2 topic info /cmd_vel\n</code></pre></p> <p>This should provide an output similar to the following: </p> <pre><code>Type: geometry_msgs/msg/Twist\nPublisher count: 1\nSubscription count: 1\n</code></pre> <p>This tells us that the type of data being communicated on the <code>/cmd_vel</code> topic is called: <code>geometry_msgs/msg/Twist</code>. </p> <p>The interface description has three parts:</p> <ol> <li><code>geometry_msgs</code>: The name of the ROS package that this interface belongs to.</li> <li><code>msg</code>: The type of interface. In this case message, but there are other types too. </li> <li><code>Twist</code>: The name of the message interface. </li> </ol> <p>We have just learnt then, that if we want to make the robot move we need to publish <code>Twist</code> messages to the <code>/cmd_vel</code> topic. </p> </li> <li> <p>We can use the <code>ros2 interface</code> command to find out more about the <code>Twist</code> message:</p> <p>TERMINAL 3: <pre><code>ros2 interface show geometry_msgs/msg/Twist\n</code></pre></p> <p>From this, we should obtain the following:</p> <pre><code>Vector3  linear\n        float64 x\n        float64 y\n        float64 z\nVector3  angular\n        float64 x\n        float64 y\n        float64 z\n</code></pre> <p>Let's find out what it all means...</p> </li> </ol>"},{"location":"waffles/basics/#velocity-control","title":"Velocity Control","text":"<p>The motion of any mobile robot can be defined in terms of its three principal axes: <code>X</code>, <code>Y</code> and <code>Z</code>. In the context of our TurtleBot3 Waffle, these axes (and the motion about them) are defined as follows:</p> <p></p> <p>In theory then, a robot can move linearly or angularly about any of these three axes, as shown by the arrows in the figure. That's six Degrees of Freedom (DOFs) in total, achieved based on a robot's design and the actuators it is equipped with. Take a look back at the <code>ros2 interface show</code> output above. Hopefully it's a bit clearer now that these topic messages are formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <pre><code>Vector3  linear\n        float64 x  &lt;-- Forwards (or Backwards)\n        float64 y  &lt;-- Left (or Right)\n        float64 z  &lt;-- Up (or Down)\nVector3  angular\n        float64 x  &lt;-- \"Roll\"\n        float64 y  &lt;-- \"Pitch\"\n        float64 z  &lt;-- \"Yaw\"\n</code></pre> <p>Our TurtleBot3 robot only has two motors, so it doesn't actually have six DOFs! The two motors can be controlled independently, which gives it what is called a \"differential drive\" configuration, but this still only allows it to move with two degrees of freedom in total, as illustrated below.</p> <p></p> <p>It can therefore only move linearly in the x-axis (Forwards/Backwards) and angularly in the z-axis (Yaw). </p>"},{"location":"waffles/basics/#exSimpleVelCtrl","title":"Exercise 5: Creating A Velocity Control Node (with Python)","text":"<p>Important</p> <p>Before you start this, close down RViz (click the \"Close without saving\" button, if asked) and stop the <code>teleop_keyboard</code> node by entering Ctrl+C in TERMINAL 2.</p> <p>As we've seen, making a robot move with ROS is simply a case of publishing the right ROS Interface (<code>geometry_msgs/msg/Twist</code>) to the right ROS Topic (<code>/cmd_vel</code>). Earlier we used the <code>teleop_keyboard</code> node to drive the robot around, a bit like a remote control car. In the background here all that was really happening was that the node was converting our keyboard button presses into velocity commands and publishing these to the <code>/cmd_vel</code> topic.</p> <p>In reality, robots need to be able to navigate complex environments autonomously, which is quite a difficult task, and requires us to build bespoke applications. We can build these applications using Python, and we'll look at the core concepts behind this now by building a simple node that will allow us to make our robot a bit more \"autonomous\". What we will do here forms the basis of the more complex applications that you will learn about in the lab course!</p> <ol> <li> <p>Above we talked about how ROS Nodes should be contained within packages, so let's create one now using a helper script that we've already put together. (This is covered in more detail in the ROS course, but for the purposes of this exercise let's just go ahead and run the script without worrying too much about it!)</p> <p>In TERMINAL 1, navigate to the <code>tuos_ros</code> Course Repo, which is located in the ROS2 Workspace on the laptop:</p> <pre><code>cd ~/ros2_ws/src/tuos_ros/\n</code></pre> <p>Here you'll find the <code>create_pkg.sh</code> helper script. Run this now using the following command to create a new ROS package called <code>waffle_demo</code>:</p> <pre><code>./create_pkg.sh waffle_demo\n</code></pre> </li> <li> <p>Navigate into this new package directory (using <code>cd</code>):</p> <p>TERMINAL 1: <pre><code>cd ../waffle_demo/scripts/ \n</code></pre></p> <p>Info</p> <p><code>..</code> means \"go back one directory,\" so that command above is telling <code>cd</code> to navigate out of the <code>tuos_ros</code> directory (and therefore back to <code>~/ros2_ws/src/</code>), and then go into the <code>waffle_demo</code> package directory from there (and then into the <code>scripts</code> directory within that).</p> </li> <li> <p>Here, create a Python file called <code>square.py</code> using the <code>touch</code> command:</p> <p>TERMINAL 1: <pre><code>touch square.py\n</code></pre></p> </li> <li> <p>You'll need to change the execution permissions for this file in order to be able to run it later on. This is also covered in more depth in the ROS course but, for now, simply run the following command:</p> <p>TERMINAL 1: <pre><code>chmod +x square.py\n</code></pre></p> </li> <li> <p>Now we need to start editing files in our package, and we'll do that using Visual Studio Code (VS Code). </p> <p>First, use <code>cd</code> to navigate back one directory, to get us back to the root of our package:</p> <p>TERMINAL 1: <pre><code>cd ..\n</code></pre></p> <p>Verify that you're in the right place by using the <code>pwd</code> command, which should provide the following output:</p> <pre><code>$ pwd\n/home/student/ros2_ws/src/waffle_demo\n</code></pre> <p>Having confirmed that you're in the right place, open up VS Code in this directory:</p> <pre><code>code .\n</code></pre> <p>Note</p> <p>Don't forget to include the <code>.</code> at the end there, it's important!!</p> </li> <li> <p>Next, we need to add our <code>square.py</code> file as an executable to our package's <code>CMakeLists.txt</code>. </p> <p>In VS Code, open the <code>CMakeLists.txt</code> file that is at the root of the <code>waffle_demo</code> package directory (<code>/home/student/ros2_ws/src/waffle_demo/CMakeLists.txt</code>). </p> <p>Locate the lines (near the bottom of the file) that read:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/minimal_node.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> <p>Replace <code>minimal_node.py</code> with <code>square.py</code> to define this as an executable of your package:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/square.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Next, in the VS Code file explorer, open up the <code>scripts</code> directory and click on the <code>square.py</code> file to open it up in the editor.</p> </li> <li> <p>Paste the following content into the <code>square.py</code> file: </p> square.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node # (1)!\nfrom rclpy.signals import SignalHandlerOptions\n\nfrom geometry_msgs.msg import Twist # (2)!\nfrom math import sqrt, pow, pi # (3)!\n\nclass Square(Node): # (4)!\n\n    def __init__(self):\n        super().__init__(\"square\") # (5)!\n\n        self.vel_pub = self.create_publisher(\n            msg_type=Twist,\n            topic=\"cmd_vel\",\n            qos_profile=10,\n        ) # (6)!\n\n        self.state = 1\n        self.change_state = True\n\n        self.vel = Twist() # (7)!\n\n        ctrl_rate = 10 \n        self.timer = self.create_timer(\n            timer_period_sec=1/ctrl_rate,\n            callback=self.timer_callback,\n        ) # (8)!\n\n        self.timestamp = self.get_clock().now().nanoseconds # (9)!\n        self.shutdown = False\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\"\n        )\n\n    def timer_callback(self): # (10)!\n        time_now = self.get_clock().now().nanoseconds\n        elapsed_time = (time_now - self.timestamp) * 1e-9 # (11)!\n        if self.change_state: # (12)!\n            self.timestamp = self.get_clock().now().nanoseconds\n            self.change_state = False\n            self.vel.linear.x = 0.0\n            self.vel.angular.z = 0.0\n            self.get_logger().info(\n                f\"Changing to state: {self.state}\")\n        elif self.state == 1: # (13)!\n            if elapsed_time &gt; 2:\n                self.state = 2\n                self.change_state = True\n            else:\n                self.vel.linear.x = 0.05\n                self.vel.angular.z = 0.0\n        elif self.state == 2: # (14)!\n            if elapsed_time &gt; 4:\n                self.state = 1\n                self.change_state = True\n            else:\n                self.vel.angular.z = 0.2\n                self.vel.linear.x = 0.0\n\n        self.get_logger().info(\n            f\"Publishing Velocities:\\n\"\n            f\"  linear.x: {self.vel.linear.x:.2f} [m/s] | angular.z: {self.vel.angular.z:.2f} [rad/s].\",\n            throttle_duration_sec=1,\n        )\n        self.vel_pub.publish(self.vel) # (15)!\n\n    def on_shutdown(self): # (17)!\n        for i in range(5):\n            self.vel_pub.publish(Twist())\n        self.shutdown = True\n\ndef main(args=None): # (16)!\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    )\n    node = Square()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.on_shutdown()\n    finally:\n        while not node.shutdown:\n            continue\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li><code>rclpy</code> is the ROS client library for Python. We need this (and the <code>Node</code> class from it) in order to build and run Python ROS nodes.</li> <li>We know from earlier that in order to make a robot move we need to publish messages to the <code>/cmd_vel</code> topic, and that this topic uses the <code>geometry_msgs/msg/Twist</code> interface messages. This is how we import that message into the node, in order to create velocity commands in Python (which we'll get to shortly...)</li> <li> <p>Here we're importing some mathematical operators that could be useful... </p> Mathematical Operation Python Syntax \\(\\sqrt{a+b}\\) <code>sqrt(a+b)</code> \\(a^{2}+(bc)^{3}\\) <code>pow(a, 2) + pow(b*c, 3)</code> \\(\\pi r^2\\) <code>pi * pow(r, 2)</code> </li> <li> <p>All the functionality of our Python node is contained within a Class called <code>Square</code>.    </p> </li> <li>When we initialise this class, we provide a name for our node, which is the name that is used to register it on the ROS network. We're calling this one \"square\".</li> <li>Here we are setting up a publisher to the <code>/cmd_vel</code> topic so that the node can write <code>Twist</code> messages to this topic to make the robot move.</li> <li>We're instantiating a <code>Twist</code> message here and calling it <code>vel</code> (we'll assign velocity values to this later on). A <code>Twist</code> message contains six different components that we can assign values to. Any idea what these six values might represent?  </li> <li>We want our node to run at a rate of 10 times per second (10 Hz), so we create a timer object here, set the timer period (<code>timer_period_sec</code>) and then point the object to a \"callback function\" that is defined later on in the code. This callback function will execute at the rate that we specify with <code>timer_period_sec</code>...</li> <li>What time is it right now? (This will help us to keep track of elapsed time in our main timer callback...)</li> <li>Here we're defining our timer callback function. Everything here will execute at the rate that we specified earlier, so we can encapsulate our main control code in here and be confident that it will execute repeatedly (and indefinitely) at our desired rate.</li> <li>Here we're comparing the time now to the time the last time we checked, to tell us how much time has elapsed since then (converting from nanoseconds to seconds by multiplying by <code>1e-9</code>). We'll use that information to decide what the robot should do...</li> <li>This variable is used to stop the robot (if necessary), check the time again, and then move into a new state.</li> <li>In state <code>1</code> we set velocities that will make the robot move forwards (linear-X velocity only). If the elapsed time is greater than 2 seconds however, we move on to state <code>2</code>.</li> <li>In state <code>2</code> we set velocities that will make the robot turn on the spot (angular-Z velocity only). In this case, if the elapsed time is greater than 4 seconds, we move back to state <code>1</code>.</li> <li>Regardless of what happens in the <code>if</code> statements above, we always publish a velocity command to the <code>/cmd_vel</code> topic here (i.e. every time this timer callback executes).</li> <li>The rest of the code here is \"boilerplate\": a standard approach that we'll use to instantiate our nodes and execute them. You'll learn about all of this throughout the lab course.</li> <li>We're defining a class method here that we can call when our node needs to shut down. We're controlling a robot here, so it's important to make sure the robot stops moving when this happens.</li> </ol> <p>Click on the  icons above to expand the code annotations. Read these carefully to ensure that you understand what's going on and how this code works.</p> </li> <li> <p>Having programmed our node and defined it as an executable in our package, we're now ready to build the package so that we can run it. We use a tool called \"Colcon\" to do this, but this MUST be run from the root of the ROS Workspace (i.e.: <code>~/ros2_ws/</code>), so let's navigate there now using <code>cd</code>:</p> <p>TERMINAL 1: <pre><code>cd ~/ros2_ws/ \n</code></pre></p> <p>Then, use the <code>colcon build</code> command to build your package:</p> <pre><code>colcon build --packages-select waffle_demo --symlink-install\n</code></pre> <p>And finally, \"re-source\" the environment:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Now, run the code.</p> <p>Note</p> <p>Make sure the robot is on the floor and has enough room to roam around before you do this!</p> <p>TERMINAL 1: <pre><code>ros2 run waffle_demo square.py\n</code></pre></p> <p>Observe what the robot does. When you've seen enough, enter Ctrl+C in TERMINAL 1 to stop the node from running, which should also stop the robot from moving.</p> </li> <li> <p>Now it's time to adapt the code:</p> <p>The aim here is to make the robot follow a square motion path. What you may have observed when you actually ran the code is that the robot doesn't actually do that! We're using a time-based approach to make the robot switch between two different states continuously:</p> <ol> <li>Moving forwards</li> <li>Turning on the spot</li> </ol> <p>Have a look at the code to work out how much time the robot will currently spend in each state.</p> <p>We want the robot to follow a 0.5m x 0.5m square motion path.  In order to properly achieve this you'll need to adjust the timings, or the robot's velocity, or both. Edit the code so that the robot actually follows a 0.5m x 0.5m square motion path!</p> </li> </ol>"},{"location":"waffles/basics/#slam","title":"SLAM","text":"<p>Simultaneous Localisation and Mapping (SLAM) is a sophisticated tool that is built into ROS. Using data from the robot's LiDAR sensor, plus knowledge of how far the robot has moved<sup>2</sup> a robot is able to create a map of its environment and keep track of its location within that environment at the same time. In the exercise that follows you'll see how easy it is to implement SLAM with the Waffle.  </p>"},{"location":"waffles/basics/#exSlam","title":"Exercise 6: Using SLAM to create a map of the environment","text":"<ol> <li> <p>In TERMINAL 1 enter the following command to launch all the necessary SLAM nodes on the laptop:</p> <p>TERMINAL 1: <pre><code>ros2 launch turtlebot3_cartographer cartographer.launch.py\n</code></pre></p> Tip <p>On the laptop, this command is also available as an alias: <code>tb3_slam</code>!</p> <p>This will launch a new RViz instance, showing a top-down view of the environment, and dots of various colours representing the real-time LiDAR data. Rather than a robot model, the robot is now represented in the environment as a series of links denoted by 3-dimensional red/green/blue crosses (you can turn these off by unchecking the <code>TF</code> option in the left-hand \"Displays\" menu, if you want to). </p> <p> </p> <p>SLAM will already have begun processing this data to start building a map of the boundaries that are currently visible to the Waffle based on its location in the environment.</p> </li> <li> <p>Return to TERMINAL 2 and launch the <code>teleop_keyboard</code> node. Start to drive the robot around slowly and carefully to build up a complete map of the area.</p> <p>Tip</p> <p>It's best to do this slowly and perform multiple circuits of the area to build up a more accurate map.</p> <p> </p> </li> <li> <p>Once you're happy that your robot has built up a good map of its environment, you can save this map using the <code>map_saver_cli</code> node from a package called <code>nav2_map_server</code>:</p> <ol> <li> <p>First, create a new directory within your ROS package on the laptop. Return to TERMINAL 3 and navigate to the root of the <code>waffle_demo</code> package that you created earlier. We can use the <code>colcon_cd</code> tool to do this now:</p> <p>TERMINAL 3: <pre><code>colcon_cd waffle_demo\n</code></pre></p> </li> <li> <p>Create a directory in here called <code>maps</code>: </p> <p>TERMINAL 3: <pre><code>mkdir maps\n</code></pre></p> </li> <li> <p>Navigate into this directory:</p> <p>TERMINAL 3: <pre><code>cd maps/\n</code></pre></p> </li> <li> <p>Then, use <code>ros2 run</code> to run the <code>map_saver_cli</code> node and save a copy of your robot's map:</p> <p>TERMINAL 3: <pre><code>ros2 run nav2_map_server map_saver_cli -f MAP_NAME\n</code></pre></p> <p>Replacing <code>MAP_NAME</code> with an appropriate name for your map. This will create two files: </p> <ol> <li>a <code>MAP_NAME.pgm</code> </li> <li>a <code>MAP_NAME.yaml</code> file</li> </ol> <p>...both of which contain data related to the map that you have just created.</p> </li> <li> <p>The <code>.pgm</code> file can be opened using an application called <code>eog</code> on the laptop: </p> <p>TERMINAL 3: <pre><code>eog MAP_NAME.pgm\n</code></pre></p> </li> </ol> </li> <li> <p>Return to TERMINAL 1 and close down SLAM by pressing Ctrl+C. The process should stop and RViz should close down.</p> </li> <li> <p>Close down the <code>teleop_keyboard</code> node in TERMINAL 2 as well, if that's still running.</p> </li> </ol>"},{"location":"waffles/basics/#next-steps","title":"Next Steps","text":"<p>\"Pro Tips\": There are some important things to consider when working with the Real Waffles. Move onto the next page to find out more...</p> <p>... and when you've done that, don't forget to power off your robot properly. </p> <ol> <li> <p>If you happen to have closed down the robot terminal, you can return to it by entering <code>waffle X term</code> from a new terminal instance on the laptop.\u00a0\u21a9</p> </li> <li> <p>You'll learn much more about \"Robot Odometry\" in the lab course.\u00a0\u21a9</p> </li> </ol>"},{"location":"waffles/essentials/","title":"Further (Essential) Exercises","text":"<p>By working through the additional short exercises below, you will become aware of the key differences in how our TurtleBot3 Waffles work in the real world, compared to how they work in simulation. Be mindful of the differences that we are trying to highlight here and the implications that they will have on the applications that you develop. </p> <p>As a general rule, when developing code for real-world applications, it's a good idea to get the basics working in simulation first, but always test out your code in the real-world... just because it works in simulation, doesn't automatically mean it will work on the real robots!!</p>"},{"location":"waffles/essentials/#ex1","title":"Essential Exercise 1: Publishing Velocity Commands","text":"<p>This one actually applies to both the real Waffles and the simulation too. The issue is more critical when working with real thing though, since we're working with real hardware in a real-world environment, where things could get damaged or people could get hurt. </p> <p>In the previous exercises you made the robot move using the <code>teleop_keyboard</code> node, and also built a Python node to control the robot's velocity. Ultimately, making a robot move is achieved by publishing velocity commands (i.e. <code>geometry_msgs/msg/Twist</code> interface messages) to the <code>/cmd_vel</code> topic. Another way to do this is by using the <code>ros2 topic pub</code> command in a terminal. Run the following command and observe what the robot does:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.2\"\n</code></pre> <p>... the robot should turn on the spot.</p> <p>Enter Ctrl+C now to stop the <code>ros2 topic pub</code> command, what happens now?</p> <p>... the robot continues to turn on the spot!</p> <p>In order to actually stop the robot, we need to run the command again, but this time set all the velocities back to zero:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.0\"\n</code></pre>"},{"location":"waffles/essentials/#why-does-this-matter","title":"Why Does This Matter?","text":"<p>If we don't issue a stop command to a robot that is moving, then it will continue to move with the last velocity command that was sent to it. </p> <p>The same applies to ROS nodes when we shut them down: if a stop command is not sent to <code>cmd_vel</code> before the node stops running (i.e. via Ctrl+C) the robot will continue to move, which clearly isn't very good!</p> <p>You must ensure that your nodes are programmed with correct shutdown procedures, to ensure that the robot actually stops moving when a node is terminated. This is covered in Part 2 of the ROS Course, but you can also see this in action in the Velocity Control Node that we created in the previous section. </p>"},{"location":"waffles/essentials/#ex2","title":"Essential Exercise 2: Out of Range LiDAR Data","text":"<p>The robot's LiDAR sensor can only obtain measurements from objects within a certain distance range. In Part 3 we look at how to work out what this range is, using the <code>ros2 topic echo</code> command. Let's apply the same techniques to the real robot now to discover the maximum and minimum distances that the real robot's LiDAR sensor can measure:</p> <p><pre><code>ros2 topic echo /scan --field range_min --once\n</code></pre> <pre><code>ros2 topic echo /scan --field range_max --once\n</code></pre></p> <p>The LiDAR sensor's measurement range is the same in simulation and on the real robots, but how out-of-range values are reported is different!</p> <p>If the LiDAR sensor detects an object that falls within this range then it will report the exact distance to this object (in meters). Conversely, if it doesn't detect anything within this range then it will report a default out-of-range value instead. In simulation, the out-of-range value is <code>inf</code>.</p> <p>Warning</p> <p>The out-of-range value reported by the real robot's LiDAR sensor is not <code>inf</code>!</p> <p>Use the <code>ros2 topic echo</code> command to view the raw LiDAR data:</p> <pre><code>ros2 topic echo /scan --field ranges\n</code></pre> <p>See if you can position the robot in the environment so that some LiDAR measurements will fall outside the measurement range that you have just determined. How are these values reported in the terminal? </p>"},{"location":"waffles/essentials/#why-does-this-matter_1","title":"Why Does This Matter?","text":"<p>In Part 3 of the ROS Course we illustrate how the LiDAR <code>ranges</code> array can be filtered to remove any out-of-range values: </p> <pre><code>valid_data = front[front != float(\"inf\")] # (1)!\n</code></pre> <ol> <li>Assuming <code>front</code> is a <code>numpy</code> array (see Part 3).</li> </ol> <p>If you apply the same technique to a real-world application, then this filtering will be ineffective, because out-of-range values are not <code>inf</code> here... You'll need to adapt this for real-world scenarios<sup>1</sup>.</p>"},{"location":"waffles/essentials/#ex3","title":"Essential Exercise 3: The Camera Image Topic","text":"<p>In Part 6 of the ROS Course we work extensively with the robot's camera and the processing of the images that are captured by it. This is done in simulation (of course), where the image data is published to a topic called <code>/camera/image_raw</code>. The name of the camera image topic is not the same on the real robots!</p> <p>With the real robot to hand now, use ROS command-line tools such as <code>ros2 topic list</code> and <code>ros2 topic info</code> to interrogate the real robot ROS Network and identify the name of the camera image topic used by the real robot.</p>"},{"location":"waffles/essentials/#why-does-this-matter_2","title":"Why Does This Matter?","text":"<p>You'll likely do a lot of development work for your real-robot applications in simulation, outside the lab sessions. Some of these applications may involve camera data and image processing. If you set up a subscriber to a topic that doesn't exist, then ROS will not warn you about it! It will simply sit quietly and wait, assuming that the topic will (sooner or later) become available. As a result, if you are running an application on a real robot, that is subscribing to image data on the <code>/camera/image_raw</code> topic, then your application will never receive any image data and any associated callback functions will never execute!<sup>2</sup></p>"},{"location":"waffles/essentials/#ex4","title":"Essential Exercise 4: Camera Image Resolution","text":"<p>In Part 6 of the ROS Course we also explore how images can be reduced in size by cropping them, to make the data processing a bit quicker and easier. We need to be aware of the original image size (i.e. resolution) however, in order to apply cropping techniques appropriately. </p> <p>As we learn in Part 6, it's possible to discover the resolution of each image that is published to the camera image topic by echoing the <code>height</code> and <code>width</code> parameters that are published as part of the image message interface (alongside the image data itself). We can of course do this now with <code>ros2 topic echo</code>, to determine the resolution of the camera images obtained on the real robot:</p> <pre><code>ros2 topic echo /camera/color/image_raw --field height --once\n</code></pre> <pre><code>ros2 topic echo /camera/color/image_raw --field width --once\n</code></pre> <p>The outputs here will indicate the <code>height</code> and <code>width</code> of the images, in pixels. See how these compare with values that you obtain from the simulation, when you get to Part 6 of the Course.</p> <p>Warning</p> <p>The real robot's camera captures images at a lower image resolution! </p>"},{"location":"waffles/essentials/#why-does-this-matter_3","title":"Why Does This Matter?","text":"<p>We'll learn a lot about image cropping and other image processing techniques through simulation, but (as above) the native image resolution of the simulated robot's camera is much larger than that of the real robot. As such, if you apply the same cropping techniques to real-world applications, without adjustment, then you will end up cropping too much of the image out, leaving nothing to actually apply any further processing to!<sup>3</sup></p>"},{"location":"waffles/essentials/#ex5","title":"Essential Exercise 5: Object Detection","text":"<p>In general, image detection gets a little more challenging in the real-world, where the same object might appear (to a robot's camera) to have slightly different colour tones under different light conditions, from different angles, in different levels of shade, etc. In simulation (again in Part 6 of the Course), you may build an extremely effective <code>colour_search.py</code> node to detect each of the four coloured pillars in the <code>tuos_simulations/coloured_pillars</code> world, but this might not perform as well in the real world without some fine-tuning</p>"},{"location":"waffles/essentials/#why-does-this-matter_4","title":"Why Does This Matter?","text":"<p>Always test out your code in the real-world, just because it works in simulation, doesn't mean it will work on the real robots!!</p>"},{"location":"waffles/essentials/#summary","title":"Summary","text":"<p>You will naturally do a fair bit of development work in simulation, where it's easier to test things out and less disastrous if things go wrong! Overall, you'll be able to develop things much faster this way, and you can do this outside of your weekly lab sessions too. Whilst you're doing this though, keep in mind all the differences that we have identified above, so that there are less nasty surprises when you come to deploy your ROS applications in the real world. </p> <p>Throughout the design phase, think about how your applications could be developed more flexibly to accommodate these variations, or how you could design things so that only small/quick changes/switches need to be made when you transition from testing in simulation, to deploying on a real Waffle. </p> <ol> <li> <p>Exercise 2 Hint: Out-of-range values on the real robots are actually reported as <code>0.0</code>!\u00a0\u21a9</p> </li> <li> <p>Exercise 3 Hint: On the real robots, the camera image topic is <code>/camera/color/image_raw</code>!\u00a0\u21a9</p> </li> <li> <p>Exercise 4 Hint: In simulation, camera images have a resolution of 1080x1920 pixels, whereas on the real robots the resolution is 640x480 pixels.\u00a0\u21a9</p> </li> </ol>"},{"location":"waffles/intro/","title":"Introduction","text":""},{"location":"waffles/intro/#handling-the-robots","title":"Handling the Robots","text":"<p>Health and Safety</p> <p>You must have completed a health and safety quiz before working with the robots for the first time. This quiz is available on Blackboard.</p> <p></p> <p>As you can see from the figure above, the robots have lots of exposed sensors and electronics, so you must take great care when handling them to avoid the robots becoming damaged in any way.  When handling a robot, always hold it by either the black Waffle Layers, or the vertical Support Pillars (as highlighted in the figure above).</p> <p>Important</p> <p>Do not pick the robot up or carry it by the camera or LiDAR sensor! These are delicate devices that could be easily damaged!</p> <p>A lot of the robot's electronics are housed on the middle waffle layer. Try not to touch any of the circuit boards, and take care not to pull on any of the cabling or try to remove or rehouse any of the connections. If you have any concerns with any of the electronics or cabling, if something has come loose, or if your robot doesn't seem to be working properly then ask a member of the teaching team to have a look for you.</p> <p>The robots will be provided to you with a battery already installed and ready to go. Don't try to disconnect or remove the battery yourselves! The robot will beep when the battery is low, and if this happens ask a member of the team to get you a replacement (we have plenty).</p>"},{"location":"waffles/intro/#laptops","title":"The Robot Laptops","text":"<p>You'll be provided with one of our pre-configured Robot Laptops in the lab when working with the real Waffles. These Laptops (and the Robots) run Ubuntu 22.04 with ROS 2 Humble. </p> <p>There's a \"student\" user account already set up, and you'll need to use this when working in the lab. The laptop should log you into this user account automatically on startup, but we'll provide you with the account password as well, during the lab sessions, should you need it.</p>"},{"location":"waffles/intro/#network","title":"Network","text":"<p>The Robots and Laptops must be able to connect to one another over an internet connection. The robots connect to a dedicated wireless network running in the Diamond called 'DIA-LAB'. In order for the laptops to be able to \"see\" the robots, they must be connected to the university network using any of the following options:</p> <ol> <li> <p>A wired (ethernet) connection (Recommended)</p> <ul> <li>(subject to provision of network sockets in CR5 by February 2025)</li> </ul> </li> <li> <p>The eduroam WiFi SSID</p> </li> <li>The DIA-LAB WiFi SSID (no internet access!)</li> </ol> <p>WiFi credentials for DIA-LAB and eduroam have already been set on the laptops, allowing you to connect to either network straight away, but speak to a member of the teaching team if you are having any issues.</p>"},{"location":"waffles/intro/#vs-code","title":"VS Code","text":"<p>Visual Studio Code is installed on the laptops for you to use when working on your ROS applications for the assignment tasks. Launch VS Code from any terminal by simply typing <code>code</code>. You can also launch it by clicking the icon in the favourites bar on the left-hand side of the screen:</p> <p></p>"},{"location":"waffles/launching-ros/","title":"Launching ROS","text":"<p>The first step is to launch ROS on the Waffle.</p> <p>Important</p> <p>This ensures that all the core ROS functionality is executed on the robot, without this the robot won't be able to do anything!</p>"},{"location":"waffles/launching-ros/#step-1-identify-your-waffle","title":"Step 1: Identify your Waffle","text":"<p>Robots are named as follows:</p> <pre><code>dia-waffleX\n</code></pre> <p>... where <code>X</code> is the 'Robot Number' (a number between 1 and 50). Make sure you know which robot you are working with by checking the label printed on top of it!</p>"},{"location":"waffles/launching-ros/#step-2-pairing-your-waffle-to-a-laptop","title":"Step 2: Pairing your Waffle to a Laptop","text":"<p>As discussed earlier, you'll be provided with one of our Robotics Laptops to work with in the lab, and the robot needs to be paired with this in order for the two to work together.  </p> <ol> <li> <p>Open up a terminal instance on the laptop, either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon in the favourites bar on the left-hand side of the desktop:</p> <p> </p> </li> <li> <p>We'll use our purpose-built <code>waffle</code> CLI to handle the pairing process. Run this in the terminal by entering the following command to pair the laptop and robot:</p> <p><pre><code>waffle X pair\n</code></pre> Replacing <code>X</code> with the number of the robot that you are working with.</p> </li> <li> <p>You may see a message like this early on in the pairing process:</p> <p> </p> <p>If so, just type <code>yes</code> and then hit Enter to confirm that you want to continue.</p> </li> <li> <p>Enter the password for the robot when requested (we'll tell you what this is in the lab!)</p> <p>Note</p> <p>You won't see anything change on the screen when you are entering the password. This is normal, just keep typing!!</p> </li> <li> <p>The pairing process will take a minute, but once it's finished you should see a message saying <code>pairing complete</code>, displayed in blue in the terminal. </p> </li> <li> <p>Then, in the same terminal, enter the following command: </p> <p><pre><code>waffle X term\n</code></pre> (again, replacing <code>X</code> with the number of your robot).</p> <p>A green banner should appear across the bottom of the terminal window:</p> <p> </p> <p>This is a terminal instance running on the robot, and any commands that you enter here will be executed on the robot (not the laptop!)</p> </li> </ol>"},{"location":"waffles/launching-ros/#step-3-launching-ros","title":"Step 3: Launching ROS","text":"<p>Launch ROS on the robot by entering the following command:</p> <pre><code>tb3_bringup\n</code></pre> <p>If all is well then the robot will play a nice \"do-re-me\" sound and a message like this should appear (amongst all the other text):</p> <pre><code>[tb3_status.py-#] ######################################\n[tb3_status.py-#] ### dia-waffleX is up and running! ###\n[tb3_status.py-#] ######################################\n</code></pre> <p>You shouldn't need to interact with this terminal instance any more now, but the screen will provide you with some regular real-time info related to the status of the robot. As such, keep this terminal open in the background and check on the <code>Battery</code> indicator every now and then:</p> <pre><code>Battery: 12.40V [100%]\n</code></pre> <p>Low Battery </p> <p>The robot's battery won't last a full 2-hour lab session!!</p> <p>When the capacity indicator reaches around 15% then it will start to beep, and when it reaches ~10% it will stop working all together.  Let a member of the teaching team know when the battery is running low and we'll replace it for you. (It's easier to do this when it reaches 15%, rather than waiting until it runs below 10%!)</p>"},{"location":"waffles/launching-ros/#shutting-down-at-the-end-of-a-lab-session","title":"Shutting Down (at the end of a Lab Session)","text":"<p>When you've finished working with a robot it's really important to shut it down properly before turning off the power switch. Please refer to the safe shutdown procedures for more info.</p>"},{"location":"waffles/shutdown/","title":"Shutdown Procedures","text":""},{"location":"waffles/shutdown/#robots","title":"Robots","text":"<p>The Waffles are powered by a Single Board Computer (SBC), which runs a full-blown operating system. As with any operating system, it's important to shut it down properly, rather than simply disconnecting the power, to avoid any data loss or other issues. </p> <p>Therefore, once you've finished working with a robot during a lab session, follow the steps below to shut it down.</p> <ol> <li> <p>Open a new terminal instance on the laptop (Ctrl+Alt+T), and enter the following:</p> <p><pre><code>waffle X off\n</code></pre> ... replacing <code>X</code> with the number of the robot that you have been working with.</p> </li> <li> <p>You'll be asked to confirm that you want to shut the robot down: </p> <pre><code>[INPUT] Are you sure you want to shutdown dia-waffleX? [y/n] &gt;&gt; \n</code></pre> <p>Enter Y and hit Enter and the robot's SBC will be shut down. </p> </li> <li> <p>Once the blue light on the corner of the SBC goes out, it's then safe to slide the power button to the left to completely turn off the device. </p> <p> </p> </li> </ol>"},{"location":"waffles/shutdown/#laptops","title":"Laptops","text":"<p>Once you've turned off the robot, remember to shut down the laptop too! Do this by clicking the battery icon in the top right of the desktop and selecting the \"Power Off / Log Out\" option in the drop-down menu.</p> <p></p> <p> <p>Hand your robot and laptop back to a member of the teaching team who will put it away for you!</p> <p></p>"}]}