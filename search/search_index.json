{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#ros2-course-assignments-for-com2009","title":"ROS2 Course Assignments for COM2009","text":"<p>Robotics Labs for the COM2009 Robotics Module at The University of Sheffield, with ROS 2 and the TurtleBot3 Waffle.</p> <p> </p> A TurtleBot3 Waffle Image courtesy of Andy Brown <p>Find out more...</p> <p></p>"},{"location":"about/","title":"About this Site","text":""},{"location":"about/#about-this-site","title":"About this Site","text":"<p>This is the home of The ROS2 Course Assignments for COM2009: a second-year undergraduate module for The School of Computer Science. </p> <p>This course is designed to teach students how to use ROS2 (The Robot Operating System, Version 2) with TurtleBot3 Waffle robots, using a mix of simulation-based learning and real robot hardware. Most of the initial learning is done in simulation, after which students are able to apply their new-found ROS knowledge to our real TurtleBot3 Waffle Robots.</p> <p>The materials here are developed by Dr Tom Howard, a University Teacher in the Multidisciplinary Engineering Education Team in The Diamond.</p> <p>These resources are also used to teach masters-level Mechatronic and Robotic Engineering students in The School of Electrical and Electronic Engineering (ACS6121). </p>"},{"location":"about/changelog/","title":"Version History","text":""},{"location":"about/changelog/#iteration-1","title":"Iteration 1","text":"<p>Academic Year: 2024-25 </p> <ul> <li>This course used to live here, and was based on ROS 1 Noetic. This year, we've upgraded everything to ROS 2, re-written the courses and moved everything over to this new site. See the previous version history here.</li> <li>Other notable changes:<ul> <li>COM2009 Assignment #2 now only involves three programming tasks (where previously it was four), with an additional assessment on documentation now included instead.</li> </ul> </li> </ul>"},{"location":"about/license/","title":"License","text":""},{"location":"about/license/#license","title":"License","text":"<p>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</p> <p>You are free to distribute, remix, adapt, and build upon this work (for non-commercial purposes only) as long as credit is given to the original author.</p> <p></p><p></p>"},{"location":"about/robots/","title":"Introducing the Robots","text":""},{"location":"about/robots/#robots","title":"The TurtleBot3 Waffle","text":""},{"location":"about/robots/#turtlebot-what","title":"Turtlebot what?!","text":"<p>To teach ROS here we use the TurtleBot3 Waffle robot, made by Robotis. This is the 3rd Generation Robot in the TurtleBot family (which has been the reference hardware platform for ROS since 2010). The TurtleBot Robot family exists to provide accessible and relatively low-cost hardware and open-source software on a robot platform, to encourage people to learn robotics and ROS and make it as easy as possible to do so.</p>"},{"location":"about/robots/#ebook","title":"The (Free) TurtleBot3 eBook","text":"<p>The TurtleBot3 Waffle developers (Robotis) have written a book on programming robots with ROS. This is available as a free eBook, which you can download here, and we recommend that you do so! This is a great resource which provides a detailed introduction to what ROS is and how it works, as well as a comprehensive \"Beginners Guide\" to ROS programming. The other great thing about this is that it is tailored to the TurtleBot3 Robot specifically, providing examples of how to use a range of TurtleBot3 packages along with a detailed description of how they work.</p> <p>We recommend that you have a look at this book to learn more about the concepts that you are exploring in this course.</p>"},{"location":"about/robots/#our-waffles","title":"Our Waffles","text":"<p>Here in the Diamond we have a total of 50 customised TurtleBot3 Waffles (aka \"The Waffles\") specifically for teaching the courses here:</p> <p> </p> <p>Our robots are an enhanced version of the TurtleBot3 WafflePi that you can buy from Robotis. We've made a few adjustments, as shown below:</p> <p></p> <p>The Waffles have the following core hardware elements:</p> <ul> <li>An OpenCR Micro-Controller Board to power and control the wheel motors, distribute power to other hardware elements and provide an interface for additional sensors.</li> <li>An UP Squared Single-Board Computer (SBC) with an Intel Processor and 32GB of on-board eMMC storage. This board acts as the \"brain\" of the robot.</li> <li>Independent left and right wheel motors (DYNAMIXEL XM430\u2019s) to drive the robot using a differential drive configuration.</li> </ul> <p>This drive configuration allows the robots to move with the following maximum velocities: </p> <p></p><p></p> Velocity Component Upper Limit Units Linear 0.26 m/s Angular 1.82 rad/s <p></p><p></p> <p>In addition to this, the robots are equipped with the following sensors:</p> <ul> <li>A Light Detection and Ranging (or LiDAR) sensor, which spins continuously when the robot is in operation. This uses light in the form of laser pulses to allow the robot to measure the distance to surrounding objects, providing it with a 360\u00b0 view of its environment.</li> <li>An Intel RealSense D435 Camera with left and right imaging sensors, allowing depth sensing as well as standard image capture.</li> <li>A 9-Axis Inertial Measurement Unit (or IMU) on-board the OpenCR Micro Controller board, which uses an accelerometer, gyroscope and magnetometer to measure the robot's specific force, acceleration and orientation. </li> <li>Encoders in each of the DYNAMIXEL wheel motors, allowing measurement of speed and rotation count for each of the wheels.</li> </ul>"},{"location":"about/robots/#software","title":"Software","text":"<p>Our robots currently run ROS 2 Humble Hawksbill (or \"Humble\" for short). The courses here are therefore based around this version of ROS. The easiest way to install Humble is via Deb packages for Ubuntu Jammy (22.04). This is the setup we recommend and - as such - all out robotics hardware runs with this OS/Software setup.</p> <p>To deliver the simulation-based parts of this course, we've created a custom simulation environment using the Windows Subsystem for Linux (WSL). This has been developed primarily to run on University of Sheffield Managed Desktop Computers, which run Windows 10, but it's also possible to run this on other machines too. We call this simulation environment \"WSL-ROS2\". See here for more details.</p> <p>You can find out more about installing ROS on your own system here.</p>"},{"location":"about/robots/#laptops","title":"Laptops","text":"<p>In the Diamond, we have dedicated Robot Laptops running the same OS &amp; ROS version as above. We use these when working with the robots in the lab. See here for more details.</p>"},{"location":"course/","title":"COM2009 Course Assignments","text":""},{"location":"course/#com2009-course-assignments","title":"COM2009 Course Assignments","text":"<p>For the COM2009 Robotics course you must complete two lab assignments:</p> <ul> <li> <p>Assignment #1: \"An Introduction to ROS (the Robot Operating System)\".</p> <p>Here you will learn what ROS is and how to use it. You will complete this assignment individually, and in your own time.</p> <p>Weighting: 25% of the overall COM2009 module mark.</p> </li> <li> <p>Assignment #2: \"Team Robotics Project\".</p> <p>Here you will work in teams of 3-4 to complete a series of real-world robotics tasks using our Tutlebot3 Waffle Robots in the Lab (Diamond Computer Room 5).</p> <p>Weighting: 30% of the overall COM2009 module mark</p> </li> </ul>"},{"location":"course/assignment3/","title":"Assignment #3 (COM3009 only)","text":"<p>Develop ROS node(s) that enable a TurtleBot3 Waffle to search an environment, detect a coloured object and beacon towards it, stopping in proximity without crashing into it!</p> <p>This is an individual assignment, which will be assessed in simulation NOT on a real robot.</p> <p>Deadline: Week 12, Friday 22:00. </p>"},{"location":"course/assignment3/#summary","title":"Summary","text":"<p>For this task your robot will be placed in a \"Search Arena\" containing a number of different coloured objects (\"beacons\"). Building on the exploration behaviours that you will have developed in your team for the Assignment #2 Tasks, the robot will need to search the arena for a beacon of a particular \"Target Colour\" (each beacon in the arena will have a unique colour). The arena will contain three \"Start Zones\" (also uniquely coloured) and your robot will be located in one of these three zones to begin with (selected at random). The colour of the Start Zone indicates the Target Colour for the search task (the colour of the beacon that the robot needs to find). Once the target object has been detected, the robot will need to move towards it (i.e. \"beaconing\") and stop within a \"Stop Zone\" printed on the floor surrounding it. The robot must stop in the Stop Zone without touching the beacon!</p> <p>The first thing that your robot will need to do in this task is detect the colour of the zone that it starts in. You learnt about colour-based object detection in Part 6 of Assignment #1, where we used OpenCV to analyse the images published to the <code>/camera/image_raw</code> topic. Use what you did here, as well as the further work that we did in Part 6 Exercise 3, as a starting point for achieving the desired behaviour for this initial part of the task. Note that in the Part 6 exercises you developed algorithms to detect an object of a certain colour, but here you need to work the other way around and actually establish the colour of the object instead, so you will have to reverse the logic a bit. </p> <p>You'll then need to explore the arena for the target object. Remember that both the start zone and the target object will share the same colour, so take care not to detect the start zone as the beacon! Odometry might be useful here to inform your robot of where it started from so that it knows to rule out anything in that vicinity as a potential target. </p> <p>Having located the target object within the environment your robot will then need to move towards it and stop within the allocated stop zone. This technique is known as Beaconing, and some strategies for this are discussed in COM2009/3009 Lecture 8. Perhaps you could consider an implementation of Braitenberg's Vehicle 3a as a way to control your robot's trajectory and approach to the target object?</p> <p>The concept of Visual Homing (also discussed in COM2009/3009 Lecture 8) might also be worth considering as a method to control the position and trajectory of a robot based on images from its camera. Distance measurements from the LiDAR sensor might also be helpful for this.</p>"},{"location":"course/assignment3/#creating-a-package","title":"Creating a Package","text":"<p>You'll need to create your own individual ROS package for this assignment. This is an individual assignment, and is entirely separate from the Assignment #2 team project, so you must create your own individual ROS package repo for this. Follow the steps below to do so:</p>"},{"location":"course/assignment3/#creating-your-individual-package-repo-on-github","title":"Creating Your Individual Package Repo (on GitHub)","text":"<ol> <li>Ensure that you are signed in to your GitHub account, then go to the <code>ros2_pkg_template</code> Repo. </li> <li> <p>Click on the green <code>Use this template</code> button, and then select <code>Create a new repository</code> from the dropdown menu. </p> <p></p> <p></p> <p>You should then be presented with a Create a new repository screen.</p> </li> <li> <p>Enter a name for your repository in the <code>Repository name</code> box. This name must be formatted as follows:</p> <pre><code>com3009_xxxxxx_2025\n</code></pre> <p>... where <code>xxxxxx</code> should be replaced with your Sheffield University Username.</p> <p>Important</p> <p>All characters should be lower case (e.g. <code>com3009</code>, NOT <code>COM3009</code>)</p> </li> <li> <p>Select <code>Private</code> to make the repository private, then click the green <code>Create repository</code> button. </p> </li> <li> <p>You'll then be directed to your main repository page. From here, click on <code>Settings</code>, then under <code>Access</code> click <code>Collaborators</code> (you may be prompted for 2FA).</p> </li> <li> <p>In the <code>Manage access</code> area, click the green <code>Add people</code> button and add <code>tom-howard</code>: </p> <p></p> <p></p> </li> </ol>"},{"location":"course/assignment3/#register-your-ros-package","title":"Register Your ROS Package","text":"<p>Having created your package, you'll need to tell us your GitHub username, and your repo's GitHub URL. There is a form that you must complete in order to do this. Access the form here. You must be signed in to your university email account (<code>...@sheffield.ac.uk</code>) to access this. </p>"},{"location":"course/assignment3/#initialising-your-package","title":"Initialising Your Package","text":"<p>You should do this from within your own ROS installation (or WSL-ROS2). See here for more details on how to access a ROS environment.</p> <ol> <li> <p>On GitHub, go back to your repository's main page by clicking the <code>&lt;&gt; Code</code> tab at the top-left.</p> </li> <li> <p>Click the green <code>Code</code> button and then, from the dropdown menu, click the  button to copy the remote HTTPS URL of your repo. </p> </li> <li> <p>From your local ROS installation, open a terminal instance and navigate to the <code>src</code> directory of the ROS Workspace:</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> </li> <li> <p>Clone your repo here using the remote HTTPS URL:</p> <pre><code>git clone REMOTE_HTTPS_URL\n</code></pre> <p>You'll then be asked to enter your GitHub username, followed by a password. You'll need to generate a personal access token and use this here (or use SSH keys instead).</p> </li> <li> <p>Navigate into your package directory using the <code>cd</code> command:</p> <pre><code>cd com3009_xxxxxx_2025\n</code></pre> <p>...replacing <code>xxxxxx</code> accordingly.</p> </li> <li> <p>Then, run the <code>init_pkg.sh</code> script to configure your ROS package appropriately:</p> <pre><code>./init_pkg.sh\n</code></pre> </li> </ol>"},{"location":"course/assignment3/#git-push","title":"Push Your Local Changes Back to GitHub","text":"<p>You'll need to make sure Git is configured properly with your name and email address in your local ROS installation before you do this. See here for details.</p> <ol> <li> <p>From the same terminal as above, use the <code>git status</code> command to show you all the changes that have been made to the repo in the initialisation process:</p> <pre><code>git status\n</code></pre> </li> <li> <p>Use <code>git add</code> to stage all these changes for an initial commit:</p> <pre><code>git add .\n</code></pre> </li> <li> <p>Then commit them:</p> <pre><code>git commit -m \"ROS package initialisations complete.\"\n</code></pre> </li> <li> <p>Finally, push the local changes back up the \"remote\" repository on GitHub:</p> <pre><code>git push origin main\n</code></pre> <p>You'll then be asked to enter your GitHub username and password again. (Remember that this is not your GitHub account password... Use the personal access token that you created earlier!)</p> </li> </ol>"},{"location":"course/assignment3/#details","title":"Details","text":"<p>The arena used for this task will be 5.0 m x 5.0 m and the beacons that you'll be searching for will be coloured boxes or cylinders, all between 200 mm and 400 mm in height. The Stop Zone surrounding each beacon will be 500 mm greater than the beacon's dimensions in the <code>X</code> and <code>Y</code> axis.</p> <p>There are only six possible target colours that will be used in this task, so your ROS application will only need to accommodate these six. The colours are listed below, and there is also a simulation environment in the <code>tuos_simulations</code> package called <code>beacon_colours</code> to illustrate these too<sup>1</sup>.</p> <pre><code>ros2 launch tuos_simulations beacon_colours.launch.py\n</code></pre> <p> </p> The range of possible beacon colours that could be used in this assignment. <p>As for the task itself:</p> <ol> <li>Your robot will first need to determine the \"Target Colour\" by analysing the \"Start Zone\" that it has been placed in within the simulated arena (see below).</li> <li>The arena will contain three Start Zones, each of a different colour, and your robot could be launched into any of these (selected at random).</li> <li> <p>Once the colour of the start zone has been determined by the robot a ROS log message must be printed to the terminal to indicate which beacon colour will be targetted. This log message must be formatted exactly as follows:</p> <p></p> <pre><code>SEARCH INITIATED: The target beacon colour is {}.\n</code></pre> <p>Where <code>{}</code> is replaced by the name of the target colour as defined in the table inset in the figure above.</p> <p>Note</p> <p>You must use <code>.get_logger().info()</code> method calls within your node in order to print ALL log messages for this assignment. </p> </li> <li> <p>The robot then needs to navigate the arena, avoiding contact with any of the objects that are located within it whilst searching for the beacon of the correct colour.</p> </li> <li> <p>Once the target beacon has been detected, a message must be printed to the terminal to indicate that this has happened. The terminal message needs to be clearly visible and readable, and the robot must be facing the target beacon when it is printed. This terminal message should be formatted as follows:</p> <p></p> <pre><code>TARGET DETECTED: Beaconing initiated.\n</code></pre> <p>Remember</p> <p>Use <code>.get_logger().info()</code> method calls within your node to print this. </p> </li> <li> <p>The robot then needs to start moving towards the beacon, stopping when it is close enough to be within the stop zone surrounding it, but not close enough to actually make contact. As discussed above, the stop zone surrounding each object will be 500 mm greater than the beacon dimensions in the <code>X-Y</code> plane.</p> </li> <li> <p>A further message must be printed to the terminal to indicate that the robot has successfully and intentionally stopped within the designated area. This terminal message should be formatted as follows:</p> <p></p> <pre><code>BEACONING COMPLETE: The robot has now stopped.\n</code></pre> <p>Remember</p> <p>Use <code>.get_logger().info()</code>!</p> </li> <li> <p>The robot will have a maximum of 90 seconds to complete this task. </p> <p>Timing will be determined using the \"Sim Time\" indicator in Gazebo.</p> </li> <li> <p>Your ROS package must contain a launch file called <code>assignment3.launch.py</code>, such that the functionality that you develop can be launched from your package via the command:</p> <pre><code>ros2 launch com3009_xxxxxx_2025 assignment3.launch.py\n</code></pre> <p>The robot will already have been launched into the simulated environment before we attempt to execute your launch file.</p> </li> </ol>"},{"location":"course/assignment3/#simulation-resources","title":"Simulation Resources","text":"<p>Within the <code>tuos_simulations</code> package there is an environment called <code>beaconing</code>, which can be used to develop and test out your ROS node(s) for this task:</p> <pre><code>ros2 launch tuos_simulations beaconing.launch.py\n</code></pre> <p>The arena contains three start zones: A, B &amp; C; each of a different colour, as well as a number of uniquely coloured beacons.  There is one beacon in the arena to match each of the three start zones, plus a couple more to act as decoys! </p> <p> </p> The \"beaconing\" arena. <p>You can launch the robot into any of the three start zones by supplying an optional <code>start_zone</code> argument to the <code>beaconing.launch.py</code> file, as illustrated below:</p> <pre><code>ros2 launch com2009_simulations beaconing.launch.py start_zone:={}\n</code></pre> <p>...where <code>{}</code> can be replaced with either <code>a</code>, <code>b</code> or <code>c</code> to select the start zone that you want the robot to be located in when the simulation launches. You can therefore develop and test out your beaconing algorithms in three unique scenarios.</p> <p>Note</p> <ol> <li>The same arena will be used to assess your submission for this assignment.</li> <li>The colour of the start zones and beacons will change, but the shape, size and location of all the objects will stay the same.</li> <li>Once again, the start zone that your robot is launched in for the assessment will be selected at random.</li> </ol>"},{"location":"course/assignment3/#marking","title":"Marking","text":"<p>There are 15 marks available for this assignment in total, awarded based on the criteria below. No partial credit will be awarded unless specifically stated against any of the criteria.</p> <p></p> Criteria Marks Details A: Identifying the target colour 2/15 Whilst the robot is still located within the start zone a ROS node within your package must print a log message to the terminal to indicate the target colour that has been determined and that will subsequently be used to identify the target beacon. You will receive the full marks available here provided the message is presented using a <code>get_logger().info()</code> method call, and formatted as specified here. B: Detecting the correct beacon 3/15 You will receive the full marks available here for printing a log message to the terminal to indicate that the target beacon has been identified within the environment. The message must again be presented using a <code>get_logger().info()</code> method call, it must be formatted as specified here, and the robot must be looking directly at the beacon when this message is printed. C: Stopping in the correct stop zone 5/15 Your robot must stop inside the correct stop zone within the 90-second time limit and a log message must be printed to the terminal to indicate that this has been done intentionally. You will receive the full marks available here provided this is achieved successfully, and the log message (again using a <code>get_logger().info()</code> call) is formatted as specified here. If your robot manages to stop, but part of its body lies outside the stop zone then you will be awarded half-marks. D: An \"incident-free-run\" 5/15 If your robot completes the task (or the 90 seconds elapses) without it making contact with anything in the arena then you will be awarded the maximum marks here. Marks will be deducted for any contact made, to a minimum of 0 (i.e. no negative marking). Your robot must be moving within the arena continually to be eligible for these marks though, simply turning on the spot for 90 seconds is not enough! <p></p>"},{"location":"course/assignment3/#submission","title":"Submission","text":"<p>Your work will be pulled from GitHub on the deadline listed above, so it's important that you register your ROS package with us (via the form above) so that we can access this.</p> <p>Your work must be located on the <code>main</code> branch of your package repo.</p> <ol> <li> <p>Make sure you have the most up-to-date version of the Course Repo.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment1/","title":"Assignment #1: An Introduction to ROS","text":""},{"location":"course/assignment1/#overview","title":"Overview","text":"<p>Assignment #1 is a 6-part course, which you should complete in full and in order. The course is designed to be completed in simulation, so you will therefore need access to a ROS2 installation which can either be installed on your own machine, or accessed on a range of managed computers across the University of Sheffield campus. See here for more information on how to access or install ROS2.</p> <p>Each part of the course comprises a series of step-by-step instructions and exercises to teach you how ROS works, and introduces you to the core principles of the framework. The exercises give you the opportunity to see how to apply these principles to practical robotic applications. Completing this course is essential for obtaining all the necessary skills for Assignment #2: the Team Robotics Project, where you will work in teams to program our real TurtleBot3 Waffle robots.  </p>"},{"location":"course/assignment1/#the-course","title":"The Course","text":"<ul> <li> <p>Part 1: Getting Started with ROS 2</p> <p>Learn the basics of ROS 2 and become familiar with some key tools and principles, allowing you to program robots and work with ROS2 applications effectively.</p> </li> <li> <p>Part 2: Odometry &amp; Navigation</p> <p>Learn about Odometry, which informs us of a robot's position and orientation in an environment. Apply both open and closed-loop velocity control methods to a Waffle.</p> </li> <li> <p>Part 3: Beyond the Basics</p> <p>Execute ROS applications more efficiently using launch files. Learn about the LiDAR sensor, the data that it generates, and see the benefits of this for tools like \"SLAM\".</p> </li> <li> <p>Part 4: Services</p> <p>Learn about an alternative way that ROS nodes can communicate across a ROS network, and the situations where this might be useful.</p> </li> <li> <p>Part 5: Actions</p> <p>Learn about another ROS communication method which is similar to a ROS Service, but with a few key benefits and which has some alternative use-cases.</p> </li> <li> <p>Part 6: Cameras, Machine Vision &amp; OpenCV</p> <p>Learn how to work with images from an on-board camera. Learn techniques to detect features within these images, and use this to inform robot decision-making.</p> </li> </ul>"},{"location":"course/assignment1/#assessment","title":"Assessment","text":"<p>This assignment is worth 25% of the overall mark for COM2009, and is assessed via an on-campus Blackboard-based test taking place in week 7 or 8 of the Spring Semester (see Blackboard and/or your timetable for the exact date &amp; time). </p>"},{"location":"course/assignment1/part1/","title":"Part 1: Getting Started with ROS 2","text":""},{"location":"course/assignment1/part1/#introduction","title":"Introduction","text":"<p> Exercises: 8 Estimated Completion Time: 2 hours Difficulty Level: Beginner </p>"},{"location":"course/assignment1/part1/#aims","title":"Aims","text":"<p>In the first part of this lab course you will learn the basics of ROS and become familiar with some key tools and principles of the framework which will allow you to program robots and work with ROS applications effectively.  For the most part, you will interact with ROS using the Linux command line and so you will also become familiar with some key Linux command line tools that will help you.  Finally, you will learn how to create some basic ROS Nodes using Python and get a taste of how ROS topics and messages work.</p>"},{"location":"course/assignment1/part1/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:  </p> <ol> <li>Control a TurtleBot3 Robot, in simulation, using ROS.</li> <li>Launch ROS applications using <code>ros2 launch</code> and <code>ros2 run</code>.</li> <li>Interrogate running ROS applications using key ROS command line tools.</li> <li>Create a ROS package comprised of multiple nodes and program these nodes (in Python) to communicate with one another using ROS Communication Methods.</li> <li>Create a custom ROS message interface and create Python Nodes to use this.</li> <li>Navigate a Linux filesystem and learn how to do various filesystem operations from within a Linux Terminal.</li> </ol>"},{"location":"course/assignment1/part1/#quick-links","title":"Quick Links","text":""},{"location":"course/assignment1/part1/#exercises","title":"Exercises","text":"<ul> <li>Exercise 1: Launching a simulation and making a robot move</li> <li>Exercise 2: Visualising the ROS Network</li> <li>Exercise 3: Exploring ROS Topics and Messages</li> <li>Exercise 4: Creating your own ROS Package</li> <li>Exercise 5: Creating a publisher node</li> <li>Exercise 6: Creating a subscriber node</li> <li>Exercise 7: Defining our own message</li> <li>Exercise 8: Using a custom ROS Message</li> </ul>"},{"location":"course/assignment1/part1/#additional-resources","title":"Additional Resources","text":"<ul> <li>A Simple Python Publisher</li> <li>A Simple Python Subscriber</li> </ul>"},{"location":"course/assignment1/part1/#first-steps","title":"First Steps","text":"<p>Step 1: Accessing a ROS2 Environment for this Course</p> <p>If you haven't done so already, see here for all the details on how to install or access a ROS environment for this course.</p> <p>Step 2: Launch ROS</p> <p>Launch your ROS environment.</p> <ol> <li>If you're using WSL-ROS2 on a university managed desktop machine then follow the instructions here to launch it.</li> <li>If you're running WSL-ROS2 on your own machine, then you'll need to launch the Windows Terminal to access a WSL-ROS2 terminal instance.</li> <li>If you're using another installation option, then follow the instructions on the relevant page here. </li> </ol> <p>Either way, you should now have access to ROS2 via a Linux terminal instance, and we'll refer to this terminal instance as TERMINAL 1.</p> <p>Step 3: Download The Course Repo</p> <p></p> <p>We've put together a few ROS packages specifically for this course. These all live within this GitHub repo, and you'll need to download and install this into your ROS environment now, before going any further.</p> <ol> <li> <p>In TERMINAL 1, Navigate into the \"ROS Workspace\" using the <code>cd</code> command<sup>1</sup>:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Then, run the following command to clone the Course Repo from GitHub:</p> <p>TERMINAL 1: </p><pre><code>git clone https://github.com/tom-howard/tuos_ros.git -b humble\n</code></pre><p></p> </li> <li> <p>Once this is done, you'll need to build this using a tool called \"Colcon\"<sup>2</sup>:</p> <p>TERMINAL 1: </p><pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-up-to tuos_ros &amp;&amp; source ~/.bashrc\n</code></pre><p></p> </li> </ol> <p>Don't worry too much about what you just did, for now. We'll cover this in more detail throughout the course. That's it for now though, we'll start using some of the packages that we've just installed a bit later on.</p>"},{"location":"course/assignment1/part1/#ex1","title":"Exercise 1: Launching a simulation and making a robot move","text":"<p>Now that you're all up and running, let's launch ROS and fire up a simulation of our TurtleBot3 Waffle robot... </p> <ol> <li> <p>In the terminal enter the following command to launch a simulation of a TurtleBot3 Waffle in an empty world:  </p> <p>TERMINAL 1: </p><pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre><p></p> </li> <li> <p>A Gazebo simulation window should open:</p> <p></p> <p></p> <ol> <li>Zoom in and out using the scroll wheel on your mouse.  </li> <li>Rotate the camera view by simultaneously pressing and holding the left mouse button and the Shift key on your keyboard, and then moving your mouse around.</li> </ol> <p>Using both of these methods you should be able to get a better view of the robot, which will be sat under a lot of blue LiDAR rays (more about this in Part 3), and will appear as a simplified representation of the real thing.</p> <p></p> <p></p> </li> <li> <p>With the Gazebo simulation up and running, return to your terminal and open up a second terminal instance (TERMINAL 2)</p> </li> <li> <p>In this new terminal instance enter the following command:</p> <p>TERMINAL 2: </p><pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre><p></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around using specific buttons on your keyboard:</p> <p></p> <p></p> </li> </ol>"},{"location":"course/assignment1/part1/#summary","title":"Summary","text":"<p>You have just launched a number of different applications on a ROS Network using two different ROS commands - <code>ros2 launch</code> and <code>ros2 run</code>: </p> <ol> <li><code>ros2 launch turtlebot3_gazebo empty_world.launch.py</code></li> <li><code>ros2 run turtlebot3_teleop teleop_keyboard</code></li> </ol> <p>These two commands have a similar structure, but work slightly differently. </p> <p>The first command you used was a <code>launch</code> command, which has the following two parts to it (after the <code>launch</code> bit):</p> <pre><code>ros2 launch {[1] Package name} {[2] Launch file}\n</code></pre> <p>Part [1] specifies the name of the ROS package containing the functionality that we want to execute. Part [2] is a file within that package that tells ROS exactly what scripts ('nodes') that we want to launch. We can launch multiple nodes at the same time from a single launch file.</p> <p>The second command was a <code>run</code> command, which has a structure similar to <code>launch</code>:</p> <pre><code>ros2 run {[1] Package name} {[2] Node name}\n</code></pre> <p>Here, Part [1] is the same as the <code>launch</code> command, but Part [2] is slightly different: <code>{[2] Node name}</code>. Here we are directly specifying a single script that we want to execute. We therefore use <code>ros2 run</code> if we only want to launch a single node on the ROS network: the <code>teleop_keyboard</code> node (a Python script), in this case.</p>"},{"location":"course/assignment1/part1/#ros-packages-nodes","title":"ROS Packages &amp; Nodes","text":""},{"location":"course/assignment1/part1/#packages","title":"Packages","text":"<p>ROS applications are organised into packages. Packages are basically folders containing scripts, configurations and launch files (ways to launch those scripts and configurations), all of which relate to some common robot functionality. ROS uses packages as a way to organise all the programs running on a robot. </p> <p>Info</p> <p>The package system is a fundamental concept in ROS and all ROS programs are organised in this way.</p> <p>You will create a number of packages throughout this course, each containing different nodes, launch files and other things too. We'll start to explore this later on in this part of the course.</p>"},{"location":"course/assignment1/part1/#nodes","title":"Nodes","text":"<p>ROS Nodes are executables that perform specific robot tasks and operations. Earlier on we used <code>ros2 run</code> to execute a node called <code>teleop_keyboard</code>, which allowed us to remotely control (or \"teleoperate\") the robot, for example. </p> <p>Question</p> <p>What was the name of the ROS package that contained the <code>teleop_keyboard</code> node? (Remember: <code>ros2 run {[1] Package name} {[2] Node name}</code>)</p> <p>A ROS robot might have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to share data with the other nodes on the ROS Network.</p>"},{"location":"course/assignment1/part1/#the-ros-network","title":"The ROS Network","text":"<p>We can use the <code>ros2 node</code> command to view all the nodes that are currently active on a ROS Network.</p>"},{"location":"course/assignment1/part1/#ex2","title":"Exercise 2: Visualising the ROS Network","text":"<p>You should currently have two terminal instances active: the first in which you launched the Gazebo simulation (TERMINAL 1) and the second with your <code>teleop_keyboard</code> node active (TERMINAL 2).</p> <ol> <li>Open up a new terminal instance now (TERMINAL 3).</li> <li> <p>Use the following command to have a look at which nodes are currently active on the network:</p> <p>TERMINAL 3: </p><pre><code>ros2 node list\n</code></pre><p></p> <p>Only a handful of nodes should be listed:</p> <pre><code>/camera_driver\n/gazebo\n/teleop_keyboard\n/turtlebot3_diff_drive\n/turtlebot3_imu\n/turtlebot3_joint_state\n/turtlebot3_laserscan\n</code></pre> </li> <li> <p>We can visualise the connections between the active nodes by using an application called RQT. RQT is a collection of graphical tools that allow us to interact with and interrogate the ROS network. Launch the main RQT application by entering <code>rqt</code> in TERMINAL 3 (you might see some warnings in the terminal when you do this, but don't worry about them):</p> <p>TERMINAL 3: </p><pre><code>rqt\n</code></pre><p></p> <p>A window should then open:</p> <p></p> <p></p> </li> <li> <p>From here, we then want to load the Node Graph plugin. From the top menu select <code>Plugins</code> &gt; <code>Introspection</code> &gt; <code>Node Graph</code>.</p> </li> <li> <p>Select <code>Nodes/Topics (all)</code> from the top-left most dropdown, and in the <code>Hide</code> section uncheck everything except <code>Debug</code> and <code>Params</code> (you may then need to hit the refresh button):</p> <p></p> <p></p> <p>Here, nodes are represented by ellipses and topics by rectangles (hover over a region of the graph to enable colour highlighting).</p> <p>This tool shows us that (amongst other things) the <code>/teleop_keyboard</code> and <code>/turtlebot3_diff_drive</code> nodes are communicating with one another. The direction of the arrow tells us that <code>/teleop_keyboard</code> is a Publisher and <code>/turtlebot3_diff_drive</code> is a Subscriber. The two nodes communicate via a ROS Topic called <code>/cmd_vel</code>. </p> </li> </ol>"},{"location":"course/assignment1/part1/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. Data is published to topics using ROS Messages. As we've just learnt, the <code>teleop_keyboard</code> node was publishing messages to a topic (<code>/cmd_vel</code>) to make the robot move earlier.</p> <p>Let's have a look at this in a bit more detail...</p>"},{"location":"course/assignment1/part1/#ex3","title":"Exercise 3: Exploring ROS Topics and Messages","text":"<p>We can find out more about the <code>/cmd_vel</code> topic by using the <code>ros2 topic</code> command.</p> <ol> <li> <p>Open up yet another new terminal instance (TERMINAL 4) and type the following:</p> <p>TERMINAL 4: </p><pre><code>ros2 topic list\n</code></pre><p></p> <p>This shows us all the topics that are currently available on the ROS network (a lot of which we saw in the RQT Node Graph above):</p> <pre><code>/camera/camera_info\n/camera/image_raw\n/clock\n/cmd_vel\n/imu\n/joint_states\n/odom\n/parameter_events\n/performance_metrics\n/robot_description\n/rosout\n/scan\n/tf\n/tf_static\n</code></pre> <p>Let's find out a bit more about <code>/cmd_vel</code>...</p> </li> <li> <p>Use the <code>topic info</code> command now:</p> <p>TERMINAL 4: </p><pre><code>ros2 topic info /cmd_vel\n</code></pre><p></p> <p>This should provide the following output:</p> <pre><code>Type: geometry_msgs/msg/Twist\nPublisher count: 1\nSubscription count: 1\n</code></pre> <p>We've now established the following information about <code>/cmd_vel</code>: </p> <ol> <li>The topic has 1 publisher writing data to it</li> <li>The topic also has 1 subscriber reading this data</li> <li>From RQT Node Graph we know that the <code>/teleop_keyboard</code> node is the publisher (i.e. the node writing data to the topic)</li> <li>The <code>/turtlebot3_diff_drive</code> node is receiving this data (and acting upon it). This node therefore monitors (i.e. subscribes to) the <code>/cmd_vel</code> topic and makes the robot move in the simulator whenever a velocity command is published.</li> <li> <p>Data is transmitted on the <code>/cmd_vel</code> topic using an Interface. This particular interface is defined as: <code>geometry_msgs/msg/Twist</code>. </p> <p>The interface definition has three parts to it:</p> <ol> <li><code>geometry_msgs</code>: the name of the ROS package that this interface belongs to.</li> <li><code>msg</code>: that this is a topic message rather than another type of interface (there are three types of interface, and we'll learn about the other two later in this course).</li> <li><code>Twist</code>: the actual interface name</li> </ol> <p>Interfaces define the structure of the data that is broadcast on the ROS network, so that any node can deal with this data appropriately. </p> <p>In summary then, we've established that if we want to make the robot move we need to publish <code>Twist</code> messages to the <code>/cmd_vel</code> topic.</p> </li> </ol> </li> <li> <p>We can use the <code>ros2 interface</code> command to provide further information about the message structure:</p> <p>TERMINAL 4: </p><pre><code>ros2 interface show geometry_msgs/msg/Twist\n</code></pre><p></p> <p>From this, we obtain the following:</p> <pre><code># This expresses velocity in free space \n  broken into its linear and angular parts.\n\nVector3  linear\n        float64 x\n        float64 y\n        float64 z\nVector3  angular\n        float64 x\n        float64 y\n        float64 z\n</code></pre> <p>We'll learn more about what this means in Part 2.</p> </li> <li> <p>To finish, enter Ctrl+C in each of the three terminals that should currently have ROS processes running (Terminals 1, 2 and 3). The associated Gazebo and RQT Node Graph windows should close as a result of this too.</p> <p>Tip</p> <p>Whenever you need to stop any ROS process use Ctrl+C in the terminal it's running in.</p> </li> </ol>"},{"location":"course/assignment1/part1/#creating-your-first-ros-applications","title":"Creating Your First ROS Applications","text":"<p>Shortly you will create some simple publisher and subscriber nodes in Python and send simple ROS messages between them. As we learnt earlier though, ROS applications must be contained within packages, and so we need to create a package first in order to start creating our own ROS nodes. </p> <p>It's important to work in a specific filesystem location when we create and work on our own ROS packages. These are called \"Workspaces\" and you should already have one ready to go within your local ROS environment<sup>3</sup>:</p> <pre><code>~/ros2_ws/src/\n</code></pre> <p>Note</p> <p><code>~</code> is an alias for your home directory. So <code>cd ~/ros2_ws/src/</code> is the same as typing <code>cd /home/{your username}/ros2_ws/src/</code>.</p> <p>Important</p> <p>All new packages must be located in the <code>src</code> folder of the workspace!!</p>"},{"location":"course/assignment1/part1/#ex4","title":"Exercise 4: Creating your own ROS Package","text":"<p>The <code>ros2</code> Command Line Interface (CLI) includes a tool to create a new ROS packages: <code>ros2 pkg create</code>. This tool supports two different \"build types:\"</p> <ol> <li> <p>CMake (for packages containing nodes written in C++):</p> <p><code>ros2 pkg create --build-type ament_cmake</code></p> </li> <li> <p>Python (for packages containing nodes written in well, er, Python!):</p> <p><code>ros2 pkg create --build-type ament_python</code></p> <p>Packages are structured slightly differently in each case.</p> </li> </ol> <p>You can learn more about all this from the Official ROS2 Tutorials (if you're interested).</p> <p>We'll be using Python throughout this course, but we'll actually take a slightly different approach to package creation that will provide us with a little more flexibility and ease of use (particularly for things we'll do later on in the Assignment #1 course and in Assignment #2). We've therefore created a helper script (inside the <code>tuos_ros</code> Course Repo) to help you create packages without using either of the above two commands. The approach we'll take is based on this tutorial (courtesy of the Robotics Backend), so feel free to look at this if you'd like to find out more. Then, simply follow the steps below to create your first ROS package for this course, using the <code>create_pkg.sh</code> helper tool.</p> <ol> <li> <p>Navigate into the <code>tuos_ros</code> Course Repo that you downloaded earlier by using the Linux <code>cd</code> command (change directory). In TERMINAL 1 enter the following:</p> <p>TERMINAL 1: </p><pre><code>cd ~/ros2_ws/src/tuos_ros/\n</code></pre><p></p> </li> <li> <p>Here you'll find the <code>create_pkg.sh</code> helper script. Run this now using the following command to create a new package called <code>part1_pubsub</code>:</p> <p>TERMINAL 1: </p><pre><code>./create_pkg.sh part1_pubsub\n</code></pre><p></p> </li> <li> <p>Navigate into this new package directory (using <code>cd</code>):</p> <p>TERMINAL 1: </p><pre><code>cd ../part1_pubsub/\n</code></pre><p></p> <p>Info</p> <p><code>..</code> means \"go back one directory,\" so that command above is telling <code>cd</code> to navigate out of the <code>tuos_ros</code> directory (and therefore back to <code>~/ros2_ws/src/</code>), and then go into the <code>part1_pubsub</code> directory from there.</p> </li> <li> <p><code>tree</code> is a Linux command which shows us the content of the current directory in a nice tree-like format. Use <code>tree</code> now to show the current content of the <code>part1_pubsub</code> directory:</p> <pre><code>~/ros2_ws/src/part1_pubsub$ tree\n.\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 include\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 part1_pubsub\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 minimal_header.hpp\n\u251c\u2500\u2500 package.xml\n\u251c\u2500\u2500 part1_pubsub_modules\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tb3_tools.py\n\u251c\u2500\u2500 scripts\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 minimal_node.py\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 minimal_node.cpp\n\n5 directories, 7 files\n</code></pre> <ul> <li><code>scripts</code>: is a directory that will contain all the Python Nodes that we'll create (you'll notice a <code>minimal_node.py</code> already exists in there).</li> <li> <p><code>part1_pubsub_modules</code>: is a directory that we can use to store Python modules, that we can then import into our main Python nodes</p> <p>(<code>from part1_pubsub_modules.tb3_tools import ...</code>, for example)</p> </li> <li> <p><code>package.xml</code> and <code>CMakeLists.txt</code>: are both files that define our package, and how it must be built (using <code>colcon build</code>). We'll explore these more shortly... </p> </li> </ul> </li> </ol>"},{"location":"course/assignment1/part1/#ex5","title":"Exercise 5: Creating a publisher node","text":"<ol> <li> <p>From the root of your <code>part1_pubsub</code> package, navigate to the <code>scripts</code> folder using the <code>cd</code> command.</p> <p>TERMINAL 1: </p><pre><code>cd scripts\n</code></pre><p></p> </li> <li> <p><code>touch</code> is a Linux command that we can use to create an empty file. Use this to create an empty file called <code>publisher.py</code>, which we will add content to shortly:</p> <p>TERMINAL 1: </p><pre><code>touch publisher.py\n</code></pre><p></p> </li> <li> <p>Use <code>ls</code> to verify that the file has been created, but use the <code>-l</code> option with this, so that the command provides its output in \"a long listing format\":</p> <p>TERMINAL 1: </p><pre><code>ls -l\n</code></pre><p></p> <p>This should output something similar to the following:</p> <pre><code>~/ros2_ws/src/part1_pubsub/scripts$ ls -l\ntotal 4\n-rwxr-xr-x 1 student student 339 MMM DD HH:MM minimal_node.py\n-rw-r--r-- 1 student student   0 MMM DD HH:MM publisher.py\n</code></pre> <p>This confirms that the file exists, and the <code>0</code> in the middle of the bottom line there indicates that the file is empty (i.e. its current size is 0 bytes), which is what we'd expect.</p> </li> <li> <p>We therefore now need to open the file and add content to it. We'd recommend using Visual Studio Code (VS Code) as an IDE for this course, which can be launched with the following command in TERMINAL 1:</p> <p>TERMINAL 1: </p><pre><code>code ~\n</code></pre><p></p> </li> <li> <p>Using the VS Code File Explorer, locate the <code>publisher.py</code> file that you have just created (<code>ros2_ws/src/part1_pubsub/scripts/</code>) and click on the file to open it in the main editor. </p> </li> <li> <p>Once opened, copy the code provided here into the empty file and save it. </p> <p>Note</p> <p>It's important that you understand how this code works, so make sure that you read the annotations!</p> </li> <li> <p>Next, we need to add our <code>publisher.py</code> file as an executable to our package's <code>CMakeLists.txt</code>. This will ensure that it then gets built when we run <code>colcon build</code> (in the next step).</p> <p>In VS Code, open the <code>CMakeLists.txt</code> file that is at the root of your <code>part1_pubsub</code> package directory (<code>ros2_ws/src/part1_pubsub/CMakeLists.txt</code>). Locate the lines (near the bottom of the file) that read:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/minimal_node.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> <p>Replace <code>minimal_node.py</code> with <code>publisher.py</code> to define this as a Python executable in your package:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/publisher.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Now, use <code>colcon</code> to build your package.</p> <ol> <li> <p>You MUST run this from the root of your Colcon Workspace (i.e.: <code>~/ros2_ws/</code>), NOT the <code>src</code> directory (<code>~/ros2_ws/src/</code>), so navigate there now using <code>cd</code>:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Then, use the following <code>colcon</code> command to build your package:</p> <pre><code>colcon build --packages-select part1_pubsub --symlink-install\n</code></pre> <p>What do the additional arguments above do?</p> <ul> <li><code>--packages-select</code>: Build only the <code>part1_pubsub</code> package, nothing else (without this <code>colcon</code> would attempt to build every package in the workspace).</li> <li><code>--symlink-install</code>: Ensures that you don't have to re-run <code>colcon build</code> every time you make a change to your package's executables (i.e. your Python files in the <code>scripts</code> directory).</li> </ul> </li> <li> <p>Finally, \"re-source\" your <code>bashrc</code><sup>4</sup>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>We should now be able to run this node using the <code>ros2 run</code> command. </p> <p>Remember: <code>ros2 run {package name} {script name}</code>, so:</p> <p>TERMINAL 1: </p><pre><code>ros2 run part1_pubsub publisher.py\n</code></pre><p></p> <p>... Hmm, something not quite right? If you typed the command exactly as above and then tried to run it, you probably just received the following error:</p> <pre><code>$ ros2 run part1_pubsub publisher.py\nNo executable found\n</code></pre> <p></p> <p>When we create a file using <code>touch</code> it is given certain permissions by default. Run <code>ls -l</code> again (making sure that your terminal is in the right location: <code>~/ros2_ws/src/part1_pubsub/scripts/</code>).</p> <p>The first bit tells us about the permissions that are currently set: <code>-rw-r--r--</code>. This tells us who has permission to do what with this file and (currently) the first bit: <code>-rw-</code>, tells us that we have permission to read or write to it. There is a third option we can set too though, which is the execute permission, and we can set this using the <code>chmod</code> Linux command...</p> </li> <li> <p>Run the <code>chmod</code> command as follows:</p> <p>TERMINAL 1: </p><pre><code>chmod +x publisher.py\n</code></pre><p></p> </li> <li> <p>Now, run <code>ls -l</code> again to see what has changed:</p> <p>TERMINAL 1: </p><pre><code>ls -l\n</code></pre><p></p> <p>We have now granted permission for the file to be executed too:</p> <pre><code>-rwxr-xr-x 1 student student 1125 MMM DD HH:MM publisher.py\n</code></pre> </li> <li> <p>OK, now use <code>ros2 run</code> again to (hopefully!) run the <code>publisher.py</code> node (remember: <code>ros2 run {package name} {script name}</code>).</p> <p>If you see a message in the terminal similar to the following then the node has been launched successfully:</p> <pre><code>[INFO] [#####] [simple_publisher]: The 'simple_publisher' node is inisialised.\n</code></pre> <p>Phew!</p> </li> <li> <p>We can further verify that our publisher node is running using a number of different tools. Try running the following commands in TERMINAL 2:</p> <ol> <li><code>ros2 node list</code>: This will provide a list of all the nodes that are currently active on the system. Verify that the name of our publisher node is visible in this list (it's probably the only item in the list at the moment!)</li> <li><code>ros2 topic list</code>: This will provide a list of the topics that are currently being used by nodes on the system. Verify that the name of the topic that our publisher is publishing messages to (<code>/my_topic</code>) is present within this list.</li> </ol> </li> </ol>"},{"location":"course/assignment1/part1/#rostopic","title":"Interrogating ROS Topics","text":"<p>So far we have used the <code>ros2 topic</code> ROS command with two additional arguments:</p> <ul> <li><code>list</code>: to provide us with a list of all the topics that are active on our ROS system, and</li> <li><code>info</code>: to provide us with information on a particular topic of interest.</li> </ul> <p>We can find out what other sub-commands are available for us to use with <code>ros2 topic</code> by calling for help! </p> <p>TERMINAL 2: </p><pre><code>ros2 topic --help\n</code></pre><p></p> <p>Which should provide us with a list of all the options:</p> <pre><code>Commands:\n  bw     Display bandwidth used by topic\n  delay  Display delay of topic from timestamp in header\n  echo   Output messages from a topic\n  find   Output a list of available topics of a given type\n  hz     Print the average publishing rate to screen\n  info   Print information about a topic\n  list   Output a list of available topics\n  pub    Publish a message to a topic\n  type   Print a topic's type\n\n  Call `ros2 topic &lt;command&gt; -h` for more detailed usage.\n</code></pre> <p>Let's talk about a few of these:</p> <ul> <li> <p><code>ros2 topic hz {topic name}</code> provides information on the frequency (in Hz) at which messages are being published to a topic:</p> <pre><code>ros2 topic hz /my_topic\n</code></pre> <p>This should tell us that our publisher node is publishing messages to the <code>/my_topic</code> topic at (or close to) 1 Hz, which is exactly what we ask for in the <code>publisher.py</code> file (in the <code>__init__</code> part of our <code>Publisher</code> class). Enter Ctrl+C to stop this command.</p> </li> <li> <p><code>ros2 topic echo {topic name}</code> shows the messages being published to a topic:</p> <pre><code>ros2 topic echo /my_topic\n</code></pre> <p>This will provide a live stream of the messages that our <code>publisher.py</code> node is publishing to the <code>/my_topic</code> topic. Enter Ctrl+C to stop this.</p> </li> <li> <p>We can see some additional options for the <code>echo</code> command by viewing the help documentation for this too:</p> <pre><code>ros2 topic echo --help\n</code></pre> <p>From here, for instance, we can learn that if we just wanted to print the first message that was received we could use the <code>--once</code> option, for example:</p> <pre><code>ros2 topic echo /my_topic --once\n</code></pre> </li> </ul>"},{"location":"course/assignment1/part1/#ex6","title":"Exercise 6: Creating a subscriber node","text":"<p>To illustrate how information can be passed from one node to another (via topics and messages) we'll now create another node to subscribe to the topic that our publisher node is broadcasting messages to.</p> <ol> <li> <p>In TERMINAL 2 use the filesystem commands that were introduced earlier (<code>cd</code>, <code>ls</code>, etc.) to navigate to the <code>scripts</code> folder of your <code>part1_pubsub</code> package.</p> </li> <li> <p>Use the same procedure as before to create a new empty Python file called <code>subscriber.py</code> and remember to make it executable! </p> </li> <li> <p>Then, open the newly created <code>subscriber.py</code> file in VS Code, paste in the code here and save it. </p> <p>Once again, it's important that you understand how this code works, so make sure you read the code annotations! </p> </li> <li> <p>Next, we need to add this as an additional executable for our package. </p> <p>Open up the <code>CMakeLists.txt</code> file at the root of your <code>part1_pubsub</code> package directory again, head back to the <code># Install Python executables</code> section and add the <code>subscriber.py</code> file:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/publisher.py\n  scripts/subscriber.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Now we need to <code>colcon build</code> again.</p> <ol> <li> <p>Make sure you're at the root of the Colcon Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Run <code>colcon build</code> on only the <code>part1_pubsub</code> package:</p> <pre><code>colcon build --packages-select part1_pubsub --symlink-install\n</code></pre> </li> <li> <p>And then re-source the <code>bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Use <code>ros2 run</code> to execute your newly created <code>subscriber.py</code> node (remember: <code>ros2 run {package name} {script name}</code>). If your publisher and subscriber nodes are working correctly you should see an output like this:</p> <p></p> <p></p> </li> <li> <p>Interrogate your ROS network:</p> <ol> <li> <p>As before, we can find out what nodes are running on our system by using the <code>ros2 node list</code> command. Run this in TERMINAL 3, you should see both your publisher and subscriber nodes listed there.</p> </li> <li> <p>Use the <code>ros2 topic</code> command to list all the topics that are available on the network. You should see <code>/my_topic</code> listed there.</p> </li> <li> <p>Use the <code>ros2 topic</code> command again to find more info on <code>my_topic</code>. </p> </li> <li> <p>Use the <code>ros2 interface</code> command to show you what type of data is being sent between the two nodes.</p> </li> </ol> </li> <li> <p>Finally, close down your publisher and subscriber nodes by entering Ctrl+C in the terminals where they are running (should be 1 &amp; 2).</p> </li> </ol>"},{"location":"course/assignment1/part1/#ex7","title":"Exercise 7: Defining our own message","text":"<p>We've just created a publisher and subscriber that were able to communicate with one another via a topic. The data that the publisher was sending to the topic was very simple: a <code>example_interfaces/msg/String</code> type message.</p> <p>This message just has one field called <code>data</code> of the type <code>string</code>:</p> <pre><code>$ ros2 interface show example_interfaces/msg/String\n\nstring data\n</code></pre> <p>ROS messages will generally be more complex than this, typically containing several fields in a single message. We'll define our own custom message now, this time with two fields, so you can see how things work with slightly more complex data types. </p> <ol> <li> <p>Message interfaces must be defined within a <code>msg</code> folder at the root of our package directory, so let's create this folder now in TERMINAL 1:</p> <ol> <li> <p>First, navigate into your package:</p> <pre><code>cd ~/ros2_ws/src/part1_pubsub\n</code></pre> </li> <li> <p>Then use <code>mkdir</code> to make a new directory:</p> <pre><code>mkdir msg\n</code></pre> </li> </ol> </li> <li> <p>We'll create a message called <code>Example</code>, and to do this we'll need to create a new file called <code>Example.msg</code> inside the <code>msg</code> folder:</p> <pre><code>touch msg/Example.msg\n</code></pre> </li> <li> <p>To define the data structure of this message, we now need to open up the file and add the following content:</p> Example.msg<pre><code>string info\nint32 time\n</code></pre> <p>The message will therefore have two fields:</p> <p></p><p></p> # Field Name Data Type 1 <code>info</code> <code>string</code> 2 <code>time</code> <code>int32</code> <p></p><p></p> <p>We can give our fields any name that we want, but the data types must be either built-in-types or other pre-existing ROS interfaces.</p> </li> <li> <p>We now need to declare this message in our package's <code>CMakeLists.txt</code> file, so that the necessary Python code can be created (by <code>colcon build</code>) to allow us to import this message into our own Python files.</p> <p>Add the following lines to your <code>part1_pubsub/CMakeLists.txt</code> file, above the <code>ament_package()</code> line:</p> CMakeLists.txt<pre><code>find_package(rosidl_default_generators REQUIRED)\nrosidl_generate_interfaces(${PROJECT_NAME}\n  \"msg/Example.msg\" \n)\n</code></pre> </li> <li> <p>We also need to modify our <code>package.xml</code> file. Add the following lines to this one, just above the <code>&lt;export&gt;</code> line:</p> package.xml<pre><code>&lt;buildtool_depend&gt;rosidl_default_generators&lt;/buildtool_depend&gt;\n&lt;exec_depend&gt;rosidl_default_runtime&lt;/exec_depend&gt;\n&lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;\n</code></pre> </li> <li> <p>We can now use Colcon to generate the necessary source code for the message:</p> <ol> <li> <p>First, make sure you're in the root of the ROS2 Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Then run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part1_pubsub --symlink-install \n</code></pre> </li> <li> <p>And finally re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>We can now verify that this worked with some more <code>ros2</code> command line tools:</p> <ol> <li> <p>First, list all the ROS messages that are available to us on our system:</p> <pre><code>ros2 interface list -m\n</code></pre> <p>Scroll through this list and see if you can find our message in there (it'll be listed as <code>part1_pubsub/msg/Example</code>)</p> </li> <li> <p>Next, show the data structure of the interface:</p> <pre><code>ros2 interface show part1_pubsub/msg/Example\n</code></pre> <p>This should match with how we defined it in our <code>part1_pubsub/msg/Example.msg</code> file.</p> </li> </ol> </li> </ol>"},{"location":"course/assignment1/part1/#ex8","title":"Exercise 8: Using a custom ROS Message","text":"<ol> <li> <p>Create a copy of the <code>publisher.py</code> file from Exercise 5. Let's do this from the command line too:</p> <ol> <li> <p>Navigate into your package's <code>scripts</code> folder:</p> <pre><code>cd ~/ros2_ws/src/part1_pubsub/scripts\n</code></pre> </li> <li> <p>And use the <code>cp</code> command to make a copy of the <code>publisher.py</code> file and call this new file <code>custom_msg_publisher.py</code>:</p> <pre><code>cp publisher.py custom_msg_publisher.py\n</code></pre> </li> <li> <p>Let's create a copy of the <code>subscriber.py</code> file too, while we're here:</p> <pre><code>cp subscriber.py custom_msg_subscriber.py\n</code></pre> </li> </ol> </li> <li> <p>Declare these two new files as additional executables in our <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/publisher.py\n  scripts/subscriber.py\n  scripts/custom_msg_publisher.py  # ADD THIS \n  scripts/custom_msg_subscriber.py # AND THIS\nDESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Run Colcon again (last time now!):</p> <ol> <li>First:     <pre><code>cd ~/ros2_ws\n</code></pre></li> <li>Then:     <pre><code>colcon build --packages-select part1_pubsub --symlink-install\n</code></pre></li> <li>And finally:     <pre><code>source ~/.bashrc\n</code></pre></li> </ol> </li> <li> <p>Now modify your <code>custom_msg_publisher.py</code> file as follows:</p> custom_msg_publisher.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\n\nfrom part1_pubsub.msg import Example # (1)!\n\nclass SimplePublisher(Node):\n\n    def __init__(self):\n        super().__init__(\"simple_publisher\")\n\n        self.my_publisher = self.create_publisher(\n            msg_type=Example, # (2)!\n            topic=\"my_topic\",\n            qos_profile=10,\n        )\n\n        publish_rate = 1 # Hz\n        self.timer = self.create_timer(\n            timer_period_sec=1/publish_rate,\n            callback=self.timer_callback\n        )\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\"\n        )\n\n    def timer_callback(self):\n        ros_time = self.get_clock().now().seconds_nanoseconds()\n\n        topic_msg = Example() # (3)!\n        topic_msg.info = \"The ROS time is...\"\n        topic_msg.time = ros_time[0]\n        self.my_publisher.publish(topic_msg)\n        self.get_logger().info(\n            f\"Publishing: '{topic_msg.info} {topic_msg.time:.0f}'\"\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    my_simple_publisher = SimplePublisher()\n    rclpy.spin(my_simple_publisher)\n    my_simple_publisher.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>We're now importing the <code>Example</code> message from our own <code>part1_pubsub</code> package.</p> </li> <li> <p>We're also now declaring that <code>\"my_topic\"</code> will use the <code>Example</code> message data structure to send messages.</p> </li> <li> <p>We need to deal with the topic messages differently now, to account for the more complex structure.</p> <p>We now populate our messages with two fields: <code>info</code> (a <code>string</code>) and <code>time</code> (an <code>int</code>). Identify what has changed here...</p> </li> </ol> </li> <li> <p>Final Task:</p> <p>Modify the <code>custom_msg_subscriber.py</code> node now to accommodate the new message type that is being published to <code>/my_topic</code>. </p> </li> </ol>"},{"location":"course/assignment1/part1/#wrapping-up","title":"Wrapping Up","text":"<p>In this session we've covered the basics of ROS, and learnt about some key concepts such as Packages; Nodes; and how to send data across a ROS Network using Topics, Messages, and the Publisher-Subscriber Communication Method.</p> <p>We've learnt how to use some key <code>ros2</code> commands:  </p> <ul> <li><code>launch</code>: to launch multiple ROS Nodes via launch files.</li> <li><code>run</code>: to run executables within a ROS package.</li> <li><code>node</code>: to display information about active ROS Nodes.</li> <li><code>topic</code>: to display information about active ROS topics.</li> <li><code>interface</code>: to display information about all ROS Interfaces that are available to use in a ROS application.</li> </ul> <p>We have also learnt how to work in the Linux Terminal and navigate a Linux filesystem using key commands such as:</p> <ul> <li><code>ls</code>: lists the files in the current directory.</li> <li><code>cd</code>: change directory to move around the file system.</li> <li><code>mkdir</code>: make a new directory (<code>mkdir {new_folder}</code>).</li> <li><code>chmod</code>: modify file permissions (i.e. to add execute permissions to a file for all users: <code>chmod +x {file}</code>).</li> <li><code>touch</code>: create a file without any content.</li> </ul> <p>In addition to this we've also learnt how to create a ROS2 package, and how to create simple Python nodes that can publish and subscribe to topics on a ROS network. </p> <p>We've worked with pre-made ROS messages to do this and also created our own custom message interface to offer more advanced functionality.</p>"},{"location":"course/assignment1/part1/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, the work you have done in the WSL-ROS2 environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS2 Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University <code>U:\\</code> Drive, allowing you to restore it on another managed desktop machine the next time you fire up WSL-ROS2.  </p> <ol> <li> <p>What is a ROS2 Workspace? You can find out more here.\u00a0\u21a9</p> </li> <li> <p>What is Colcon? Find out more here.\u00a0\u21a9</p> </li> <li> <p>You can learn more about ROS2 Workspaces here.\u00a0\u21a9</p> </li> <li> <p>What does <code>source ~/.bashrc</code> do? See here for an explanation.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment1/part2/","title":"Part 2: Odometry & Navigation","text":""},{"location":"course/assignment1/part2/#introduction","title":"Introduction","text":"<p> Exercises: 6 Estimated Completion Time: 3 hours Difficulty Level: Intermediate  </p>"},{"location":"course/assignment1/part2/#aims","title":"Aims","text":"<p>In Part 2 you will learn how to control a ROS robot's position and velocity from both the command line and through ROS Nodes. You will also learn how to interpret the data that allows us to monitor a robot's position in its physical environment (odometry).  The things you will learn here form the basis for all robot navigation in ROS, from simple open-loop methods to more advanced closed-loop control (both of which you will explore).</p>"},{"location":"course/assignment1/part2/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Interpret the Odometry data published by a ROS Robot and identify the parts of these messages that are relevant to a 2-wheeled differential drive robot (such as the TurtleBot3).</li> <li>Develop Python nodes to obtain Odometry messages from an active ROS network and translate them to provide useful information about a robot's pose in a convenient, human-readable way.</li> <li>Implement open-loop velocity control of a robot using ROS command-line tools.</li> <li>Develop Python nodes that use open-loop velocity control methods to make a robot follow a pre-defined motion path.</li> <li>Combine both publisher &amp; subscriber communication methods into a single Python node to implement closed-loop (odometry-based) velocity control of a robot.</li> <li>Explain the limitations of Odometry-based motion control methods. </li> </ol>"},{"location":"course/assignment1/part2/#quick-links","title":"Quick Links","text":""},{"location":"course/assignment1/part2/#exercises","title":"Exercises","text":"<ul> <li>Exercise 1: Exploring Odometry Data</li> <li>Exercise 2: Creating a Python Node to Process Odometry Data</li> <li>Exercise 3: Controlling Velocity with the ROS2 CLI</li> <li>Exercise 4: Creating a Python Node to Make a Robot Move in a circle</li> <li>Exercise 5: Implementing a Shutdown Procedure</li> <li>Exercise 6: Making our Robot Follow a Square Motion Path</li> </ul>"},{"location":"course/assignment1/part2/#additional-resources","title":"Additional Resources","text":"<ul> <li>An Odometry Subscriber Node</li> <li>A Simple Velocity Control Node (Move Circle)</li> <li>Odometry-based Navigation (Move Square)</li> </ul>"},{"location":"course/assignment1/part2/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>If you haven't done so already, launch your ROS environment now:</p> <ol> <li>WSL-ROS2 on a university managed desktop: follow the instructions here to launch it.</li> <li>Running WSL-ROS2 on your own machine: launch the Windows Terminal to access a WSL-ROS2 terminal instance.</li> <li>Other Users: follow the relevant instructions.</li> </ol> <p>You should now have access to ROS via a Linux terminal instance. We'll refer to this terminal instance as TERMINAL 1.</p> <p>Step 2: Restore your work (WSL-ROS Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS2 Environment will not be preserved between sessions or across different University computers. At the end of Part 1 you should have run the <code>wsl_ros</code> tool to back up your home directory to your University <code>U:\\</code> Drive. Once WSL-ROS2 is up and running, you should be prompted to restore this:</p> <pre><code>It looks like you already have a backup from a previous session:\n  U:\\wsl-ros\\ros2-backup-XXX.tar.gz\nDo you want to restore this now? [y/n]\n</code></pre> <p>Enter Y+Enter to restore your work from last time. You can also restore your work at any time using the following command:</p> <pre><code>wsl_ros restore\n</code></pre> <p>Step 3: Launch VS Code </p> <p>It's also worth launching VS Code now, so that it's ready to go for when you need it later on. </p> WSL Users... <p>It's important to launch VS Code within your WSL environment using the \"WSL\" extension. Always remember to check for this:</p> <p></p> <p></p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p></p> <p>In Part 1 you should have downloaded and installed The Course Repo into your ROS environment. If you haven't done this yet then go back and do it now. If you have already done it, then it's worth just making sure it's all up-to-date, so run the following command now to do so:</p> <p>TERMINAL 1: </p><pre><code>cd ~/ros2_ws/src/tuos_ros/ &amp;&amp; git pull\n</code></pre><p></p> <p>Then build with Colcon: </p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-up-to tuos_ros\n</code></pre> <p>And finally, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Warning</p> <p>If you have any other terminal instances open, then you'll need run <code>source ~/.bashrc</code> in these too, in order for any changes made by the Colcon build process to propagate through to these as well.</p> <p>Step 5: Launch a Waffle Simulation</p> <p>In TERMINAL 1 enter the following command to launch a simulation of a TurtleBot3 Waffle in an empty world:  </p> <p>TERMINAL 1: </p><pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre><p></p> <p>A Gazebo simulation window should open and within this you should see a TurtleBot3 Waffle in empty space:</p> <p></p>"},{"location":"course/assignment1/part2/#velocity","title":"Velocity (Motion)","text":"<p>In Part 1 we learnt about ROS Topics, and about how the <code>teleop_keyboard</code> node could be used to publish messages to a particular topic in order to control the velocity of the robot (and thus change its position).</p> <p>Questions</p> <ol> <li>Which topic is used to control the velocity of the robot?</li> <li>What interface does this topic use?</li> </ol> <p>Return here if you need a reminder on how to find the answers to these questions.</p> <p>We also learnt how to find out more about this particular interface (using the <code>ros2 interface show</code> command): </p> <pre><code>Vector3  linear\n        float64 x\n        float64 y\n        float64 z\nVector3  angular\n        float64 x\n        float64 y\n        float64 z\n</code></pre>"},{"location":"course/assignment1/part2/#velocity-commands","title":"Velocity Commands","text":"<p>When defining velocity commands for a ROS robot, there are six \"fields\" that we can assign values to: two velocity types, each with three velocity components: </p> <p></p><p></p> Velocity Type Component 1 Component 2 Component 3 <code>linear</code> <code>x</code> <code>y</code> <code>z</code> <code>angular</code> <code>x</code> <code>y</code> <code>z</code> <p></p><p></p> <p>These relate to a robot's six degrees of freedom, and the topic messages are therefore formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <p></p><p></p> Component (Axis) Linear Velocity Angular Velocity X \"Forwards/Backwards\" \"Roll\" Y \"Left/Right\" \"Pitch\" Z \"Up/Down\" \"Yaw\" <p></p><p></p>"},{"location":"course/assignment1/part2/#the-degrees-of-freedom-of-a-waffle","title":"The Degrees of Freedom of a Waffle","text":"<p>The three \"axes\" in the table above are termed the \"Principal Axes.\" In the context of our TurtleBot3 Waffle, these axes and the motion about them are defined as follows:</p> <p></p> <p></p> <p>As discussed above, a mobile robot can have up to six degrees of freedom in total, but this depends upon the robot's design and the actuators it is equipped with. </p> <p>Our TurtleBot3 Waffles only have two motors. These two motors can be controlled independently (in what is known as a \"differential drive\" configuration), which ultimately provides it with a total of two degrees of freedom overall, as illustrated below.</p> <p></p> <p>When issuing Velocity Commands therefore, only two (of the six) velocity command fields are applicable: linear velocity in the x-axis (Forwards/Backwards) and angular velocity about the z-axis (Yaw).</p> <p></p><p></p> Principal Axis Linear Velocity Angular Velocity X \"Forwards/Backwards\" \"Roll\" Y \"Left/Right\" \"Pitch\" Z \"Up/Down\" \"Yaw\" <p></p><p></p> <p></p> <p>Maximum Velocity Limits</p> <p>Keep in mind (while we're on the subject of velocity) that our TurtleBot3 Waffles have maximum velocity limits:</p> <p></p><p></p> Velocity Component Upper Limit Units Linear (X) 0.26 m/s Angular (Z) 1.82 rad/s <p></p><p></p>"},{"location":"course/assignment1/part2/#odometry","title":"Odometry (Position)","text":""},{"location":"course/assignment1/part2/#odometry-in-action","title":"Odometry In Action","text":"<p>Let's take another look at all the topics that can be used to communicate with our robot:</p> <pre><code>ros2 topic list\n</code></pre> <p>Another topic of interest here is <code>/odom</code>. This topic contains Odometry data, which is also essential for robot navigation, giving us an approximation of a robot's location in its environment.</p> <p>Let's explore this further now, using <code>rqt</code>.</p>"},{"location":"course/assignment1/part2/#ex1","title":"Exercise 1: Exploring Odometry Data","text":"<ol> <li> <p>In TERMINAL 2 launch <code>rqt</code>:</p> <p>TERMINAL 2: </p><pre><code>rqt\n</code></pre><p></p> </li> <li> <p>From the top menu select <code>Plugins</code> &gt; <code>Topics</code> &gt; <code>Topic Monitor</code></p> <p>Topic Monitor should then present you with a list of active topics which matches the topic list from the <code>ros2 topic list</code> command that you ran earlier.</p> </li> <li> <p>Check the box next to <code>/odom</code> and click the arrow next to it to expand the topic and reveal four base fields.</p> </li> <li> <p>Expand the <code>pose</code> &gt; <code>pose</code> &gt; <code>position</code> and <code>orientation</code> fields to reveal the data being published to the three position and four orientation values of this message.</p> </li> <li> <p>Also expand the <code>twist</code> &gt; <code>twist</code>, <code>linear</code> and <code>angular</code> fields to reveal the six values being published here too.</p> <p></p> <p></p> </li> <li> <p>Next, launch a new terminal instance, we'll call this one TERMINAL 3. Arrange this next to the <code>rqt</code> window, so that you can see them both simultaneously.</p> </li> <li> <p>In TERMINAL 3 launch the <code>teleop_keyboard</code> node as you did in Part 1: </p> <p>TERMINAL 3: </p><pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre><p></p> </li> <li> <p>Enter A a couple of times to make the robot rotate on the spot. Observe how the odometry data changes in Topic Monitor.</p> <p>Questions</p> <ol> <li>Which <code>pose</code> fields are changing?</li> <li>Is there anything in the <code>twist</code> part of the message that corresponds to the angular velocity that is being published by the <code>teleop_keyboard</code> node in TERMINAL 3? </li> </ol> </li> <li> <p>Now press the S key to halt the robot, then press W a couple of times to make the robot drive forwards.</p> <p>Questions</p> <ol> <li>Which <code>pose</code> fields are changing now? How does this relate to the position of the robot in the simulated world?</li> <li>How does the <code>twist</code> part of the message now correspond with the linear velocity setting in TERMINAL 3?</li> </ol> </li> <li> <p>Now press D a couple of times and your robot should start to move in a circle.</p> <p>Questions</p> <ol> <li>What linear and angular velocities are you requesting in TERMINAL 3, and how are these represented in the <code>twist</code> part of the <code>/odom</code> message?</li> <li>What about the <code>pose</code> part of the message? How is this data changing as your robot moves in a circular path.</li> <li>What are <code>twist</code> and <code>pose</code> actually telling us?</li> </ol> </li> <li> <p>Press S in TERMINAL 3 to stop the robot (but leave the <code>teleop_keyboard</code> node running).  Then, press Ctrl+C in TERMINAL 2 to close down <code>rqt</code>. </p> </li> <li> <p>Let's look at the Odometry data differently now. With the robot stationary, use <code>ros2 run</code> to run a Python node from the <code>tuos_examples</code> package: </p> <p>TERMINAL 2: </p><pre><code>ros2 run tuos_examples robot_pose.py\n</code></pre><p></p> </li> <li> <p>Now (using the <code>teleop_keyboard</code> node in TERMINAL 3) drive your robot around again, keeping an eye on the outputs that are being printed by the <code>robot_pose.py</code> node in TERMINAL 2 as you do so.</p> <p>The output of the <code>robot_pose.py</code> node shows you how the robot's position and orientation (i.e. \"pose\") are changing in real-time as you move the robot around. The <code>\"initial\"</code> column tells us the robot's pose when the node was first launched, and the <code>\"current\"</code> column show us what its pose currently is. The <code>\"delta\"</code> column then shows the difference between the two.</p> <p>Question</p> <p>Which pose parameters haven't changed, and is this what you would expect (considering the robot's principal axes, as illustrated above)?</p> </li> <li> <p>Press Ctrl+C in TERMINAL 2 and TERMINAL 3, to stop the <code>robot_pose.py</code> and <code>teleop_keyboard</code> nodes. </p> </li> </ol>"},{"location":"course/assignment1/part2/#odometry-explained","title":"Odometry: Explained","text":"<p>Hopefully you have a good idea of what Odometry is now, but let's dig a little deeper using some key ROS command line tools again:</p> <p>TERMINAL 2: </p><pre><code>ros2 topic info /odom\n</code></pre><p></p> <p>This provides information about the interface used by this topic:</p> <pre><code>Type: nav_msgs/msg/Odometry\n</code></pre> <p>We can find out more about this interface using the <code>ros2 interface show</code> command:</p> <p>TERMINAL 2: </p><pre><code>ros2 interface show nav_msgs/msg/Odometry\n</code></pre><p></p> <p>Look down the far left-hand side to identify the four base fields of the interface (i.e. the fields that are not indented):</p> <p></p> <p></p><p></p> # Field Name Field Type 1 <code>header</code> <code>std_msgs/Header</code> 2 <code>child_frame_id</code> <code>string</code> 3 <code>pose</code> <code>geometry_msgs/PoseWithCovariance</code> 4 <code>twist</code> <code>geometry_msgs/TwistWithCovariance</code> <p></p><p></p> <p>We saw all these in <code>rqt</code> earlier. As before, its items 3 and 4 that are of most interest to us...</p>"},{"location":"course/assignment1/part2/#pose","title":"Pose","text":"<pre><code># Estimated pose that is typically relative to a fixed world frame.\ngeometry_msgs/PoseWithCovariance pose\n        Pose pose\n                Point position\n                        float64 x\n                        float64 y\n                        float64 z\n                Quaternion orientation\n                        float64 x\n                        float64 y\n                        float64 z\n                        float64 w\n        float64[36] covariance\n</code></pre> <p>As you can see above, there are two key components to Pose:</p> <ol> <li><code>position</code>: Tells us where our robot is located in 3-dimensional space. This is expressed in units of meters.</li> <li><code>orientation</code>: Tells us which way our robot is pointing in its environment. This is expressed in units of Quaternions, which is a mathematically convenient way to store data related to a robot's orientation (it's a bit hard for us humans to understand and visualise this though, so we'll talk about how to convert it to a different format later).</li> </ol> <p>Pose is defined relative to an arbitrary reference point (typically where the robot was when it was turned on), and is determined from:</p> <ul> <li>Data from the Inertial Measurement Unit (IMU) on the OpenCR board</li> <li>Data from both the left and right wheel encoders</li> <li>A kinematic model of the robot</li> </ul> <p>All the above information can then be used to calculate (and keep track of) the distance travelled by the robot from its pre-defined reference point using a process called \"dead-reckoning.\"</p>"},{"location":"course/assignment1/part2/#what-are-quaternions","title":"What are Quaternions?","text":"<p>Quaternions use four values to represent the orientation of something in 3 dimensional space<sup>1</sup>, as we can observe from the structure of the <code>nav_msgs/msg/Odometry</code> ROS interface:</p> <pre><code>Quaternion orientation\n        float64 x\n        float64 y\n        float64 z\n        float64 w\n</code></pre> <p>For us, it's easier to think about the orientation of our robot in a \"Euler Angle\" representation, which tell us the degree of rotation about the three principal axes (as discussed above):</p> <ul> <li>\\(\\theta_{x}\\), aka: \"Roll\"</li> <li>\\(\\theta_{y}\\), aka: \"Pitch\"</li> <li>\\(\\theta_{z}\\), aka: \"Yaw\"</li> </ul> <p>Fortunately, the maths involved in converting between these two orientation formats is fairly straight forward (see here).</p> <p>Recall from above however, that our TurtleBot3 can only move in a 2D plane (unfortunately, it can't fly!) and so, actually, its pose can be fully represented by just 3 terms: </p> <ul> <li>\\(x\\) &amp; \\(y\\): the 2D coordinates of the robot in the <code>X-Y</code> plane</li> <li>\\(\\theta_{z}\\): the angle of the robot about the <code>z</code> (yaw) axis</li> </ul>"},{"location":"course/assignment1/part2/#twist","title":"Twist","text":"<p>The fourth base field within the <code>nav_msgs/msg/Odometry</code> interface is Twist:</p> <pre><code># Estimated linear and angular velocity relative to child_frame_id.\ngeometry_msgs/TwistWithCovariance twist\n        Twist twist\n                Vector3  linear\n                        float64 x\n                        float64 y\n                        float64 z\n                Vector3  angular\n                        float64 x\n                        float64 y\n                        float64 z\n        float64[36] covariance\n</code></pre> <p>This might look familiar from earlier! This tells us the current linear and angular velocities of the robot. These velocities are set by messages published to <code>/cmd_vel</code>, but are then monitored by data coming directly from the robot's wheel encoders, and are provided here as a feedback signal.</p>"},{"location":"course/assignment1/part2/#odometry-data-as-a-feedback-signal","title":"Odometry Data as a Feedback Signal","text":"<p>Odometry data can be really useful for robot navigation, allowing us to keep track of where a robot is, how it's moving and how to get back to where we started. We therefore need to know how to use odometry data effectively within our Python nodes, and we'll explore this now.</p>"},{"location":"course/assignment1/part2/#ex2","title":"Exercise 2: Creating a Python Node to Process Odometry Data","text":"<p>In Part 1 we learnt how to create a package and build simple Python nodes to publish and subscribe to messages on a topic (called <code>/my_topic</code>). In this exercise we'll build a new subscriber node, much like we did previously, but this one will subscribe to the <code>/odom</code> topic that we've been talking about above. We'll also create a new package called <code>part2_navigation</code> for this node to live in!</p> <ol> <li> <p>First, head to the <code>src</code> folder of your ROS2 workspace in your terminal and into the <code>tuos_ros</code> Course Repo:</p> <pre><code>cd ~/ros2_ws/src/tuos_ros/\n</code></pre> </li> <li> <p>Then, use the <code>create_pkg.sh</code> helper script to create your new package:</p> <pre><code>./create_pkg.sh part2_navigation\n</code></pre> </li> <li> <p>Then navigate into the <code>scripts</code> folder of the new package using the <code>cd</code> command again:</p> <pre><code>cd ../part2_navigation/scripts/\n</code></pre> </li> <li> <p>The subscriber that we will build here will be structured in much the same way as the subscriber that we built in Part 1. </p> <p>As a starting point, copy across the <code>subscriber.py</code> file from your <code>part1_pubsub</code> package:</p> <pre><code>cp ~/ros2_ws/src/part1_pubsub/scripts/subscriber.py ./odom_subscriber.py\n</code></pre> </li> <li> <p>Next, follow the steps for converting this into an Odometry subscriber. </p> </li> <li> <p>You'll need to add a new dependency to your package's <code>package.xml</code> file now. Below the <code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;</code> line, add an execution dependency for <code>nav_msgs</code>:</p> package.xml<pre><code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;\n&lt;exec_depend&gt;nav_msgs&lt;/exec_depend&gt;\n</code></pre> </li> <li> <p>Next, declare the <code>odom_subscriber.py</code> node as an executable. Replace <code>minimal_node.py</code> with <code>odom_subscriber.py</code> in the <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/odom_subscriber.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Head back to the terminal and use Colcon to build the package again (which now contains the new <code>odom_subscriber.py</code> node):</p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-select part2_navigation --symlink-install\n</code></pre> </li> <li> <p>And then, finally, don't forget to re-source:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Now we're ready to run this! Do so using <code>ros2 run</code> and see what it does:</p> <pre><code>ros2 run part2_navigation odom_subscriber.py\n</code></pre> </li> <li> <p>Having followed all the steps, the output from your node should be similar to that shown below:</p> <p></p> <p></p> </li> <li> <p>Observe how the output (the formatted odometry data) changes whilst you move the robot around using the <code>teleop_keyboard</code> node in a new terminal instance (TERMINAL 3).</p> </li> <li>Stop your <code>odom_subscriber.py</code> node in TERMINAL 2 and the <code>teleop_keyboard</code> node in TERMINAL 3 by entering Ctrl+C in each of the terminals.</li> </ol>"},{"location":"course/assignment1/part2/#basic-navigation-open-loop-velocity-control","title":"Basic Navigation: Open-loop Velocity Control","text":""},{"location":"course/assignment1/part2/#ex3","title":"Exercise 3: Controlling Velocity with the ROS2 CLI","text":"<p>Warning</p> <p>Make sure that you've stopped the <code>teleop_keyboard</code> node before starting this exercise!</p> <p>We can use the <code>ros2 topic pub</code> command to publish data to a topic from a terminal by using the command in the following way:</p> <pre><code>ros2 topic pub {topic_name} {message_type} {message_data}\n</code></pre> <p>As we discovered earlier, the <code>/cmd_vel</code> topic is expecting messages containing linear and angular velocity data, each with an <code>x</code>, <code>y</code> and <code>z</code> component. When publishing topic messages in a terminal the commands can get quite long and complicated, but we can use autocomplete functionality to help us format the full command correctly.</p> <ol> <li> <p>In TERMINAL 3 type the following, using the Tab key where indicated to invoke autocompletion...</p> <ol> <li> <p>First, type the text as shown below and then press the Tab key where indicated to complete the topic name for you:</p> <pre><code>ros2 topic pub /cmd_[TAB]\n</code></pre> </li> <li> <p>Then, type <code>g</code> and then press Tab again to format the rest of the message type for you: </p> <pre><code>ros2 topic pub /cmd_vel g[TAB]\n</code></pre> </li> <li> <p>The message data then needs to be entered inside quotation marks, type <code>\"l</code> and then press Tab again to obtain the format of the message data:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"l[TAB]\n</code></pre> <p>The full command will then be presented:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.0\"\n</code></pre> <p>Tip</p> <p>You can use Tab to autocomplete lots of terminal commands, experiment with it - it'll save you lots of time! </p> </li> </ol> </li> <li> <p>Scroll back through the message using the Left key on your keyboard and then edit the values of the various fields, as appropriate.</p> <p>First, define some values that would make the robot rotate on the spot.  </p> </li> <li> <p>Enter Ctrl+C in TERMINAL 3 to stop the message from being published.</p> <p>What happens to the robot when you stop the <code>ros2 topic pub</code> command?</p> <p>... it keeps on moving at the requested velocity!</p> <p>In order to make the robot actually stop, we need to publish a new message containing alternative velocity commands.</p> </li> <li> <p>In TERMINAL 3 press the Up key on your keyboard to recall the previous command, but don't press Enter just yet! Now press the Left key to track back through the message and change the velocity field values in order to now make the robot stop.</p> </li> <li> <p>Once again, enter Ctrl+C in TERMINAL 3 to stop the publisher from actively publishing new messages, and then follow the same steps as above to compose another new message to now make the robot move in a circle.</p> </li> <li> <p>Enter Ctrl+C to again stop the message from being published, publish a further new message to stop the robot, and then compose (and publish) a message that would make the robot drive in a straight line.</p> </li> <li> <p>Finally, stop the robot again!</p> </li> </ol>"},{"location":"course/assignment1/part2/#ex4","title":"Exercise 4: Creating a Python Node to Make a Robot Move in a circle","text":"<p>Controlling a robot from the terminal (or by using the <code>teleop_keyboard</code> node) is all well and good, but what about if we need to implement some more advanced control or autonomy?</p> <p>We'll now learn how to control the velocity of our robot programmatically, from a Python Node. We'll start out with a simple example to achieve a simple velocity profile (a circle), but this will provide us with the basis on which we can build more complex velocity control algorithms (which we'll look at in the following exercise).</p> <p>In Part 1 we built a simple publisher node, and this one will work in much the same way, but this time however, we need to publish <code>Twist</code> type messages to the <code>/cmd_vel</code> topic instead... </p> <ol> <li> <p>In TERMINAL 2, ensure that you're located within the <code>scripts</code> folder of your <code>part2_navigation</code> package (you could use <code>pwd</code> to check your current working directory).</p> <p>If you aren't located here then navigate to this directory using <code>cd</code>.</p> </li> <li> <p>Create a new file called <code>move_circle.py</code>:</p> <p>TERMINAL 2: </p><pre><code>touch move_circle.py\n</code></pre> ... and make this file executable using the <code>chmod</code> command (as we did in Part 1).<p></p> </li> <li> <p>The task is to make the robot move in a circle with a path radius of approximately 0.5 meters.</p> <p>Follow the steps here for building this (using the Part 1 Publisher Node as a starting point). </p> </li> <li> <p>Our <code>move_circle.py</code> node has a new dependency:</p> <pre><code>from geometry_msgs.msg import Twist\n</code></pre> <p>We therefore need to add this dependency to our package's <code>package.xml</code> file.</p> <p>Earlier on we added <code>nav_msgs</code> to this. Below this, add a new <code>&lt;exec_depend&gt;</code> for <code>geometry_msgs</code>:</p> package.xml<pre><code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;\n&lt;exec_depend&gt;nav_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;geometry_msgs&lt;/exec_depend&gt;  &lt;!-- (1)! --&gt;\n</code></pre> <ol> <li>ADD THIS LINE!</li> </ol> </li> <li> <p>Next (hopefully you're getting the idea by now!), declare the <code>move_circle.py</code> node as an executable in the <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/odom_subscriber.py\n  scripts/move_circle.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Finally, head back to the terminal and use Colcon to build the new node alongside everything else in the package:</p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-select part2_navigation --symlink-install\n</code></pre> <p>... and re-source again:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Run this node now, using <code>ros2 run</code> and see what happens:</p> <pre><code>ros2 run part2_navigation move_circle.py\n</code></pre> <p>Head back to the Gazebo simulation and watch as the robot moves around in a circle of 0.5 meter radius!</p> </li> <li> <p>Once you're done, enter Ctrl+C in TERMINAL 2 to stop the <code>move_circle.py</code> node. Notice what happens to the robot when you do this...</p> <p>Question</p> <p>What does happen to the robot when you hit Ctrl+C to stop the node?</p> <p>Answer: It carries on moving !</p> </li> </ol>"},{"location":"course/assignment1/part2/#ex5","title":"Exercise 5: Implementing a Shutdown Procedure","text":"<p>Clearly, our work on the <code>move_circle.py</code> node isn't quite done. When we terminate our node we'd expect the robot to stop moving, but this (currently) isn't the case. </p> <p>You may have also noticed (with all the nodes that we have created so far) an error traceback in the terminal, every time we hit Ctrl+C. </p> <p>None of this is very good, and we'll address this now by modifying the <code>move_circle.py</code> file to incorporate a proper (and safe) shutdown procedure.</p> <ol> <li> <p>Return to the <code>move_circle.py</code> file in VS Code. </p> </li> <li> <p>First, we need to add an import to our Node:</p> <pre><code>from rclpy.signals import SignalHandlerOptions\n</code></pre> <p>You'll see what this is for shortly...</p> </li> <li> <p>Then move on to the <code>__init__()</code> method of your <code>Circle()</code> class.</p> <p>Add in a boolean flag here called <code>shutdown</code>:</p> <pre><code>self.shutdown = False\n</code></pre> <p>... to begin with, we want this to be set to <code>False</code>.</p> </li> <li> <p>Next, add a new method to your <code>Circle()</code> class, called <code>on_shutdown()</code>:</p> <pre><code>def on_shutdown(self):\n    self.get_logger().info(\n        \"Stopping the robot...\"\n    )\n    self.my_publisher.publish(Twist()) # (1)!\n    self.shutdown = True # (2)!\n</code></pre> <ol> <li>All velocities within the <code>Twist()</code> message class are set to zero by default, so we can just publish this as-is, in order to ask the robot to stop.</li> <li>Set the <code>shutdown</code> flag to true to indicate that a stop message has now been published.</li> </ol> </li> <li> <p>Finally, head to the <code>main()</code> function of the script. This is where most of the changes need to be made...</p> <pre><code>def main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    ) # (1)!\n    move_circle = Circle()\n    try:\n        rclpy.spin(move_circle) # (2)!\n    except KeyboardInterrupt: # (3)!\n        print(\n            f\"{move_circle.get_name()} received a shutdown request (Ctrl+C).\"\n        )\n    finally: \n        move_circle.on_shutdown() # (4)!\n        while not move_circle.shutdown: # (5)!\n            continue\n        move_circle.destroy_node() # (6)!\n        rclpy.shutdown()\n</code></pre> <ol> <li> <p>When initialising <code>rclpy</code>, we're requesting for our <code>move_circle.py</code> node to handle \"signals\" (i.e. events like a Ctrl+C), rather than letting <code>rclpy</code> handle these for us. Here we're using the <code>SignalHandlerOptions</code> object that we imported from <code>rclpy.signals</code> earlier.</p> </li> <li> <p>We set our node to spin inside a Try-Except block now, so that we can catch a <code>KeyboardInterrupt</code> (i.e. a Ctrl+C) and act accordingly when this happens.</p> </li> <li> <p>On detection of the <code>KeyboardInterrupt</code> we print a message to the terminal. After this, the code will move on to the <code>finally</code> block...</p> </li> <li> <p>Call the <code>on_shutdown()</code> method that we defined earlier. This will ensure that a STOP command is published to the robot (via <code>/cmd_vel</code>).</p> </li> <li> <p>This <code>while</code> loop will continue to iterate until our boolean <code>shutdown</code> flag has turned <code>True</code>, to indicate that the STOP message has been published.</p> </li> <li> <p>The rest is the same as before...</p> <p>... destroy the node and then shutdown <code>rclpy</code>.</p> </li> </ol> </li> <li> <p>With all this in place, run the node again now (<code>ros2 run ...</code>).</p> <p>Now, when you hit Ctrl+C you should find that the robot actually stops moving. Ah, much better!</p> </li> </ol>"},{"location":"course/assignment1/part2/#odometry-based-navigation","title":"Odometry-based Navigation","text":"<p>Over the course of the previous two exercises we've created a Python node to make your robot move using open-loop control. To achieve this we published velocity commands to the <code>/cmd_vel</code> topic to make the robot follow a circular motion path.</p> <p>Questions</p> <ol> <li>How do we know if our robot actually achieved the motion path that we asked for?</li> <li>In a real-world environment, what external factors might result in the robot not achieving its desired trajectory?</li> </ol> <p>Earlier on we also learnt about Robot Odometry, which is used by the robot to keep track of its position and orientation (aka Pose) in the environment.  As explained earlier, this is determined by a process called \"dead-reckoning,\" which is only really an approximation, but it's a fairly good one in any case, and we can use this as a feedback signal to understand if our robot is moving in the way that we expect it to.</p> <p>We can therefore build on the techniques that we used in the <code>move_circle.py</code> exercise, and now also build in the ability to subscribe to a topic too and obtain some real-time feedback. To do this, we'll need to subscribe to the <code>/odom</code> topic, and use this to implement some basic closed-loop control.</p>"},{"location":"course/assignment1/part2/#ex6","title":"Exercise 6: Making our Robot Follow a Square Motion Path","text":"<ol> <li> <p>Make sure your <code>move_circle.py</code> node is no longer running in TERMINAL 2, stopping it with Ctrl+C if necessary.</p> </li> <li> <p>Make sure TERMINAL 2 is still located inside your <code>part2_navigation</code> package.</p> </li> <li> <p>Navigate to the package <code>scripts</code> directory and use the Linux <code>touch</code> command to create a new file called <code>move_square.py</code>:</p> <p>TERMINAL 2: </p><pre><code>touch move_square.py\n</code></pre><p></p> </li> <li> <p>Then make this file executable using <code>chmod</code>:</p> <p>TERMINAL 2: </p><pre><code>chmod +x move_square.py\n</code></pre><p></p> </li> <li> <p>Define <code>move_square.py</code> as a package executable in your <code>CMakeLists.txt</code> file (you should know how to do this by now?!) </p> </li> <li> <p>Use the VS Code File Explorer to navigate to this <code>move_square.py</code> file and open it up, ready for editing.</p> </li> <li> <p>There's a template here to help you with this exercise. Copy and paste the template code into your new <code>move_square.py</code> file to get you started. </p> </li> <li> <p>Re-build your <code>part2_navigation</code> package, to include your new <code>move_square.py</code> node:</p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-select part2_navigation --symlink-install\n</code></pre> <p>... and don't forget to re-source again:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Run the code as it is to see what happens... </p> <p>Fill in the Blank!</p> <p>Something not quite working as expected? Did we forget something very crucial on the very first line of the code template?!</p> </li> <li> <p>Fill in the blank as required and then adapt the code to make your robot follow a square motion path of 1 x 1 meter dimensions.</p> </li> <li> <p>Perform the necessary steps to re-build your package once more (with <code>colcon</code>), as you have done each time you've created a new node in the previous exercises. </p> </li> <li> <p>Having built your package again (and re-sourced) you should now be able to run the <code>move_square.py</code> node with <code>ros2 run</code></p> <p>After following a square motion path a few times, your robot should return to the same location that it started from.</p> <p>Advanced feature</p> <p>Adapt the node to make the robot automatically stop once it has performed two complete loops.</p> </li> </ol>"},{"location":"course/assignment1/part2/#wrapping-up","title":"Wrapping Up","text":"<p>In this session we've learnt how to control the velocity and position of a robot from both the command-line (using ROS command-line tools) and from ROS Nodes by publishing correctly formatted messages to the <code>/cmd_vel</code> topic.  </p> <p>We've also learnt about Odometry, which is published by our robot to the <code>/odom</code> topic.  The odometry data tells us the current linear and angular velocities of our robot in relation to its 3 principal axes.  In addition to this though, it also tells us where in physical space our robot is located and oriented, which is determined based on dead-reckoning. </p> <p>Questions</p> <ol> <li>What information (sensor/actuator data) is used to do this?</li> <li>Do you see any potential limitations of this?</li> </ol> <p>Consider reading Chapter 11.1.3 (\"Pose of Robot\") in the ROS Robot Programming eBook that we mentioned here.</p> <p>In the final exercise we explored the development of odometry-based control to make a robot follow a square motion path. You will likely have observed some degree of error in this which could be due to the fact that Odometry data is determined by dead-reckoning and is therefore subject to drift and accumulated error. Consider how other factors may impact the accuracy of control too.</p> <p>Questions</p> <ol> <li>How might the rate at which the odometry data is sampled play a role?</li> <li>How quickly can your robot receive new velocity commands, and how quickly can it respond?</li> </ol> <p>Be aware that we did all this in simulation here too. In fact, in a real world environment, this type of navigation might be less effective, since things such as measurement noise and calibration errors can also have considerable impact. You will have the opportunity to experience this first hand in the labs.</p> <p>Ultimately then, we've seen a requirement here for additional information to provide more confidence of a robot's location in its environment, in order to enhance its ability to navigate effectively and avoid crashing into things! We'll explore this further later in this course.</p>"},{"location":"course/assignment1/part2/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, the work you have done in the WSL-ROS2 environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS2 Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University <code>U:\\</code> Drive, allowing you to restore it on another managed desktop machine the next time you fire up WSL-ROS2.  </p> <ol> <li> <p>Quaternions are explained very nicely here, if you'd like to learn more.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment1/part3/","title":"Part 3: Beyond the Basics","text":""},{"location":"course/assignment1/part3/#introduction","title":"Introduction","text":"<p> Exercises: 5 Estimated Completion Time: 2 hours Difficulty Level: Intermediate  </p>"},{"location":"course/assignment1/part3/#aims","title":"Aims","text":"<p>In this part of the course we'll look at some more advanced ROS concepts, and explore another of our robot's on-board sensors: the LiDAR sensor. From the work you did in Part 2 you may have started to appreciate the limitations associated with using odometry data alone as a feedback signal when trying to control a robot's position in its environment. The LiDAR sensor can provide further information about an environment, thus enhancing a robot's knowledge and capabilities. To begin with however, we'll look at how to launch ROS applications more efficiently by creating our own launch files and executing these with the <code>ros2 launch</code> command. </p>"},{"location":"course/assignment1/part3/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Create launch files to allow the execution of multiple ROS Nodes simultaneously with <code>ros2 launch</code>.</li> <li>Learn about the robot's LiDAR sensor and the measurements obtained from this.</li> <li>Interpret the <code>LaserScan</code> data that is published to the <code>/scan</code> topic and use existing ROS tools to visualise this.</li> <li>Perform numeric analysis on data arrays (using the <code>numpy</code> Python library) to process <code>LaserScan</code> data for use in ROS applications.</li> <li>Use existing ROS tools to implement SLAM and build a map of an environment. </li> </ol>"},{"location":"course/assignment1/part3/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Creating a Launch File</li> <li>Exercise 2: Launching Another Launch File</li> <li>Exercise 3: Using RViz to Visualise LaserScan Data</li> <li>Exercise 4: Building a LaserScan Callback Function</li> <li>Exercise 5: Building a map of an environment with SLAM</li> </ul>"},{"location":"course/assignment1/part3/#additional-resources","title":"Additional Resources","text":"<ul> <li>A Basic LaserScan Subscriber Node (for Exercise 4)</li> </ul>"},{"location":"course/assignment1/part3/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>If you haven't done so already, launch your ROS environment now:</p> <ol> <li>Using WSL-ROS2 on a university managed desktop machine: follow the instructions here to launch it.</li> <li>Running WSL-ROS2 on your own machine: launch the Windows Terminal to access a WSL-ROS2 terminal instance.</li> <li>Other Users: Follow the relevant steps to launch a terminal instance into your local ROS installation.</li> </ol> <p>You should now have access to a Linux terminal instance, and we'll refer to this terminal instance as TERMINAL 1.</p> <p>Step 2: Restore your work (WSL-ROS2 Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS2 Environment will not be preserved between sessions or across different University computers. At the end of Part 2 you should have run the <code>wsl_ros</code> tool to back up your home directory to your University <code>U:\\</code> Drive. Once WSL-ROS2 is up and running, you should be prompted to restore this:</p> <pre><code>It looks like you already have a backup from a previous session:\n  U:\\wsl-ros\\ros2-backup-XXX.tar.gz\nDo you want to restore this now? [y/n]\n</code></pre> <p>Enter Y+Enter to restore your work from last time. You can also restore your work at any time using the following command:</p> <pre><code>wsl_ros restore\n</code></pre> <p>Step 3: Launch VS Code </p> <p>It's also worth launching VS Code now, so that it's ready to go for when you need it later on. </p> WSL Users... <p>It's important to launch VS Code within your ROS environment using the \"WSL\" extension. Always remember to check for this: </p> <p></p> <p></p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>In Part 1 you should have downloaded and installed The Course Repo into your ROS environment. Hopefully you've done this by now, but if you haven't then go back and do it now (you'll need it for some exercises here). If you have already done it, then (once again) it's worth just making sure it's all up-to-date, so run the following command now to do so:</p> <p>TERMINAL 1: </p><pre><code>cd ~/ros2_ws/src/tuos_ros/ &amp;&amp; git pull\n</code></pre><p></p> <p>Then run <code>colcon build</code> </p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-up-to tuos_ros\n</code></pre> <p>And finally, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Remember</p> <p>If you have any other terminal instances open, then you'll need run <code>source ~/.bashrc</code> in these too, in order for the changes to propagate through to these as well!</p>"},{"location":"course/assignment1/part3/#launch-files","title":"Launch Files","text":"<p>So far (in Parts 1 &amp; 2) we've used the <code>ros2 run</code> command to execute a variety of ROS nodes, such as <code>teleop_keyboard</code>, as well as a number of nodes that we've created of our own. You may also have noticed that we've used a <code>ros2 launch</code> command now and again too, mainly to launch Gazebo Simulations of our robot, but why do we have these two commands, and what's the difference between them?</p> <p>Complex ROS applications typically require the execution of multiple nodes at the same time. The <code>ros2 run</code> command only allows us to execute a single node, and so this isn't that convenient for such complex applications, where we'd have to open multiple terminals, use <code>ros2 run</code> multiple times and make sure that we ran everything in the correct order without forgetting anything! <code>ros2 launch</code>, on the other hand, provides a means to launch multiple ROS nodes simultaneously by defining exactly what we want to launch within launch files. This makes the execution of complex applications more reliable, repeatable and easier for others to launch these applications correctly. </p>"},{"location":"course/assignment1/part3/#ex1","title":"Exercise 1: Creating a Launch File","text":"<p>In order to see how launch files work, let's create some of our own!</p> <p>In Part 1 we created <code>publisher.py</code> and <code>subscriber.py</code> nodes that could talk to one another via a topic called <code>/my_topic</code>. We launched these independently using the <code>ros2 run</code> command in two separate terminals. Wouldn't it be nice if we could have launched them both at the same time, from the same terminal instead?</p> <p>To start with, let's create another new package, this time called <code>part3_beyond_basics</code>. </p> <ol> <li> <p>In TERMINAL 1...</p> <ol> <li> <p>Head to the <code>src</code> folder of your ROS workspace, and into the <code>tuos_ros</code> Course Repo from there:</p> <p>TERMINAL 1: </p><pre><code>cd ~/ros2_ws/src/tuos_ros/\n</code></pre><p></p> </li> <li> <p>Use the <code>create_pkg.sh</code> helper script to create a new package once again:</p> <pre><code>./create_pkg.sh part3_beyond_basics\n</code></pre> </li> <li> <p>And navigate into the root of this new package, using <code>cd</code>:</p> <pre><code>cd ../part3_beyond_basics/\n</code></pre> </li> </ol> </li> <li> <p>Launch files should be located in a <code>launch</code> directory at the root of the package directory, so use <code>mkdir</code> to do this:</p> <p>TERMINAL 1: </p><pre><code>mkdir launch\n</code></pre><p></p> </li> <li> <p>Use the <code>cd</code> command to enter the <code>launch</code> folder that you just created, then use the <code>touch</code> command to create a new empty file called <code>pubsub.launch.py</code>.</p> <p>TERMINAL 1: </p><pre><code>cd launch &amp;&amp; touch pubsub.launch.py\n</code></pre><p></p> </li> <li> <p>Open this launch file in VS Code and enter the following:</p> pubsub.launch.py<pre><code>from launch import LaunchDescription # (1)!\nfrom launch_ros.actions import Node # (2)!\n\ndef generate_launch_description(): # (3)!\n    return LaunchDescription([ # (4)!\n        Node( # (5)!\n            package='part1_pubsub', # (6)!\n            executable='publisher.py', # (7)!\n            name='my_publisher' # (8)!\n        )\n    ])\n</code></pre> <ol> <li>Everything that we want to execute with a launch file must be encapsulated within a <code>LaunchDescription</code>, which is imported here from the <code>launch</code> module.</li> <li>In order to execute a node from a launch file we need to define it using the <code>Node</code> class from <code>launch_ros.actions</code> (not to be confused with the ROS Action communication method covered in Part 5!)</li> <li>We encapsulate a Launch Description inside a <code>generate_launch_description()</code> function.</li> <li>Here we define everything that we want this launch file to execute: in this case a Python list <code>[]</code> containing a single <code>Node()</code> item (for now).</li> <li>Here we describe the node that we want to launch.</li> <li>The name of the package that the node is part of.</li> <li>The name of the actual node that we want to launch from the above package.</li> <li> <p>A name to register this node as on the ROS network. While this is also defined in the node itself: </p> <pre><code>super().__init__(\"simple_publisher\")\n</code></pre> <p>...we can override this here with something else. </p> </li> </ol> </li> <li> <p>We need to make sure we tell <code>colcon</code> about our new <code>launch</code> directory, so that it can build the launch files within it when we run <code>colcon build</code>. To do this, we need to add a directory install instruction to our package's <code>CMakeLists.txt</code>:</p> <p>Open up the <code>CMakeLists.txt</code> file and add the following text just above the <code>ament_package()</code> line at the very bottom:</p> part3_beyond_basics/CMakeLists.txt<pre><code>install(DIRECTORY\n  launch\n  DESTINATION share/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Now, let's build the package... </p> <ol> <li> <p>Navigate back to the root of the ROS workspace:</p> <p>TERMINAL 1: </p><pre><code>cd ~/ros2_ws/\n</code></pre><p></p> </li> <li> <p>Run <code>colcon build</code> on your new package only:</p> <pre><code>colcon build --packages-select part3_beyond_basics --symlink-install\n</code></pre> </li> <li> <p>And finally, re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Use <code>ros2 launch</code> to launch this file and test it out as it is:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch part3_beyond_basics pubsub.launch.py\n</code></pre><p></p> </li> <li> <p>The code that we've given you above will launch the <code>publisher.py</code> node from the <code>part1_pubsub</code> package, but not the <code>subscriber.py</code> node.  We therefore need to add another <code>Node()</code> object to our <code>LaunchDescription</code>:</p> pubsub.launch.py<pre><code>from launch import LaunchDescription \nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='part1_pubsub',\n            executable='publisher.py',\n            name='my_publisher'\n        ),\n        Node(\n            # TODO: define the subscriber.py node...\n        )\n    ])\n</code></pre> <p>Using the same methods as above, add the necessary definitions for the <code>subscriber.py</code> node into your launch file.</p> </li> <li> <p>Once you've made these changes you'll need to run <code>colcon build</code> again.</p> <p>Warning</p> <p>You'll need to run <code>colcon build</code> every time you make changes to a launch file, even if you use the <code>--symlink-install</code> option (as this only applies to nodes in the <code>scripts</code> directory)</p> </li> <li> <p>Once you've completed this, it should be possible to launch both the publisher and subscriber nodes with <code>ros2 launch</code> and the <code>pubsub.launch.py</code> file. Verify this in TERMINAL 1 by executing the launch file. Soon after launching this, you should see the following messages to indicate that both nodes are alive:</p> <pre><code>[subscriber.py-2] [INFO] [###] [my_subscriber]: The 'my_subscriber' node is initialised.\n[publisher.py-1] [INFO] [###] [my_publisher]: The 'my_publisher' node is initialised.\n</code></pre> <p>... and following this, the outputs of both nodes should be printed to the screen continually:</p> <pre><code>[publisher.py-1] [INFO] [###] [my_publisher]: Publishing: 'The ROS time is 1737545960 (seconds).'\n[subscriber.py-2] [INFO] [###] [my_subscriber]: The 'my_subscriber' node heard:\n[subscriber.py-2] [INFO] [###] [my_subscriber]: 'The ROS time is 1737545960 (seconds).' \n</code></pre> </li> <li> <p>We can further verify this in a new terminal (TERMINAL 2), using commands that we've use in Parts 1 &amp; 2 to list all nodes and topics that are active on our ROS network:</p> <p>TERMINAL 2:</p> <p></p><pre><code>ros2 node list\n</code></pre> <pre><code>ros2 topic list\n</code></pre><p></p> <p>Do you see what you'd expect to see in the output of these two commands?</p> </li> </ol>"},{"location":"course/assignment1/part3/#ex2","title":"Exercise 2: Launching Another Launch File","text":"<p>Using the processes above, we can develop launch files to execute as many nodes as we want on a ROS network simultaneously. Another thing we can do with launch files is launch other launch files! </p> <p>To illustrate this, think back to the <code>move_circle.py</code> node that we developed in Part 2, as part of our <code>part2_navigation</code> package. In order to launch this node we must first launch a robot simulation, e.g.: </p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>In this exercise we'll look at how we can launch the above launch file and our <code>move_circle.py</code> node simultaneously from a single <code>ros2 launch</code> command...</p> <ol> <li> <p>Make sure you're in the <code>launch</code> directory of your <code>part3_beyond_basics</code> package, we can actually use a <code>colcon</code> command to get us to the root of the package directory:</p> <p>TERMINAL 1: </p><pre><code>colcon_cd part3_beyond_basics\n</code></pre><p></p> <p>... and <code>cd</code> to get us into the <code>launch</code> directory from there:</p> <pre><code>cd launch/\n</code></pre> </li> <li> <p>Make a new launch file in here, called <code>circle.launch.py</code>:</p> <p>TERMINAL 1: </p><pre><code>touch circle.launch.py\n</code></pre><p></p> </li> <li> <p>Open this up in VS Code and enter the following:</p> circle.launch.py<pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\nimport os\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    return LaunchDescription([\n        IncludeLaunchDescription( # (1)!\n            PythonLaunchDescriptionSource( # (2)!\n                os.path.join( # (3)!\n                    get_package_share_directory(\"turtlebot3_gazebo\"), \n                    \"launch\", \"empty_world.launch.py\" # (4)!\n                )\n            )\n        )\n    ])\n</code></pre> <ol> <li>To include another launch file in a launch description, we use a <code>IncludeLaunchDescription()</code> class instance (imported from a module called <code>launch.actions</code>).</li> <li> <p>We want to launch the \"Empty World\" simulation from the <code>turtlebot3_gazebo</code> package, which (as we know) can be done from a terminal with the following command:</p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>Based on the above, we know that the launch file itself is a Python launch file, due to the <code>.py</code> file extension at the end.</p> <p>As such, the launch description that we want to include is a Python launch description, which must therefore be defined using a <code>PythonLaunchDescriptionSource()</code> instance (imported from a module called <code>launch.launch_description_sources</code>)</p> </li> <li> <p>The <code>os.path.join()</code> method (from the standard Python <code>os</code> library) can be used to build file paths. </p> </li> <li> <p>The Python Launch Description Source is defined by providing the full path to the launch file that we want to include. We don't necessarily know where this file is on our filesystem, but ROS does!</p> <p>We can therefore use a function called <code>get_package_share_directory()</code> (from a module called <code>ament_index_python.packages</code>) to provide us with the path to the root of this package directory.</p> <p>From there, we know that the launch file itself must exist in a <code>launch</code> directory, so we use the <code>os.path.join()</code> method to construct this full file path for us.</p> </li> </ol> <p>... Currently, the launch file above contains only the code necessary to include the <code>empty_world.launch.py</code> launch file into our <code>circle.launch.py</code> launch description. There's a few new things that have been introduced here to achieve this, so click on the  icons in the code above to find out what all these things are doing.</p> </li> <li> <p>Now, add a <code>Node()</code> item to the launch description so that the <code>move_circle.py</code> node (from your <code>part2_navigation</code> package) is launched after the \"Empty World\" simulation has been launched.</p> <p>Refer back to Exercise 1 for a reminder on how to do this.</p> </li> <li> <p>When you're ready, remember to run <code>colcon build</code> again before attempting to execute your new <code>circle.launch.py</code> launch file:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch part3_beyond_basics circle.launch.py\n</code></pre><p></p> </li> </ol> <p>If you've done this successfully, on launching the above command the Gazebo Empty World simulation should launch and, once it's loaded up, the robot should instantly start moving around in a circle (while printing information to TERMINAL 1 at the same time).</p> <p>We've learnt some key launch file techniques now, so let's move on to another more advanced and very important topic...</p>"},{"location":"course/assignment1/part3/#lidar","title":"Laser Displacement Data and The LiDAR Sensor","text":"<p>As you'll know from Part 2, odometry is really important for robot navigation, but it can be subject to drift and accumulated error over time. You may have observed this in simulation during Part 2 Exercise 5, and you would most certainly notice it if you were to do the same on a real robot. Fortunately, The Waffles have another sensor on-board which provides even richer information about the environment, and we can use this to supplement the odometry information and enhance the robot's navigation capabilities.</p>"},{"location":"course/assignment1/part3/#introducing-the-laserscan-interface","title":"Introducing the LaserScan Interface","text":""},{"location":"course/assignment1/part3/#ex3","title":"Exercise 3: Using RViz to Visualise LaserScan Data","text":"<p>We're now going to place the robot in a more interesting environment than the \"empty world\" that we've been working with so far...</p> <ol> <li> <p>In TERMINAL 1 enter the following command to launch this:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch turtlebot3_gazebo turtlebot3_world.launch.py\n</code></pre><p></p> <p>A Gazebo simulation should now be launched with a TurtleBot3 Waffle in a new environment:</p> <p></p> <p></p> </li> <li> <p>Open a new terminal instance (TERMINAL 2) and enter the following:</p> <p>TERMINAL 2: </p><pre><code>ros2 launch tuos_simulations rviz.launch.py\n</code></pre><p></p> <p>On running the command a new window should open:</p> <p></p> <p></p> <p>This is RViz, which is a ROS tool that allows us to visualise the data being measured by a robot in real-time. </p> <p>The green dots scattered around the robot represent laser displacement data which is measured by the LiDAR sensor located on the top of the robot, allowing it to measure the distance to any obstacles in its surroundings. </p> <p>The LiDAR sensor spins continuously, sending out laser pulses as it does so. These laser pulses are reflected from any objects and sent back to the sensor. Distance can then be determined based on the time it takes for the pulses to complete the full journey (from the sensor, to the object, and back again), by a process called \"time of flight\". Because the LiDAR sensor spins and performs this process continuously, a full 360\u00b0 scan of the environment can be generated.</p> <p>In this case (because we are working in simulation here) the data represents the objects surrounding the robot in its simulated environment, so you should notice that the green dots produce an outline that resembles the objects in the world that is being simulated in Gazebo (or partially at least).</p> </li> <li> <p>Next, open up a new terminal instance (TERMINAL 3). Laser displacement data from the LiDAR sensor is published by the robot to the <code>/scan</code> topic. We can use the <code>ros2 topic info</code> command to find out more about the nodes that are publishing and subscribing to this topic, as well as the type of interface used to transmit this topic data:</p> <p>TERMINAL 3: </p><pre><code>ros2 topic info /scan\n</code></pre> <pre><code>Type: sensor_msgs/msg/LaserScan\nPublisher count: 1\nSubscription count: 0\n</code></pre><p></p> </li> <li> <p>As we can see from above, <code>/scan</code> data is of the <code>sensor_msgs/msg/LaserScan</code> type, and we can find out more about this interface using the <code>ros2 interface show</code> command:</p> <p>TERMINAL 3: </p><pre><code>ros2 interface show sensor_msgs/msg/LaserScan\n</code></pre> <pre><code># Single scan from a planar laser range-finder\n\nstd_msgs/Header header # timestamp in the header is the acquisition time of\n        builtin_interfaces/Time stamp\n                int32 sec\n                uint32 nanosec\n        string frame_id\n                             # the first ray in the scan.\n                             #\n                             # in frame frame_id, angles are measured around\n                             # the positive Z axis (counterclockwise, if Z is up)\n                             # with zero angle being forward along the x axis\n\nfloat32 angle_min            # start angle of the scan [rad]\nfloat32 angle_max            # end angle of the scan [rad]\nfloat32 angle_increment      # angular distance between measurements [rad]\n\nfloat32 time_increment       # time between measurements [seconds] - if your scanner\n                             # is moving, this will be used in interpolating position\n                             # of 3d points\nfloat32 scan_time            # time between scans [seconds]\n\nfloat32 range_min            # minimum range value [m]\nfloat32 range_max            # maximum range value [m]\n\nfloat32[] ranges             # range data [m]\n                             # (Note: values &lt; range_min or &gt; range_max should be discarded)\nfloat32[] intensities        # intensity data [device-specific units].  If your\n                             # device does not provide intensities, please leave\n                             # the array empty.\n</code></pre><p></p> </li> </ol>"},{"location":"course/assignment1/part3/#interpreting-laserscan-data","title":"Interpreting LaserScan Data","text":"<p>The <code>LaserScan</code> interface is a standardised ROS message interface (from the <code>sensor_msgs</code> package) that any ROS Robot can use to publish data that it obtains from a Laser Displacement Sensor such as the LiDAR on the TurtleBot3.  </p> <p><code>ranges</code> is an array of <code>float32</code> values (array data-types are suffixed with <code>[]</code>). This is the part of the message containing all the actual distance measurements that are being obtained by the LiDAR sensor (in meters).</p> <p>Consider a simplified example here, taken from a TurtleBot3 robot in a different environment:</p> <p></p> <p>As illustrated in the figure, we can associate each data-point of the <code>ranges</code> array to an angular position by using the <code>angle_min</code>, <code>angle_max</code> and <code>angle_increment</code> values that are also provided within the <code>LaserScan</code> message.  We can use the <code>ros2 topic echo</code> command to find out what their values are:</p> <p></p><pre><code>$ ros2 topic echo /scan --field angle_min --once\n0.0\n---\n</code></pre> <pre><code>$ ros2 topic echo /scan --field angle_max --once\n6.28000020980835\n---\n</code></pre> <pre><code>$ ros2 topic echo /scan --field angle_increment --once\n0.01749303564429283\n---\n</code></pre><p></p> <p>Question</p> <p>What do these values represent? (Compare them with the figure above)</p> <p>Tip</p> <p>Notice how we were able to access specific variables within the <code>/scan</code> data using the <code>--field</code> flag, and ask the command to only provide us with a single message by using <code>--once</code>?</p> <p>The <code>ranges</code> array contains 360 values in total, i.e. a distance measurement at every 1\u00b0 (an <code>angle_increment</code> of 0.0175 radians) around the robot. The first value in the <code>ranges</code> array (<code>ranges[0]</code>) is the distance to the nearest object directly in front of the robot (i.e. at \u03b8 = 0 radians, or <code>angle_min</code>). The last value in the <code>ranges</code> array (<code>ranges[359]</code>) is the distance to the nearest object at 359\u00b0 (i.e. \u03b8 = 6.283 radians, or <code>angle_max</code>) from the front of the robot, i.e.: 1 degree to the right of the X-axis. <code>ranges[65]</code>, for example, would represent the distance to the closest object at an angle of 65\u00b0 (1.138 radians) from the front of the robot (anti-clockwise), as shown in the figure.</p> <p>The <code>LaserScan</code> message also contains the parameters <code>range_min</code> and <code>range_max</code>, which represent the minimum and maximum distances (again, in meters) that the LiDAR sensor can detect, respectively. Use the <code>ros2 topic echo</code> command to report these directly too.  </p> <p>Questions</p> <ol> <li>What is the maximum and minimum range of the LiDAR sensor? Use the same technique as we used above to find out.</li> <li> <p>Consider the note against <code>ranges</code> in the <code>ros2 interface show</code> output earlier:</p> <pre><code>float32[] ranges    # range data [m]\n                    # (Note: values &lt; range_min or &gt; range_max should be discarded)\n</code></pre> <p>(this might be worth thinking about).</p> </li> </ol> <p>Finally, use the <code>ros2 topic echo</code> command again to display the <code>ranges</code> portion of the <code>LaserScan</code> data. There's a lot of data here (360 data points per message in fact, as you know from above!):</p> <pre><code>ros2 topic echo /scan --field ranges\n</code></pre> <p>We're dropping the <code>--once</code> option now, so that we can see the data as it comes in, in real-time.  You might need to expand the terminal window so that you can see all the data points; data will be bound by square brackets <code>[]</code>, and there should be a <code>---</code> at the end of each message too, to help you confirm that you are viewing the whole thing.</p> <p>The main thing you'll notice here is that there's lots of information, and it changes rapidly! As you have already seen though, it is the numbers that are flying by here that are represented by green dots in RViz.  Head back to the RViz screen to have another look at this now. As you'll no doubt agree, this is a much more useful way to visualise the <code>ranges</code> data, and illustrates how useful RViz can be for interpreting what your robot can see in real-time.</p> <p>What you may also notice is several <code>inf</code> values scattered around the array.  These represent sensor readings that are outside the sensor's measurement range (i.e. greater than <code>range_max</code> or less than <code>range_min</code>), so the sensor can't report a distance measurement in such cases. Remember from above: </p> <pre><code>(Note: values &lt; range_min or &gt; range_max should be discarded)\n</code></pre> <p>Note</p> <p>This behaviour is different on the real robots! Be aware of this when developing code for real robots!!</p> <p>Stop the <code>ros2 topic echo</code> command from running in the terminal window by entering Ctrl+C in TERMINAL 3. Also close down the RViz process running in TERMINAL 2 now as well, but leave the simulation (in TERMINAL 1 running). </p>"},{"location":"course/assignment1/part3/#ex4","title":"Exercise 4: Building a LaserScan Callback Function","text":"<p>LaserScan data presents us with a new challenge: processing large datasets. In this exercise we'll look at some basic approaches that can be taken to deal with this data, and get something meaningful out of it that can be used in your robot applications.</p> <ol> <li> <p>Navigate into the <code>scripts</code> folder of your <code>part3_beyond_basics</code> package:</p> <p>TERMINAL 2: </p><pre><code>colcon_cd part3_beyond_basics &amp;&amp; cd scripts/\n</code></pre><p></p> </li> <li> <p>Create a new node called <code>lidar_subscriber.py</code>:</p> <p>TERMINAL 2: </p><pre><code>touch lidar_subscriber.py\n</code></pre><p></p> <p>and give this execute permissions:</p> <pre><code>chmod +x lidar_subscriber.py\n</code></pre> </li> <li> <p>Declare this as a package executable by opening up your package's <code>CMakeLists.txt</code> in VS Code, and replacing <code>minimal_node.py</code> with <code>lidar_subscriber.py</code> as shown below:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/lidar_subscriber.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Then, add some new dependencies to your package's <code>package.xml</code> file, so open this up in VS Code too. Below the <code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;</code> line, add the following:</p> package.xml<pre><code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;\n&lt;exec_depend&gt;sensor_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;python3-numpy&lt;/exec_depend&gt;\n</code></pre> </li> <li> <p>Head back to the terminal and use Colcon to build this (even though <code>lidar_subscriber.py</code> is still just an empty file at this stage):</p> <p>TERMINAL 2: </p><pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-select part3_beyond_basics --symlink-install\n</code></pre><p></p> <p>And after that, re-source your <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>With all of that out of the way, it's time to start building the <code>lidar_subscriber.py</code> Python Node! Open up the file in VS Code, then follow the steps here to construct it. </p> </li> <li> <p>Once you're happy with what's going on with this, run the node using <code>ros2 run</code>:</p> <p>TERMINAL 2: </p><pre><code>ros2 run part3_beyond_basics lidar_subscriber.py\n</code></pre><p></p> </li> <li> <p>Open another terminal (so you can still see the outputs from your <code>lidar_subscriber.py</code> node). Launch the <code>teleop_keyboard</code> node, and drive the robot around, noting how the outputs from your <code>lidar_subscriber.py</code> node change as you do so.</p> </li> <li> <p>Close everything down now (including the simulation running in TERMINAL 1). Then launch the \"empty world\" simulation again:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre><p></p> </li> <li> <p>Go back to TERMINAL 2 and launch your <code>lidar_subscriber.py</code> node again:</p> <p>TERMINAL 2: </p><pre><code>ros2 run part3_beyond_basics lidar_subscriber.py\n</code></pre><p></p> <p>What output do you see from this now?</p> <p>You should notice that your <code>lidar_subscriber.py</code> node reports <code>nan meters</code> now. That's because there's nothing in the environment for the LiDAR sensor to detect, so all readings are out of range and hence our analysis of the 40\u00b0 arc of LiDAR readings at the front of the robot has filtered out everything and therefore returned <code>nan</code> (not a number).</p> </li> <li> <p>Use the Box tool in Gazebo to place a box in the environment. </p> <p></p> <p></p> </li> <li> <p>Click the Translate tool to move the box around until the <code>lidar_subscriber.py</code> node returns some reading that aren't <code>nan</code> again.</p> </li> <li> <p>Move the box around some more to observe what our analysis of the <code>LaserScan</code> data can detect, and where the box falls out of the detectable range.</p> </li> <li> <p>Think about how you could adapt the callback function of the <code>lidar_subscriber.py</code> node so that it picks up on more than one <code>LaserScan</code> subset, so that it could detect situations such as this (for example):</p> <p></p> <p></p> </li> </ol>"},{"location":"course/assignment1/part3/#slam","title":"Simultaneous Localisation and Mapping (SLAM)","text":"<p>In combination, the data from the LiDAR sensor and the robot's odometry (the robot pose specifically) are really powerful, and allow some very useful conclusions to be made about the environment a robot is operating within.  One of the key applications of this data is \"Simultaneous Localisation and Mapping\", or SLAM.  This is a tool that's built into ROS, and allows a robot to build up a map of its environment and locate itself within that map at the same time!  We'll now look at how easy it is to leverage this in ROS.</p>"},{"location":"course/assignment1/part3/#ex5","title":"Exercise 5: Building a map of an environment with SLAM","text":"<ol> <li> <p>Close down all ROS processes that are running now by entering Ctrl+C in each terminal. </p> </li> <li> <p>We're going to launch our robot into another new simulated environment now, which we'll be creating a map of using SLAM! To launch the simulation enter the following command in TERMINAL 1:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch tuos_simulations nav_world.launch.py\n</code></pre><p></p> <p>The environment that launches should look like this:</p> <p></p> <p></p> </li> <li> <p>Now launch SLAM to start building a map of this environment. In TERMINAL 2, launch SLAM as follows:</p> <p>TERMINAL 2: </p><pre><code>ros2 launch tuos_simulations cartographer.launch.py\n</code></pre><p></p> <p>This will launch RViz again, where you should see a top-down view of an environment with a model of the robot, surrounded by some red/green dots representing the real-time LiDAR data. </p> <p></p> <p></p> <p>SLAM has already started building a map of the boundaries that are currently visible to the robot, based on its starting position in the environment. </p> <p>If you leave this for a minute the walls of the arena will start to become visible in RViz, and the floor will start to turn a lighter grey. As time passes the SLAM algorithms are becoming more certain of what is being observed in the environment, and are starting to determine what is free space, and where the boundaries are.</p> <p></p> <p></p> </li> <li> <p>In TERMINAL 3 launch the <code>teleop_keyboard</code> node (you should know how to do this by now). Re-arrange and re-size your windows so that you can see Gazebo, RViz and the <code>teleop_keybaord</code> terminal instances all at the same time:</p> <p></p> <p></p> </li> <li> <p>Drive the robot around the arena slowly, using the <code>teleop_keyboard</code> node, and observe the map being updated in the RViz window as you do so. </p> </li> <li> <p>As you're doing this open up another terminal instance (TERMINAL 4) and run the <code>odom_subscriber.py</code> node that you created back in Part 2:</p> <p>TERMINAL 4: </p><pre><code>ros2 run part2_navigation odom_subscriber.py\n</code></pre><p></p> <p>This will provide you with the robot's <code>X</code> and <code>Y</code> coordinates (in meters) within the environment, as you are driving it around, and you can use this to determine the centre coordinates of the four circles (A, B, C &amp; D) that are printed on the arena floor. </p> <p>Drive your robot into each of these circular zones and stop the robot inside them.    </p> <p>Record the zone marker coordinates in a table such as the one below.</p> <p></p><p></p> Zone X Position (m) Y Position (m) START 0.5 -0.04 A B C D <p></p><p></p> </li> <li> <p>Drive the robot around until a full map of the environment has been generated.</p> <p></p> <p></p> </li> <li> <p>Once you're happy that your robot has built a complete map of the environment (and you've got the coordinates of all the circles), you can then save your map for later use. We do this using a ROS <code>map_server</code> package.  First, stop the robot by pressing S in TERMINAL 3 and then enter Ctrl+C to shut down the <code>teleop_keyboard</code> node.</p> </li> <li> <p>Then, remaining in TERMINAL 3, navigate to the root of your <code>part3_beyond_basics</code> package directory and create a new folder in it called <code>maps</code>:</p> <p>TERMINAL 3: </p><pre><code>cd ~/ros2_ws/src/part3_beyond_basics/\n</code></pre> <pre><code>mkdir maps\n</code></pre><p></p> </li> <li> <p>Navigate into this new directory:</p> <p>TERMINAL 3: </p><pre><code>cd maps/\n</code></pre><p></p> </li> <li> <p>Then, run the <code>map_saver_cli</code> node from the <code>map_server</code> package to save a copy of your map: </p> <p>TERMINAL 3: </p><pre><code>ros2 run nav2_map_server map_saver_cli -f MAP_NAME\n</code></pre> Replacing <code>MAP_NAME</code> with a name of your choosing. <p></p> <p>This will create two files: a <code>MAP_NAME.pgm</code> and a <code>MAP_NAME.yaml</code> file, both of which contain data related to the map that you have just created.  The <code>.pgm</code> file contains an Occupancy Grid Map (OGM), which is used for autonomous navigation in ROS.  Have a look at the map by launching it in an Image Viewer Application called <code>eog</code>:</p> <p>TERMINAL 3: </p><pre><code>eog MAP_NAME.pgm\n</code></pre><p></p> <p>A new window should launch containing the map you have just created with SLAM and the <code>map_saver_cli</code> node: </p> <p></p> <p></p> <p>White regions represent the area that your robot has determined is open space and that it can freely move within.  Black regions, on the other hand, represent boundaries or objects that have been detected.  Any grey area on the map represents regions that remain unexplored, or that were inaccessible to the robot.</p> </li> <li> <p>Compare the map generated by SLAM to the real simulated environment. In a simulated environment this process should be pretty accurate, and the map should represent the simulated environment very well (unless you didn't allow your robot to travel around and see the whole thing!)  In a real environment this is often not the case.  </p> <p>Questions</p> <ul> <li>How accurately did your robot map the environment?</li> <li>What might impact this when working in a real-world environment?</li> </ul> </li> <li> <p>Close the image using the  button on the right-hand-side of the eog window.</p> </li> </ol>"},{"location":"course/assignment1/part3/#summary-of-slam","title":"Summary of SLAM","text":"<p>See how easy it was to map an environment in the previous exercise? This works just as well on a real robot in a real environment too (as you will observe in one of the Real Waffle \"Getting Started Exercises\" for Assignment #2). </p> <p>This illustrates the power of ROS: having access to tools such as SLAM, which are built into the ROS framework, makes it really quick and easy for a robotics engineer to start developing robotic applications on top of this. Our job was made even easier here since we used some packages that had been pre-made by the manufacturers of our TurtleBot3 Robots to help us launch SLAM with the right configurations for our exact robot.  If you were developing a robot yourself, or working with a different type of robot, then you might need to do a bit more work in setting up and tuning the SLAM tools to make them work for your own application.</p>"},{"location":"course/assignment1/part3/#wrapping-up","title":"Wrapping Up","text":"<p>As we learnt in Part 2, a robot's odometry is determined by dead-reckoning and control algorithms based on this alone (like the <code>move_square.py</code> node) may be subject to drift and accumulated error. </p> <p>Ultimately then, a robot needs additional information to pinpoint its precise location within an environment, and thus enhance its ability to navigate effectively and avoid crashing into things!</p> <p>This additional information can come from a LiDAR sensor, which you learnt about in this session. We explored where this data is published to, how we access it, and what it tells us about a robot's immediate environment.  We then looked at some ways odometry and laser displacement data can be combined to perform advanced robotic functions such as the mapping of an environment. This is all complicated stuff but, using ROS, we can leverage these tools with relative ease, which illustrates just how powerful ROS can be for developing robotic applications quickly and effectively without having to re-invent the wheel!</p>"},{"location":"course/assignment1/part3/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, the work you have done in the WSL-ROS2 environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS2 Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University <code>U:\\</code> Drive, allowing you to restore it on another managed desktop machine the next time you fire up WSL-ROS2.  </p>"},{"location":"course/assignment1/part4/","title":"Part 4: Services","text":""},{"location":"course/assignment1/part4/#introduction","title":"Introduction","text":"<p> Exercises: 6 Estimated Completion Time: 2 hours Difficulty Level: Intermediate  </p>"},{"location":"course/assignment1/part4/#aims","title":"Aims","text":"<p>In this part you'll learn about Services: an alternative communication method that can be used to transmit data/information or invoke actions on a ROS Network. You'll learn how this works, and why it might be useful. You'll also look at some practical applications of this.</p>"},{"location":"course/assignment1/part4/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Recognise how ROS Services differ from the standard topic-based publisher-subscriber approach, and identify appropriate use-cases for this type of messaging system.</li> <li>Identify the services that are available on a ROS network, and use ROS command-line tools to interrogate and call them.</li> <li>Develop Python Service Client Nodes.</li> <li>Invoke different services using various service-type interfaces.</li> </ol>"},{"location":"course/assignment1/part4/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Using Command-line Tools to Interrogate a Service and its Interface</li> <li>Exercise 2: Playing the Number Game (from the Command-line)</li> <li>Exercise 3: Creating a Service Interface</li> <li>Exercise 4: Adapting the Number Game Server</li> <li>Exercise 5: Creating a Python Service Client</li> <li>Exercise 6: Developing A Map Saver Service Client</li> </ul>"},{"location":"course/assignment1/part4/#additional-resources","title":"Additional Resources","text":"<ul> <li>The <code>number_game_client.py</code> Node (for Exercise 5)</li> </ul>"},{"location":"course/assignment1/part4/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS2 Environment</p> <p>If you haven't done so already, launch your ROS environment now. Having done this, you should now have access to a Linux terminal instance (aka TERMINAL 1).</p> <p>Step 2: Restore your work (WSL-ROS2 Managed Desktop Users ONLY)</p> <p>Remember that any work you do within the WSL-ROS2 Environment will not be preserved between sessions or across different University computers, so you should be backing up your work to your <code>U:\\</code> drive regularly. When prompted (on first launch of WSL-ROS2 in TERMINAL 1) enter Y+Enter to restore your data<sup>1</sup>.</p> <pre><code>It looks like you already have a backup from a previous session:\n  U:\\wsl-ros\\ros2-backup-XXX.tar.gz\nDo you want to restore this now? [y/n]\n</code></pre> <p>Step 3: Launch VS Code </p> <p>It's also worth launching VS Code now. WSL users remember to check for this:</p> <p></p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Once again, it's worth quickly checking that the Course Repo is up-to-date before you start on the Part 4 exercises. Go back to Part 1 if you haven't installed it yet (really?!) or - alternatively - see here for how to update.</p>"},{"location":"course/assignment1/part4/#an-introduction-to-services","title":"An Introduction to Services","text":"<p>So far, we've learnt about ROS topics and the message-type interfaces that we use to transmit data on them. We've also learnt how individual nodes can access data on a robot by simply subscribing to topics that another node on the ROS network is publishing messages to. In addition to this, we know that any node can publish messages to any topic, which broadcasts data across the ROS network, making it available to any other node on the network that may wish to access it.</p> <p>Another way to pass data between ROS Nodes is by using Services. These are based on a call and response type of communication:</p> <ul> <li>A Service Client sends a Request to a Service Server.</li> <li>The Service Server processes that request and sends back a Response.</li> </ul> <p></p> <p>This is a bit like a transaction: one node requests something, and another node fulfils that request and responds. This is good for quick, short duration tasks, e.g.:</p> <ol> <li>Turning a device on or off.</li> <li>Grabbing some data and saving it to a file (a map for example).</li> <li>Performing a calculation and returning a result.</li> <li>Making a sound<sup>2</sup>.</li> </ol> <p>A single service can have many clients, but you can only have a single Server providing that particular service at any one time.</p> <p> </p> Multiple Clients to a single Service Server <p>Let's see how this all works in practice now, by playing a number game! We don't need a simulation up and running for this one, so in TERMINAL 1 use the following command to launch the Guess the Number Service: </p> <p>TERMINAL 1: </p><pre><code>ros2 run tuos_examples number_game.py\n</code></pre><p></p> <p>Having launched the service successfully, you should be presented with the following:</p> <pre><code>[INFO] [#####] [number_game_service]: The '/guess_the_number' service is active.\n[INFO] [#####] [number_game_service]: A secret number has been set... Game on!\n</code></pre> <p>We need to interrogate this now, in order to work out how to play the game...</p>"},{"location":"course/assignment1/part4/#interrogating-a-service","title":"Interrogating a Service","text":""},{"location":"course/assignment1/part4/#ex1","title":"Exercise 1: Using Command-line Tools to Interrogate a Service and its Interface","text":"<ol> <li> <p>Open up a new terminal instance (TERMINAL 2) and use the <code>ros2 service</code> command to list all active ROS services:</p> <p>TERMINAL 2: </p><pre><code>ros2 service list\n</code></pre><p></p> <p>There'll be a few items in this list, most of them with the prefix: <code>/number_game_service</code>. This is the name of the node that is providing the service (i.e. the Server) and these items are all automatically generated by ROS. What we're really interested in is the service itself, which should be listed as <code>/guess_the_number</code>. </p> </li> <li> <p>Next, we need to find the interface type used by this service, which we can do a couple of ways:</p> <p>TERMINAL 2:</p> <ol> <li> <p>Use the <code>type</code> sub-command:</p> <pre><code>ros2 service type /guess_the_number\n</code></pre> </li> <li> <p>Use the <code>list</code> sub-command again, but with the <code>-t</code> flag:</p> <pre><code>ros2 service list -t\n</code></pre> <p>The latter will provide the same list of services as before, but each one will now have its interface type provided alongside it.</p> </li> </ol> </li> <li> <p>Regardless of the method that you used above, you should have discovered that the interface type used by the <code>/guess_the_number</code> service is:</p> <pre><code>tuos_interfaces/srv/NumberGame\n</code></pre> <p>Notice how (much like with message interfaces used by topics), there are three fields to this type definition:</p> <ol> <li><code>tuos_interfaces</code>: the name of the ROS package that this interface belongs to.</li> <li><code>srv</code>: that this is a service interface, the second interface type we've covered now (we'll learn about the third and final one in Part 5).</li> <li><code>NumberGame</code>: the data structure.</li> </ol> <p>We need to know the data structure in order to make a call to the service, so let's identify this next.</p> </li> <li> <p>We can use the <code>ros2 interface list</code> command to list all interface types available to us on our ROS system, but this will provide us with a long list!</p> <p>TERMINAL 2:</p> <ol> <li> <p>We can use the <code>-m</code> flag to filter for message interfaces, or the <code>-s</code> flag to filter for service interfaces. Try the latter:</p> <pre><code>ros2 interface list -s\n</code></pre> <p></p> </li> <li> <p>Still quite a lot there, right!? Let's filter this further with Grep to identify only interfaces that belong to the <code>tuos_interfaces</code> package:</p> <pre><code>ros2 interface list -s | grep tuos_interfaces\n</code></pre> <p>Hopefully, the <code>srv/NumberGame</code> interface is now listed.</p> </li> <li> <p>Use the <code>show</code> sub-command to show the message structure:</p> <pre><code>ros2 interface show tuos_interfaces/srv/NumberGame\n</code></pre> </li> </ol> <p>The interface structure should be shown as follows:</p> <pre><code>int32 guess\n---\nint32 guesses\nstring hint\nbool success\n</code></pre> </li> </ol>"},{"location":"course/assignment1/part4/#the-format-of-service-interfaces","title":"The Format of Service Interfaces","text":"<p>Service interfaces have two parts to them, separated by three hyphens (<code>---</code>). Above the separator is the Service Request, and below it is the Service Response:</p> <pre><code>int32 guess      &lt;-- Request\n---\nint32 guesses    &lt;-- Response (1 of 3)\nstring hint      &lt;-- Response (2 of 3)\nbool success     &lt;-- Response (3 of 3)\n</code></pre> <p>In order to Call a service, we need to provide data to it in the format specified in the Request section of the interface. A service Server will then send data back to the caller in the format specified in the Response section of the interface.</p> <p>The <code>tuos_interfaces/srv/NumberGame</code> service interface has only one request parameter:</p> <ol> <li><code>guess</code>: a <code>int32</code> (32-bit integer)     ...which is the only thing we need to send to the <code>/number_game_service</code> Service Server in order to call it.</li> </ol> <p>There are then three response parameters:</p> <ol> <li>A 32-bit integer called <code>guesses</code></li> <li>A text string called <code>hint</code> </li> <li> <p>A boolean flag called <code>success</code></p> <p>...all of which will be returned by the server, once it has processed our request.</p> </li> </ol>"},{"location":"course/assignment1/part4/#ex2","title":"Exercise 2: Playing the Number Game (from the Command-line)","text":"<p>We're now ready to make a call to the service, and we can do this using the <code>ros2 service</code> command again (from TERMINAL 2):</p> <ol> <li> <p>To start, let's send an initial guess of <code>0</code> and see what happens:</p> <pre><code>ros2 service call /guess_the_number tuos_interfaces/srv/NumberGame \"{guess: 0}\"\n</code></pre> <p>The request will be echoed back to us, followed by a response, which will likely look something like this, and which shows us the value of the three response parameters that we identified above:</p> <pre><code>response:\ntuos_interfaces.srv.NumberGame_Response(guesses=1, hint='Higher', success=False)\n</code></pre> <ol> <li><code>guesses</code>: tells us how many times we've tried to guess the number in total (just once so far)</li> <li><code>hint</code>: tells us if we should go \"higher\" or \"lower\" on our next guess in order to get closer to the secret number</li> <li><code>success</code>: tells us if we guessed the right number or not (unlikely on the first attempt!)</li> </ol> </li> <li> <p>Make another service call, this time changing the value of your <code>guess</code>, e.g.:</p> <pre><code>ros2 service call /guess_the_number tuos_interfaces/srv/NumberGame \"{guess: 10}\"\n</code></pre> </li> <li> <p>Try making a guess of 500 next.</p> <p>The service should respond with the hint <code>'Error'</code> now. Have a look back in TERMINAL 1 (where the Server is running) to get more information on this.</p> </li> <li> <p>Keep going until you guess the magic number, how many guesses does it take you?! </p> </li> <li> <p>Stop the server, by entering Ctrl+C in TERMINAL 1.</p> </li> </ol>"},{"location":"course/assignment1/part4/#creating-our-own-services","title":"Creating Our Own Services","text":"<p>Over the next three exercises, we'll learn how to create a service interface of our own, and build a Server and Client (in Python) that use this.</p> <p>First though, we need to create a new package, so follow the same procedure as you have in the previous parts of this course to create one called <code>part4_services</code>.</p> <p>TERMINAL 1: </p><pre><code>cd ~/ros2_ws/src/tuos_ros/\n</code></pre><p></p> <pre><code>./create_pkg.sh part4_services\n</code></pre>"},{"location":"course/assignment1/part4/#ex3","title":"Exercise 3: Creating a Service Interface","text":"<p>Let's create a service interface now which has a similar structure to the one used by the <code>/guess_the_number</code> service, but this time with two request parameters, rather than just one...</p> <ol> <li> <p>In TERMINAL 1, navigate into the root of your <code>part4_services</code> package directory:</p> <pre><code>cd ~/ros2_ws/src/part4_services/\n</code></pre> </li> <li> <p>Create a new directory there called <code>srv</code>:</p> <pre><code>mkdir srv\n</code></pre> </li> <li> <p>Create a new file in this directory called <code>MyNumberGame.srv</code>:</p> <pre><code>touch srv/MyNumberGame.srv\n</code></pre> <p>In here is where we will define the structure of our own <code>MyNumberGame</code> service interface.</p> </li> <li> <p>Open up this file in VS Code, enter the following content and save the file:</p> MyNumberGame.srv<pre><code>int32 guess\nbool cheat\n---\nint32 num_guesses\nstring hint\nbool correct\n</code></pre> <p>The Request will therefore have two fields now:</p> <p></p><p></p> # Field Name Data Type 1 <code>guess</code> <code>int32</code> 2 <code>cheat</code> <code>bool</code> <p></p><p></p> </li> <li> <p>The rest of the process now is very similar to creating a message interface, like we did in Part 1. </p> <p>First, we need to declare the interface in our <code>CMakeLists.txt</code> file, by adding the following above the <code>ament_package()</code> line:</p> part4_services/CMakeLists.txt<pre><code>find_package(rosidl_default_generators REQUIRED)\nrosidl_generate_interfaces(${PROJECT_NAME}\n  \"srv/MyNumberGame.srv\" \n)\n</code></pre> </li> <li> <p>Next, we need to modify the <code>package.xml</code> file. Add the following lines to this one, just above the <code>&lt;export&gt;</code> line:</p> package.xml<pre><code>&lt;buildtool_depend&gt;rosidl_default_generators&lt;/buildtool_depend&gt;\n&lt;exec_depend&gt;rosidl_default_runtime&lt;/exec_depend&gt;\n&lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;\n</code></pre> </li> <li> <p>And finally, we use Colcon to generate the necessary source code for the service interface:</p> <ol> <li> <p>Navigate to the root of the ROS2 Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part4_services --symlink-install \n</code></pre> </li> <li> <p>And re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Let's verify that this worked, using the <code>ros2</code> CLI (the same way as we did earlier when interrogating <code>tuos_interfaces/srv/NumberGame</code>):</p> <ol> <li> <p>First, list all the ROS service interfaces that are available on the system (<code>-s</code> to filter for service interface types remember!):</p> <pre><code>ros2 interface list -s\n</code></pre> <p>Scroll through this list and see if you can find <code>part4_services/srv/MyNumberGame</code> (or, use <code>grep</code> again).</p> </li> <li> <p>If it's there, use the <code>show</code> sub-command to show the data structure:</p> <pre><code>ros2 interface show part4_services/srv/MyNumberGame\n</code></pre> </li> </ol> <p>Does it match with the definition in our <code>MyNumberGame.srv</code> file?</p> </li> </ol>"},{"location":"course/assignment1/part4/#ex4","title":"Exercise 4: Adapting the Number Game Server","text":"<p>We're going to take a copy of the <code>tuos_examples/number_game.py</code> server node now, and adapt it to use the service interface that we created above.</p> <ol> <li> <p>In TERMINAL 1 still, navigate into the <code>part4_services/scripts</code> directory:</p> <pre><code>cd ~/ros2_ws/src/part4_services/scripts/\n</code></pre> </li> <li> <p>Copy the <code>number_game.py</code> script from the course repo into here, renaming it to <code>my_number_game.py</code> at the same time:</p> <pre><code>cp ../../tuos_ros/tuos_examples/scripts/number_game.py ./my_number_game.py\n</code></pre> <p>This file should already have execute permissions, but it's always worth checking...</p> </li> <li> <p>Declare this as a package executable by going back to the <code>CMakeLists.txt</code> file, and adding <code>my_number_game.py</code> below <code>minimal_node.py</code> (or just replacing <code>minimal_node.py</code> entirely):</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/my_number_game.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Build and re-source now: </p> <p></p><pre><code>cd ~/ros2_ws/\n</code></pre> <pre><code>colcon build --packages-select part4_services --symlink-install\n</code></pre> <pre><code>source ~/.bashrc \n</code></pre><p></p> </li> <li> <p>Now, let's look at the code, and see what needs to be adapted:</p> <ol> <li> <p>Open up the <code>my_number_game.py</code> node in VS Code and review it.</p> </li> <li> <p>As it stands, the node imports the <code>NumberGame</code> service interface from <code>tuos_interfaces</code>, so you'll need to change this to use the interface from our own package:</p> <pre><code>from part4_services.srv import MyNumberGame\n</code></pre> <p>You'll also need to change the <code>srv_type</code> definition, when the service is created in the <code>__init__()</code>:</p> <pre><code>self.srv = self.create_service(\n    srv_type=MyNumberGame,\n    srv_name='guess_the_number',\n    callback=self.srv_callback\n)\n</code></pre> </li> <li> <p>Everything that this service does (when a Request is sent to it), is contained within the <code>srv_callback()</code> method. </p> <p>In here, <code>request</code> parameters are processed, <code>response</code> parameters are defined and the overall <code>response</code> is returned once the callback tasks have been completed.</p> </li> <li> <p>You may have noticed that when we created our <code>MyNumberGame</code> interface in the previous exercise, the Response parameters were the same as the originals from Exercise 1 &amp; 2, except that some of their names were changed slightly!</p> <p>Work through the <code>srv_callback()</code> method and make sure that all <code>response</code> attributes are renamed to match the new names that we've given them in <code>MyNumberGame.srv</code>. </p> </li> <li> <p>Remember that our <code>MyNumberGame</code> interface has an additional Request parameter too: </p> <pre><code>bool cheat\n</code></pre> <p>... i.e. a boolean flag with the attribute name <code>cheat</code>.</p> <p>Adapt the <code>srv_callback()</code> method further now to read this value as well. </p> <ul> <li> <p>If (when a request is made to the server) the value of <code>cheat</code> is <code>True</code> the hint that the server returns should contain the actual secret number! E.g.:</p> <pre><code>hint='The answer is 67!'\n</code></pre> </li> <li> <p>In such situations, the value of <code>response.num_guesses</code> should still go up by one, and the <code>correct</code> flag should still return <code>False</code>.</p> </li> </ul> </li> </ol> </li> <li> <p>Once you've adapted the node, test it out by running it:</p> <p>TERMINAL 1: </p><pre><code>ros2 run part4_services my_number_game.py\n</code></pre><p></p> <p>You should then be able to make calls to this from TERMINAL 2 using the <code>ros2 service call</code> sub-command, as we did in Exercise 2.</p> <p>Hint</p> <p>There are two request parameters now, so when sending a request from the command-line both need to be supplied. Do this by including them both within the braces (<code>{}</code>) and separated with a comma, e.g.</p> <pre><code>ros2 service call /guess_the_number part4_services/srv/MyNumberGame \"{guess: X, cheat: Y}\" \n</code></pre> </li> </ol>"},{"location":"course/assignment1/part4/#ex5","title":"Exercise 5: Creating a Python Service Client","text":"<p>So far we've been making service calls from the command-line, but we can also call services from within Python Nodes. When a node calls (i.e. requests) a service, it becomes a Service \"Client\".</p> <ol> <li> <p>Make sure your <code>my_number_game.py</code> node is still active in TERMINAL 1 for this exercise.</p> </li> <li> <p>In TERMINAL 2, create a new file in the <code>part4_services/scripts</code> directory called <code>number_game_client.py</code>.</p> <pre><code>touch ~/ros2_ws/src/part4_services/scripts/number_game_client.py\n</code></pre> </li> <li> <p>Make this executable (with <code>chmod</code>).</p> </li> <li> <p>Add this to your package's Python executables list in <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/my_number_game.py\n  scripts/number_game_client.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Re-build your package (as before), remembering that there are three steps to this:</p> <ol> <li>Navigate to the root of the ROS2 workspace,</li> <li>Run <code>colcon build</code> (with the necessary additional arguments), </li> <li>Re-source your <code>~/.bashrc</code>.</li> </ol> <p></p> </li> <li> <p>Now, open up the <code>number_game_client.py</code> file in VS Code. Have a look at the code here, review it (including all the annotations), then copy and paste it and save the file.</p> </li> <li> <p>You should now be able to run the code with <code>ros2 run</code>. To begin with, run it without supplying any additional command-line arguments:</p> <p>TERMINAL 2: </p><pre><code>ros2 run part4_services number_game_client.py\n</code></pre><p></p> <p>You'll probably then get an output like this:</p> <pre><code>[INFO] [#####] [number_game_client]: Sending the request:\n - guess: 0\n - cheat: False\n   Awaiting response...\n[INFO] [#####] [number_game_client]: The server has responded with:\n - Incorrect guess :(\n - Number of attempts so far: 1\n - A hint: 'Higher'.\n</code></pre> <p>Notice how the request parameters <code>guess</code> and <code>cheat</code> have defaulted to <code>0</code> and <code>False</code> respectively?</p> </li> <li> <p>Supply a guess now, using our node's CLI:</p> <p>TERMINAL 2: </p><pre><code>ros2 run part4_services number_game_client.py --guess GUESS\n</code></pre><p></p> <p>... replacing <code>GUESS</code> with an actual number!</p> </li> <li> <p>Have a go at cheating now too:</p> <p>TERMINAL 2: </p><pre><code>ros2 run part4_services number_game_client.py --cheat\n</code></pre><p></p> <p>... notice how we only need to supply the <code>--cheat</code> flag, no actual value is required.</p> </li> </ol>"},{"location":"course/assignment1/part4/#the-map-saver-service","title":"The Map Saver Service","text":"<p>Clearly the examples that we've been working with here so far have been fairly trivial: it's unlikely that you'll ever need to program a robot to play the number game! The aim however has been to illustrate how ROS Services work, and how to develop your own. </p> <p>One application that you might find useful however is map saving. In Part 3 we learnt about SLAM, where we drove a robot around in an environment while SLAM algorithms were working in the background to generate a map of the world using data from the robot's LiDAR sensor and its odometry system:</p> <p></p> <p>Having mapped out the environment, we called up a node called <code>map_saver_cli</code> from the <code>nav2_map_server</code> package, to save a copy of that map to a file:</p> <pre><code>ros2 run nav2_map_server map_saver_cli -f MAP_NAME\n</code></pre> <p>... wouldn't it be nice if there was a way to be able to do this programmatically (i.e. from within a Python node, for example) rather than having to run the above command manually? Well, there is a way, and guess what - it involves Services!</p>"},{"location":"course/assignment1/part4/#ex6","title":"Exercise 6: Developing A Map Saver Service Client","text":"<ol> <li> <p>Make sure everything in TERMINALS 1 and 2 from the previous exercises are closed down now.</p> </li> <li> <p>In TERMINAL 1, let's fire up the Nav World again, like we did in Part 3:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch tuos_simulations nav_world.launch.py\n</code></pre><p></p> <p></p> <p></p> </li> <li> <p>In TERMINAL 2, let's also fire up Cartographer again (the SLAM algorithms):</p> <p>TERMINAL 2: </p><pre><code>ros2 launch tuos_simulations cartographer.launch.py\n</code></pre><p></p> <p></p> <p></p> </li> <li> <p>Open up another terminal instance now (TERMINAL 3), and use this one to fire up the Map Saver Service (wouldn't it be nice if we could launch all of these launch files at once?!):</p> <p>TERMINAL 3: </p><pre><code>ros2 launch nav2_map_server map_saver_server.launch.py\n</code></pre><p></p> <p>This will add a number of <code>/map_saver</code> services to our ROS network.</p> </li> <li> <p>Use a <code>ros2 service</code> sub-command to identify all the <code>/map_saver</code> services (like we did in Exercise 1).</p> <p>Question</p> <p>Do you see an item in this list that could be related to saving a map? (It has a <code>/map_saver</code> prefix<sup>3</sup>!)</p> </li> <li> <p>Use another <code>ros2 service</code> sub-command to determine the type of interface used by this service (again, like we did in Exercise 1).</p> </li> <li> <p>Next, use a <code>ros2 interface</code> command to discover the structure of this service interface.</p> <p>Questions</p> <ol> <li>How many Request parameters does this interface have?</li> <li>How many Response parameters are there too?<sup>4</sup></li> <li>What are their data types?</li> </ol> </li> <li> <p>Develop a Python Service Client to make calls to this service:</p> <ol> <li> <p>Create a new Node in your <code>part4_services</code> package called <code>map_saver_client.py</code> for this. </p> <p>Things to remember when doing this:</p> <ul> <li> Create this in your <code>part4_services/scripts</code> directory.</li> <li> Make sure it has execute permissions.</li> <li> Declare it as a package executable in your <code>CMakeLists.txt</code>.</li> <li> Re-build your package with <code>colcon</code>, making sure you follow the full three-step process </li> </ul> </li> <li> <p>Use the <code>number_game_client.py</code> Node from Exercise 5 as a starting point when building your new <code>map_saver_client.py</code> node... all the same principals will apply here, you are just applying them to a different service (and therefore you need to account for a different service interface).</p> </li> <li> <p>Use <code>argparse</code> again to build a CLI for your <code>map_saver_client.py</code> node.</p> <p>In this case, your CLI should only need one argument though (where, in <code>number_game_client.py</code> there were two). This should be used to pass in a filename for a map, e.g.:</p> <pre><code>ros2 run part4_services map_saver_client.py --map-file my_amazing_map\n</code></pre> <p>You should ensure that this argument is optional though, i.e. if the argument isn't provided when your node is called, then a default value is applied instead.</p> </li> <li> <p>When constructing service requests, consider the following tips:</p> <ol> <li>The SLAM map data (as generated by Cartographer) is published to a topic called <code>/map</code>. </li> <li> <p>When providing a name for the map file:</p> <ul> <li>You don't need to include a file extension</li> <li> <p>File names are interpreted relative to your home directory, so:</p> <p><code>my_amazing_map</code> would result in a map file at <code>~/my_amazing_map.yaml</code></p> <p><code>my/amazing/map</code> would result in a map file at <code>~/my/amazing/map.yaml</code> (assuming the directory structure already exists!)</p> </li> </ul> </li> <li> <p>For further guidance see here for a usage example.</p> </li> <li>The server will apply its own defaults to certain parameters, if they aren't set in the request.</li> </ol> </li> </ol> </li> </ol>"},{"location":"course/assignment1/part4/#summary-of-the-map-saver-service","title":"Summary of the Map Saver Service","text":"<p>Back in Part 3 we saved our SLAM map once via a command-line call after we had fully explored and mapped out the environment:</p> <pre><code>ros2 run nav2_map_server map_saver_cli -f MAP_NAME\n</code></pre> <p>... which resulted in something like:</p> <p> </p> The <code>MAP_NAME.pgm</code> file <p>In real-world tasks however (i.e. tasks that you might need to complete in Assignment #2 for example), your robot might be exploring an environment autonomously, and you don't necessarily know when the full environment has been explored, nor are you always going to be there to run the <code>map_saver_cli</code> node manually! You might therefore want to program your robot with the ability to save a map incrementally and periodically as more and more of the environment is explored.</p> <p>The process that we explored in the previous exercise allows you to do just that! In the example, our client node was programmed to make only one request to the server and then stop. It could however be programmed to make regular service requests (say, once every 5 or 10 seconds) in order to continuously update its map as the robot explores further and further.</p> <p>Think about how you might adapt the <code>map_saver_client.py</code> node to achieve this, drawing upon other exercises that you have worked through in previous parts of this course.  </p>"},{"location":"course/assignment1/part4/#wrapping-up","title":"Wrapping Up","text":"<p>In Part 4 we've learnt about ROS Services and why they might be useful for robot applications:</p> <ul> <li>Services differ from standard topic-based communication methods in ROS in that they are a call and response type of communication, taking place between one node and another.  </li> <li>Typically, a service Caller will request a service, and then wait for a response (although it is possible to do other things in the meantime).</li> <li>In general however, Services are useful for controlling quick, short-duration tasks or calculations.</li> </ul>"},{"location":"course/assignment1/part4/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, to save the work you have done in WSL-ROS2 during this session so that you can restore it on a different machine at a later date. Run the following script in any idle WSL-ROS2 Terminal Instance now:</p> <pre><code>wsl_ros backup\n</code></pre> <p>You'll then be able to restore it to a fresh WSL-ROS2 environment next time you fire one up (<code>wsl_ros restore</code>).  </p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> <li> <p>On the real Waffles, there's a service called <code>/sound</code>. Have a look at this next time you're in the lab... Once you've worked through the whole of Part 4 you'll know exactly how to interrogate this service and leverage the functionality that it provides!\u00a0\u21a9</p> </li> <li> <p>There should be one in the list called <code>/map_saver/save_map</code> \u21a9</p> </li> <li> <p>The <code>nav2_msgs/srv/SaveMap</code> Service Interface has 6 Requests (<code>map_topic</code>, <code>map_url</code>, <code>image_format</code>, <code>map_mode</code>, <code>free_thresh</code>, <code>occupied_thresh</code>) and 1 Response (<code>result</code>).\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment1/part5/","title":"Part 5: Actions","text":""},{"location":"course/assignment1/part5/#introduction","title":"Introduction","text":"<p> Exercises: 6 (5 core, 1 advanced) Estimated Completion Time: 3 hours (core exercises only) Difficulty Level: Advanced </p>"},{"location":"course/assignment1/part5/#aims","title":"Aims","text":"<p>In this part of the course we'll learn about a third method of communication available in ROS: Actions.  Actions are essentially an advanced version of Services, and we'll look at exactly how these two differ and why you might choose to employ an action over a service for certain tasks. </p>"},{"location":"course/assignment1/part5/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Recognise how ROS Actions differ from ROS Services and explain where this method might be useful in robot applications.</li> <li>Explain the structure of Action interfaces and identify the relevant information within them, enabling you to build Action Servers and Clients.</li> <li>Implement Python Action Client nodes that can handle feedback and results and can also cancel an action part-way through.</li> <li>Develop Action Server &amp; Client nodes that could be used as the basis for a robotic exploration strategy.</li> </ol>"},{"location":"course/assignment1/part5/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Launching an Action Server and calling it from the command-line</li> <li>Exercise 2: Building a Python Action Client Node</li> <li>Exercise 3: Creating an Action Interface</li> <li>Exercise 4: Building the \"ExploreForward\" Action Server</li> <li>Exercise 5: Building a Basic \"ExploreForward\" Client</li> <li>Exercise 6 (Advanced): Implementing an Exploration Strategy</li> </ul>"},{"location":"course/assignment1/part5/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Explore Server Template (for Exercise 4)</li> </ul>"},{"location":"course/assignment1/part5/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>Launch your ROS environment now so that you have access to a Linux terminal instance (aka TERMINAL 1).</p> <p>Step 2: Restore your work (WSL-ROS2 Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS2 Environment will not be preserved between sessions or across different University computers, and so you should be backing up your work to your <code>U:\\</code> drive regularly. When prompted (on first launch of WSL-ROS2 in TERMINAL 1) enter <code>Y</code> to restore this<sup>1</sup>.</p> <p>Step 3: Launch VS Code </p> <p>WSL users remember to check for this.</p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Check that the Course Repo is up-to-date before you start on these exercises. See here for how to install and/or update.</p>"},{"location":"course/assignment1/part5/#calling-an-action-server","title":"Calling an Action Server","text":"<p>Before we talk about what actions actually are, we're going to dive straight in and see one in action (excuse the pun). </p>"},{"location":"course/assignment1/part5/#ex1","title":"Exercise 1: Launching an Action Server and calling it from the command-line","text":"<p>We'll play a little game here. We're going to launch our TurtleBot3 Waffle in a mystery environment now, and we're going to do this by launching Gazebo headless i.e. Gazebo will be running behind the scenes, but there'll be no Graphical User Interface (GUI) to show us what the environment actually looks like.  Then, we'll use an action server to make our robot scan the environment and take pictures for us, to reveal its surroundings!</p> <ol> <li> <p>To launch the TurtleBot3 Waffle in this mystery environment, use the following <code>ros2 launch</code> command:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch tuos_simulations mystery_world.launch.py\n</code></pre><p></p> <p>Messages in the terminal should indicate that something has happened, but that's about all you will see!</p> </li> <li> <p>Next, open up a new terminal (TERMINAL 2), and have a look at all the topics that are currently active on the ROS network (you should know exactly how to do this by now!)</p> <p>The output of this should confirm that ROS and our robot are indeed active...</p> How? <p>When the robot is active, the output of the <code>ros2 topic list</code> command should provide a long list of topics, a number of which we've been working with throughout this course so far, such as <code>cmd_vel</code>, <code>odom</code>, <code>scan</code>, and so on. If the Waffle simulation isn't active then we would be presented with a much smaller list, containing only the core ROS topics:</p> <p>TERMINAL 2: </p><pre><code>$ ros2 topic list\n/parameter_events\n/rosout\n</code></pre><p></p> </li> <li> <p>Next, run the following command to launch an Action Server on the network:</p> <p>TERMINAL 2: </p><pre><code>ros2 run tuos_examples camera_sweep_action_server.py\n</code></pre><p></p> </li> <li> <p>Now, open up another new terminal instance (TERMINAL 3), but so that you can view this and TERMINAL 2 side-by-side. Enter the following command to list all actions that are active on the ROS network:</p> <p>TERMINAL 3: </p><pre><code>ros2 action list\n</code></pre><p></p> <p>There should be an item here called <code>/camera_sweep</code>, use the <code>info</code> command to find out more about this:</p> <pre><code>ros2 action info /camera_sweep\n</code></pre> <p>This tells us the name of the action: <code>Action: /camera_sweep</code>, as well as the number of client and server nodes this action has. Currently, the action should have 0 clients and 1 server, and the node acting as the server here should be listed as <code>/camera_sweep_action_server_node</code> (the node that we just launched with the <code>ros2 run</code> command in TERMINAL 2).</p> <p>Finally, call the <code>ros2 action info</code> command again, but this time providing an additional argument:</p> <pre><code>ros2 action info -t /camera_sweep\n</code></pre> <pre><code>Action: /camera_sweep\nAction clients: 0\nAction servers: 1\n    /camera_sweep_action_server [tuos_interfaces/action/CameraSweep]\n</code></pre> <p>The <code>-t</code> argument additionally shows the action type against the server node, indicating to us the type of interface used by the server.</p> </li> <li> <p>Let's now find out more about the interface itself. As with any interface (message, service or action) we can use the <code>ros2 interface</code> command to do this.</p> <p>TERMINAL 3: </p><pre><code>ros2 interface show tuos_interfaces/action/CameraSweep\n</code></pre><p></p> <p>Which should present us with the following:</p> <pre><code>#goal\nfloat32 sweep_angle    # the angular sweep (in degrees) over which to capture images\nint32 image_count      # the number of images to capture during the sweep\n---\n#result\nstring image_path      # The filesystem location of the captured images\n---\n#feedback\nint32 current_image    # the number of images taken\nfloat32 current_angle  # the current angular position of the robot (in degrees)\n</code></pre> <p>There are three parts to an action interface, and we'll talk about these in a bit more detail later on, but for now, all we need to know is that in order to call an action, we need to send the action server a Goal.</p> Comparing with ROS Services <p>This is a bit like sending a Request to a ROS Service Server, like we did in the previous session.</p> </li> <li> <p>We can issue a goal to an action server from the command-line using the <code>ros2 action</code> command again. Let's give this a go in TERMINAL 3.</p> <p>First, let's identify the right <code>ros2 action</code> sub-command:</p> <p>TERMINAL 3: </p><pre><code>ros2 action --help\n</code></pre><p></p> <pre><code>Commands:\n  info       Print information about an action\n  list       Output a list of action names\n  send_goal  Send an action goal\n</code></pre> <p>As above, there are three sub-commands to choose from, and we've already used the first two! Clearly, the <code>send_goal</code> command is the one we want now.</p> <p>Let's get some help on this one:</p> <pre><code>ros2 action send_goal --help\n</code></pre> <p>From this, we learn that there are three positional arguments, which must be supplied in the correct order:</p> <pre><code>ros2 action send_goal action_name action_type goal\n</code></pre> <p>We know from our earlier interrogation with the <code>ros2 action list</code>, <code>info</code> and <code>ros2 interface show</code> commands how to provide the right data here:</p> <ol> <li><code>action_name</code>: <code>/camera_sweep</code></li> <li><code>action_type</code>: <code>tuos_interfaces/action/CameraSweep</code></li> <li><code>goal</code>: a data packet (in YAML format) containing two parameters:<ol> <li><code>sweep_angle</code>: the angle (in degrees) that the robot will rotate on the spot (i.e. 'sweep')</li> <li><code>image_count</code>: the number of images it will capture from its front-facing camera while 'sweeping'</li> </ol> </li> </ol> </li> <li> <p>Now, again in TERMINAL 3, have a go at using the <code>ros2 action send_goal</code> command, but keep an eye on TERMINAL 2 as you do this:</p> <p>TERMINAL 3: </p><pre><code>ros2 action send_goal /camera_sweep tuos_interfaces/action/CameraSweep \"{sweep_angle: 0, image_count: 0}\"\n</code></pre><p></p> <p>Having called the action, you should then be presented with a message (in TERMINAL 3) that the <code>Goal was rejected.</code> In TERMINAL 2 (where the action server is running), we should see some additional information about why this was the case. Read this, and then head back to TERMINAL 3 and have another go at sending a goal to the action server, by supplying valid inputs this time!</p> <p>Once valid goal parameters have been supplied, the action server (in TERMINAL 2), will respond to inform you of what it's going to do. You'll then need to wait for it to do its job...</p> </li> <li> <p>Once the action has completed (it could up to 20 seconds), a message should appear in TERMINAL 3 to inform us of the outcome:</p> <p>TERMINAL 3: </p><pre><code>Result:\n    image_path: ~/myrosdata/action_examples/YYYYMMDD_hhmmss\n\nGoal finished with status: SUCCEEDED\n</code></pre><p></p> <p>Additionally, we should see some further text in TERMINAL 2 as well:</p> <p>TERMINAL 2: </p><pre><code>[INFO] [#####] [camera_sweep_action_server]: camera_sweep_action_server completed successfully:\n  - Angular sweep = # degrees\n  - Images captured = #\n  - Time taken = # seconds\n</code></pre><p></p> <ol> <li> <p>The result of the action (presented to us in TERMINAL 3) is a file path. Navigate to this directory in TERMINAL 3 (using <code>cd</code>) and have a look at the content using <code>ll</code> (a handy alias for the <code>ls</code> command):</p> <p>You should see the same number of image files in there as was requested with the <code>image_count</code> parameter.</p> </li> <li> <p>Launch <code>eog</code> in this directory and click through all the images to reveal your robot's mystery environment:</p> <p>TERMINAL 3: </p><pre><code>eog .\n</code></pre><p></p> </li> </ol> </li> <li> <p>Let's do this one more time. Close down the <code>eog</code> window, head back to TERMINAL 3 and issue the <code>ros2 action send_goal</code> command again, but this time use the optional <code>-f</code> flag: </p> <p>TERMINAL 3: </p><pre><code>ros2 action send_goal -f /camera_sweep tuos_interfaces/action/CameraSweep \"{sweep_angle: 0, image_count: 0}\" \n</code></pre><p></p> <p>Tip</p> <p>Don't forget to supply valid goal parameters again!</p> <p>Now, as well as being provided with a result once the action has completed, we're also provided with some regular updates while the action is in progress (aka \"feedback\")! </p> </li> <li> <p>To finish off, close down the action server in TERMINAL 2 and the headless Gazebo process in TERMINAL 1 by entering Ctrl+C in each terminal. </p> </li> </ol>"},{"location":"course/assignment1/part5/#what-is-a-ros-action","title":"What is a ROS Action?","text":"<p>In this Exercise we launched an action server and then called it from the command-line using the <code>ros2 action send_goal</code> sub-command. Using the <code>-f</code> flag we were able to ask the server to provide us with real-time feedback on how it was getting on (in TERMINAL 3). In the same way as a ROS Service, the action also provided us with a result once the task had been completed. Feedback is one of the key features that differentiates a ROS Action from a ROS Service: An Action Server provides feedback messages at regular intervals whilst performing an action and working towards its goal. Another feature of ROS Actions is that they can be cancelled part-way through (which we'll play around with shortly).</p> <p></p> <p>Ultimately, Actions use a combination of both Topic- and Service-based communication, to create a more advanced messaging protocol. Due to the provision of feedback and the ability to cancel a process part-way through, Actions are designed to be used for longer running tasks. You can read more about Actions in the official ROS 2 documentation here (which also includes a nice animation to explain how they work).</p>"},{"location":"course/assignment1/part5/#the-format-of-action-interfaces","title":"The Format of Action Interfaces","text":"<p>Like Services, Action Interfaces have multiple parts to them, and we need to know what format these action messages take in order to be able to use them.</p> <p>We ran <code>ros2 interface show</code> in the previous exercise, to interrogate the action interface used by the <code>/camera_sweep</code> action server:</p> <pre><code>$ ros2 interface show tuos_interfaces/action/CameraSweep\n\n#goal\nfloat32 sweep_angle    # the angular sweep (in degrees) over which to capture images\nint32 image_count      # the number of images to capture during the sweep\n---\n#result\nstring image_path      # The filesystem location of the captured images\n---\n#feedback\nint32 current_image    # the number of images taken\nfloat32 current_angle  # the current angular position of the robot (in degrees)\n</code></pre> <p>As we know from Exercise 1, in order to call this action server, we need to send a goal, and (as we know from Exercise 1) there are two goal parameters that must be provided:</p> <ol> <li><code>sweep_angle</code>: a 32-bit floating-point value</li> <li><code>image_count</code>: a 32-bit integer</li> </ol> <p>Questions</p> <ul> <li>What are the names of the result and feedback interface parameters? (There are three in total.)</li> <li>What data types do these parameters use?</li> </ul> <p>You'll learn how we use this information to develop Python Action Server &amp; Client nodes in the following exercises.</p>"},{"location":"course/assignment1/part5/#creating-python-action-clients","title":"Creating Python Action Clients","text":"<p>In the previous exercise we called a pre-existing Action Server from the command-line, by sending a goal to it. Let's look at how we can do this from within a Python ROS node now.</p>"},{"location":"course/assignment1/part5/#ex2","title":"Exercise 2: Building a Python Action Client Node","text":"<ol> <li> <p>In TERMINAL 1 launch the mystery world simulation again, but this time with an additional argument:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch tuos_simulations mystery_world.launch.py with_gui:=true\n</code></pre><p></p> <p>Which will launch Gazebo in full now, with the GUI attached.</p> </li> <li> <p>Then, in TERMINAL 2, launch the Camera Sweep Action Server again: </p> <p>TERMINAL 2: </p><pre><code>ros2 run tuos_examples camera_sweep_action_server.py\n</code></pre><p></p> </li> </ol>"},{"location":"course/assignment1/part5/#part-1-a-minimal-action-client","title":"Part 1: A Minimal Action Client","text":"<ol> <li> <p>Now, in TERMINAL 3, create a new package called <code>part5_actions</code> using the <code>create_pkg.sh</code> helper script from the <code>tuos_ros</code> course repo (return here for a reminder on how to do this).</p> </li> <li> <p>Navigate into the <code>scripts</code> folder of your package using the <code>cd</code> command:</p> <pre><code>cd ~/ros2_ws/src/part5_actions/scripts/\n</code></pre> </li> <li> <p>In here, create a new Python file called <code>camera_sweep_action_client.py</code> (using the <code>touch</code> command) and make it executable (using <code>chmod</code>). </p> </li> <li> <p>Then, declare this as an executable in the package's <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/camera_sweep_action_client.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Also modify the <code>package.xml</code> file (below the <code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;</code> line) to include following <code>msg</code> dependency:</p> package.xml<pre><code>&lt;exec_depend&gt;action_msgs&lt;/exec_depend&gt;\n</code></pre> </li> <li> <p>At an absolute minimum, the Action Client can be constructed as follows:</p> camera_sweep_action_client.py<pre><code>#! /usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient # (1)!\n\nfrom tuos_interfaces.action import CameraSweep # (2)!\n\nclass CameraSweepActionClient(Node):\n\n    def __init__(self):\n        super().__init__(\"camera_sweep_action_client\") # (3)!\n        self.actionclient = ActionClient(\n            node=self, \n            action_type=CameraSweep, \n            action_name=\"camera_sweep\"\n        ) # (4)!\n\n    def send_goal(self, images=0, angle=0): # (5)!\n        goal = CameraSweep.Goal()\n        goal.sweep_angle = float(angle)\n        goal.image_count = images\n\n        self.actionclient.wait_for_server()\n\n        # send the goal to the action server:\n        return self.actionclient.send_goal_async(goal)\n\ndef main(args=None): # (6)!\n    rclpy.init(args=args)\n    action_client = CameraSweepActionClient()\n    future = action_client.send_goal()\n    rclpy.spin_until_future_complete(action_client, future)\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>As you know by now, in order to develop ROS nodes using Python we need to import the <code>rclpy</code> client library, and the <code>Node</code> class to base our node upon. In addition, here we're also importing an <code>ActionClient</code> class too.  </p> </li> <li> <p>We know that the <code>/camera_sweep</code> Action server uses the <code>CameraSweep</code> <code>action</code> interface from the <code>tuos_interfaces</code> package, so we import that here too (which we use to make a call to the server). </p> </li> <li> <p>Standard practice when we initialise ROS nodes: we must give them a name</p> </li> <li> <p>Here, we instantiate an <code>ActionClient</code> class object. In doing this we define the <code>node</code> to add the action client too (in our case <code>self</code>, i.e. our <code>CameraSweepActionClient</code> class). We then also define the interface type used by the server (<code>CameraSweep</code>), and the name of the action that we want to call (<code>action_name=\"camera_sweep\"</code>).</p> </li> <li> <p>Here we define a class method to construct and deliver a goal to the server. </p> <p>As we know from earlier, a <code>CameraSweep.Goal()</code> contains two parameters that we can assign values to: <code>sweep_angle</code> and <code>image_count</code>.</p> <p>The goal is sent to the server using the <code>send_goal_async()</code> method, which returns a future: i.e. something that will happen in the future, that we can wait on.</p> <p>Tip</p> <p>Both goal parameters are set to <code>0</code> by default!</p> </li> <li> <p>In our <code>main</code> method we initialise <code>rclpy</code> and our <code>CameraSweepActionClient</code> class (nothing new here), but then we call the <code>send_goal()</code> method of our class (as discussed above), which returns a future. We can then use the <code>rclpy.spin_until_future_complete()</code> method to spin up our node only until this future object has finished.</p> <p>Warning</p> <p>When the <code>send_goal()</code> method is called, no additional arguments are provided, which means default values will be applied... which were defined above!</p> </li> </ol> </li> <li> <p>Let's build the Node now, so that we can run it. Head back to TERMINAL 3 and use Colcon to build the package: </p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-select part5_actions --symlink-install\n</code></pre> </li> <li> <p>Re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Run your node with <code>ros2 run</code>...</p> <p>As you have hopefully just observed, the node that we've created here makes a call to the action server, waits for the action to take place and then stops. The only way that you'd know what was happening however, is if you were to keep an eye on TERMINAL 2, to see the action server respond to the goal that was sent to it... The client itself provides no feedback during the action, nor the result at the end. Let's look to incorporate that now...</p> </li> </ol>"},{"location":"course/assignment1/part5/#part-2-handling-a-result","title":"Part 2: Handling a Result","text":"<ol> <li> <p>Go back to the <code>camera_sweep_action_client.py</code> file in VS Code.</p> </li> <li> <p>In order to be able to handle the result that is sent from an action server, we first need to handle the response that the server sends to the goal itself.</p> <p>Within the <code>send_goal()</code> method of the <code>CameraSweepActionClient()</code> class, find the line that reads:</p> <pre><code>return self.actionclient.send_goal_async(goal)\n</code></pre> <p>and change this to:</p> <pre><code>self.send_goal_future = self.actionclient.send_goal_async(goal)\nself.send_goal_future.add_done_callback(self.goal_response_callback)\n</code></pre> <p>This method is no longer returning the future that is sent from <code>send_goal_async()</code>, but is now handling this and adding a callback to it: <code>goal_response_callback</code>. This callback will be executed to inform the client of whether the server has accepted the goal or not.</p> </li> <li> <p>Define this as a new class method of the <code>CameraSweepActionClient()</code> class (i.e. underneath the <code>send_goal()</code> class method that has already been defined)...</p> <pre><code>def goal_response_callback(self, future):\n    goal_handle = future.result()\n    if not goal_handle.accepted:\n        self.get_logger().warn(\"The goal was rejected by the server.\")\n        return\n\n    self.get_logger().info(\"The goal was accepted by the server.\")\n\n    self.get_result_future = goal_handle.get_result_async()\n    self.get_result_future.add_done_callback(self.get_result_callback)\n</code></pre> <p>The input to this method will be the future that is created by the <code>send_goal_async()</code> call. We assign this to <code>goal_handle</code> here, and can then use this for two purposes:</p> <ol> <li>To check if the goal that we sent was accepted by the server</li> <li>If it was accepted, then we can get the result (using <code>get_result_async()</code>) and we can attach another callback to this to actually process that result: <code>get_result_callback</code>.</li> </ol> </li> <li> <p>Now, define <code>get_result_callback</code> as another new method of the <code>CameraSweepActionClient()</code> class (i.e. underneath the <code>goal_response_callback()</code> class method that we have just defined)...</p> <pre><code>def get_result_callback(self, future):\n    result = future.result().result\n    self.get_logger().info(\n        f\"The action has completed.\\n\"\n        f\"Result:\\n\"\n        f\"  - Image Path = {result.image_path}\"\n    )\n    rclpy.shutdown()\n</code></pre> <p>The input to this class method is another future object which contains the actual result sent from the server. We assign this to <code>result</code> and use a <code>get_logger().info()</code> call to print this to the terminal when the action has finished.</p> <p>As we know from our work earlier, the <code>CameraSweep</code> interface contains one <code>result</code> parameter called <code>image_path</code>.</p> </li> <li> <p>Finally, in the <code>main</code> method, change this:</p> <pre><code>rclpy.spin_until_future_complete(action_client, future)\n</code></pre> <p>to this:</p> <pre><code>rclpy.spin(action_client)\n</code></pre> </li> <li> <p>Save all your changes!</p> </li> <li> <p>Run this node again now (with the <code>ros2 run</code> command) and observe the changes in action.</p> <p>Our client node now presents us with the result that is sent by the server on completion of the action, but wouldn't it be nice if we could see the real-time feedback as the action takes place? Let's add this in now...</p> </li> </ol>"},{"location":"course/assignment1/part5/#part-3-handling-feedback","title":"Part 3: Handling Feedback","text":"<ol> <li> <p>Go back to the <code>camera_sweep_action_client.py</code> file in VS Code.</p> </li> <li> <p>In order to be able to handle the feedback that is sent from an action server, we need to add yet another callback! Go back to the <code>send_goal()</code> method and the line where we are actually sending the goal to the server:</p> <pre><code>self.send_goal_future = self.actionclient.send_goal_async(goal)\n</code></pre> <p>As it stands, all we're doing here is sending the goal, but we can also add a feedback callback to this too:</p> <pre><code>self.send_goal_future = self.actionclient.send_goal_async(\n    goal=goal, \n    feedback_callback=self.feedback_callback\n)\n</code></pre> <p>The <code>feedback_callback</code> will be executed every time a new feedback message is received from the server.</p> </li> <li> <p>In order to define what we want to do with these feedback messages we need to define this yet another new method of the <code>CameraSweepActionClient()</code> class. Underneath the <code>get_result_callback()</code> class method that we defined earlier, add this new one as well:</p> <pre><code>def feedback_callback(self, feedback_msg):\n    feedback = feedback_msg.feedback\n    fdbk_current_angle = feedback.current_angle\n    fdbk_current_image = feedback.current_image\n    self.get_logger().info(\n        f\"\\nFEEDBACK:\\n\"\n        f\"  - Current angular position = {fdbk_current_angle:.1f} degrees.\\n\"\n        f\"  - Image(s) captured so far = {fdbk_current_image}.\"\n    )\n</code></pre> <p>As we know from our work earlier, the <code>CameraSweep</code> interface contains two feedback parameters: <code>fdbk_current_angle</code> and <code>fdbk_current_image</code>.</p> </li> <li> <p>Save all your changes once again, run the node again with the <code>ros2 run</code> command and observe the changes in action.</p> <p>The node we've now built can send a goal to an action server, process the feedback sent form the server as the action is in progress, and present the result to us once everything is complete.</p> <p>As discussed earlier though, the other key feature of Actions is the ability to cancel them part-way through. So let's look at how to incorporate this now as well.</p> </li> </ol>"},{"location":"course/assignment1/part5/#part-4-cancelling-an-action","title":"Part 4: Cancelling an Action","text":"<ol> <li> <p>First, create a copy of your <code>camera_sweep_action_client.py</code> node and call it <code>camera_sweep_action_client_cancel.py</code> (make sure you're still in the <code>scripts</code> directory of your <code>part5_actions</code> package before you run this command):</p> <p>TERMINAL 3: </p><pre><code>cp camera_sweep_action_client.py camera_sweep_cancel_client.py\n</code></pre><p></p> </li> <li> <p>Don't forget to declare this as an additional executable in the package's <code>CMakeLists.txt</code>. You'll then also need to re-build the package with <code>colcon build</code> (go back for a reminder).</p> </li> <li> <p>Open the <code>camera_sweep_cancel_client.py</code> file in VS Code.</p> </li> <li> <p>We want this client to be able to cancel the goal under two different circumstances:</p> <ol> <li>The client itself is shutdown by the user (via a Ctrl+C in the terminal)</li> <li>A conditional event that happens as the action is underway.</li> </ol> <p>In order to address item 1 first, we need to draw upon some of the work we did in Part 2 in the implementation of safe shutdown procedures...</p> </li> <li> <p>As you hopefully recall, we first need to import <code>SignalHandlerOptions</code> into our node, so add this as an additional import at the start of the code:</p> <pre><code>from rclpy.signals import SignalHandlerOptions\n</code></pre> <p>Then, in the <code>main()</code> node function, modify the <code>rclpy.init()</code> call:</p> <pre><code>rclpy.init(\n    args=args,\n    signal_handler_options=SignalHandlerOptions.NO\n)\n</code></pre> </li> <li> <p>Inside the <code>__init__()</code> method of our <code>CameraSweepActionClient()</code> class we now need to add some additional flags:</p> <pre><code>self.goal_succeeded = False\nself.goal_cancelled = False\nself.stop = False\n</code></pre> <p>In the <code>get_result_callback()</code> class method, we can then ensure that the <code>self.goal_succeeded</code> flag is sent to <code>True</code> when a result is received. In this class methods, locate the <code>rclpy.shutdown()</code> line and add the following additional line just above it:</p> <pre><code>self.goal_succeeded = True\n</code></pre> </li> <li> <p>Actions can be cancelled using a <code>cancel_goal_async()</code> method of the <code>goal_handle</code> that is obtained from the <code>goal_response_callback()</code>. As such, we need to make this accessible across our entire <code>CameraSweepActionClient()</code> class. Locate the <code>goal_response_callback()</code> class method, and add this line at the bottom as the last line of the <code>goal_response_callback()</code> method:</p> <pre><code>self._goal_handle = goal_handle\n</code></pre> <p>This makes <code>goal_handle</code> accessible across the entire <code>CameraSweepActionClient()</code> class as <code>self._goal_handle</code>. </p> </li> <li> <p>We can only attempt to cancel an Action when it's in progress, therefore the feedback callback is the best place to trigger this. Locate the <code>feedback_callback()</code> class method and place the following at the end of it:</p> <pre><code>if self.stop:\n    future = self._goal_handle.cancel_goal_async()\n    future.add_done_callback(self.cancel_goal)\n</code></pre> <p>Here, we call the <code>cancel_goal_async()</code> method from <code>self._goal_handle</code>, and add another new callback (<code>cancel_goal()</code>) to it (i.e. to encapsulate what we want to happen when the action is cancelled).</p> </li> <li> <p>Now, let's define this as another new (and final!) class method:</p> <pre><code>def cancel_goal(self, future):\n    cancel_response = future.result()\n    if len(cancel_response.goals_canceling) &gt; 0:\n        self.get_logger().info('Goal successfully canceled')\n        self.goal_cancelled = True\n    else:\n        self.get_logger().info('Goal failed to cancel')\n</code></pre> <p>The input to this callback is another future, which we can use to determine if the goal has been cancelled (as shown above). If it has, then we set our <code>self.goal_cancelled</code> flag to <code>True</code>.</p> </li> <li> <p>Finally, go back the <code>main()</code> function of the node. We're going to replace the <code>rclpy.spin(action_client)</code> line now, with a <code>rclpy.spin_once()</code>, wrapped inside a <code>try</code> - <code>except</code>, wrapped inside a <code>while</code> loop!</p> <pre><code>while not action_client.goal_succeeded:\n    try:\n        rclpy.spin_once(action_client)\n        if action_client.goal_cancelled:\n            break\n    except KeyboardInterrupt:\n        print(\"Ctrl+C\")\n        action_client.stop = True\n</code></pre> <p>The <code>while</code> loop will execute until the action completes successfully, or until the goal is cancelled, or we shutdown the node with a Ctrl+C interrupt.</p> <p>Look back through the node to see how all this will flow through your class.</p> </li> <li> <p>We may wish to cancel a goal conditionally if - say - too much time has elapsed since the call was made, or the caller has been made aware of something else that has happened in the meantime (perhaps we're running out of storage space on the robot and can't save any more images!). </p> <p>For the purposes of this exercise, we want to modify our node so that the action is always cancelled after a total of 5 images have been captured. This can be done by making a fairly small modification to the <code>feedback_callback()</code>. Have a go at implementing this now. </p> <ul> <li>Have a go now at introducing a conditional call to the <code>cancel_goal()</code> method once a total of 5 images have been captured.</li> <li>You could use the <code>captured_images</code> attribute from the <code>CameraSweepFeedback</code> message to trigger this.</li> </ul> </li> </ol>"},{"location":"course/assignment1/part5/#a-summary-of-ros-actions","title":"A Summary of ROS Actions","text":"<p>ROS Actions work a lot like ROS Services, but they have the following key differences:</p> <ol> <li>They can be cancelled: If something is taking too long, or if something else has happened, then an Action Client can cancel an Action whenever it needs to.</li> <li>They provide feedback: so that a client can monitor what is happening and act accordingly (i.e. cancel the action, if necessary).</li> </ol> <p>This mechanism is therefore useful for operations that may take a long time to execute, and where intervention might be necessary.</p>"},{"location":"course/assignment1/part5/#creating-your-own-action-servers-clients-and-interfaces","title":"Creating Your Own Action Servers, Clients and Interfaces","text":"<p>Important</p> <p>Cancel all active processes that you may have running before moving on.</p> <p>So far we have looked at how to call a pre-existing action server, but what about if we actually want to set up our own, and use our own custom Action Interfaces too? </p> <p>To start with, have a look at the Action Server that you've been working with in the previous exercises. You've been launching this with the following command:</p> <pre><code>ros2 run tuos_examples camera_sweep_action_server.py\n</code></pre> <p>Questions</p> <ul> <li>Which package does the action server node belong to?</li> <li>Where (in that package directory) is this node likely to be located?</li> </ul> <p>Once you've identified the name and the location of the source code, open it up in VS Code and have a look through it to see how it all works. Don't worry too much about all the content associated with obtaining and manipulating camera images in there, we'll learn more about this in the next part of this course. Instead, focus on the general overall structure of the code and the way that the action server is implemented.</p> <p>Some things to review:</p> <ol> <li> <p>The way the server is initialised and the numerous callbacks that are attached to it:</p> <pre><code>self.actionserver = ActionServer(\n    node=self, \n    action_type=CameraSweep,\n    action_name=\"camera_sweep\",\n    execute_callback=self.server_execution_callback, # (1)!\n    callback_group=ReentrantCallbackGroup(), # (2)!\n    goal_callback=self.goal_callback, # (3)!\n    cancel_callback=self.cancel_callback # (4)!\n)\n</code></pre> <ol> <li>This callback contains all the code that will be executed by the server once a valid goal is sent to it (i.e. the core functionality of the Action)</li> <li>This server node is set up as a Multi-threaded Executor (see the setup in <code>main()</code>), to control the execution of the various callbacks that we need. Here, we're assigning the Action Server to a Reentrant Callback Group, allowing all its callbacks to run in parallel, as well as other subscriber callbacks too. </li> <li>This callback is used to check the goal parameters that have been sent to the server, to decide whether to accept or reject a request. </li> <li>This callback contains everything that needs to happen in the event that the Action is cancelled part-way through.</li> </ol> </li> <li> <p>Take a look at the various Action callbacks to see what's happening in each:</p> <ol> <li>How are goal parameters checked, and subsequently accepted or rejected?</li> <li>How are cancellations implemented, and how is this monitored in the main <code>server_execution_callback()</code>?</li> <li>How is feedback handled and published?</li> <li>How is the result handled and published too?</li> </ol> </li> <li> <p>Finally, consider the shutdown operations.</p> </li> </ol>"},{"location":"course/assignment1/part5/#explore","title":"Implementing an \"Exploration\" strategy using the Action Framework","text":"<p>An exploration strategy allows a robot to autonomously navigate an unknown environment while simultaneously avoiding crashing into things! One way to achieve this is to utilise two distinct motion states: moving forwards and turning on the spot, and repeatedly switching between them. Brownian Motion and Levy Flight are examples of this kind of approach. Randomising the time spent in either, or both, of these two states will result in a navigation strategy that allows a robot to slowly and randomly explore an environment. Forward motion could be performed until - say - a certain distance has been travelled, a set time has elapsed or something gets in the way. Likewise, the direction, speed and/or duration of turning could also be randomised to achieve this.</p> <p>Over the next few exercises we'll construct our own Action Interface, Server and Client nodes with the aim of creating the basis for a basic exploration behaviour. Having completed these exercises you'll have something that - with further development - could be turned into an exploration type behaviour such as the above.</p>"},{"location":"course/assignment1/part5/#ex3","title":"Exercise 3: Creating an Action Interface","text":"<p>In Exercise 2 we created the <code>part5_actions</code> package. Inside this package we will now define an Action Interface to be used by the subsequent Action Server and Client that we will create in the later exercises. </p> <ol> <li> <p>Action interfaces must be defined within an <code>action</code> folder at the root of the package directory, so let's create this now in TERMINAL 1:</p> <ol> <li> <p>First, navigate into your package:</p> <pre><code>cd ~/ros2_ws/src/part5_actions\n</code></pre> </li> <li> <p>Then use <code>mkdir</code> to make a new directory:</p> <pre><code>mkdir action\n</code></pre> </li> </ol> </li> <li> <p>In here we'll now define an Action Interface called <code>ExploreForward</code>:</p> <pre><code>touch action/ExploreForward.action\n</code></pre> </li> <li> <p>Open this up in VS Code and define the data structure of the Interface as follows:</p> ExploreForward.action<pre><code>#goal\nfloat32 fwd_velocity               # The speed at which the robot should move forwards (m/s)\nfloat32 stopping_distance          # Minimum distance of an approaching obstacle before the robot stops (meters) \n---\n#result\nfloat32 total_distance_travelled   # Total distance travelled during the action (meters)\nfloat32 closest_obstacle           # LaserScan distance to the closest detected obstacle up ahead (meters)\n---\n#feedback\nfloat32 current_distance_travelled # Distance travelled so far during the action (meters)\n</code></pre> <p>This interface therefore has two goal parameters, one feedback parameter, and two result parameters:</p> <p>Goal:</p> <ol> <li><code>fwd_velocity</code>: The speed (in m/s) at which the robot should move forwards when the action server is called. </li> <li><code>stopping_distance</code>: The distance (in meters) at which the robot should stop ahead of any objects or boundary walls that are in front of it (this data will come from <code>LaserScan</code> data from the robot's LiDAR sensor).</li> </ol> <p>Feedback:</p> <ol> <li><code>current_distance_travelled</code>: The distance travelled (in meters) since the current action was initiated (based on <code>Odometry</code> data).</li> </ol> <p>Result:</p> <ol> <li><code>total_distance_travelled</code>: The total distance travelled (in meters) over the course of the action (based on <code>Odometry</code> data).</li> <li><code>closest_obstacle</code>: The distance to the closest obstacle up ahead of the robot when the action completes.</li> </ol> </li> <li> <p>Now, declare this interface in the package's <code>CMakeLists.txt</code> file, so that the necessary Python code can be created:</p> CMakeLists.txt<pre><code>find_package(rosidl_default_generators REQUIRED)\nrosidl_generate_interfaces(${PROJECT_NAME}\n  \"action/ExploreForward.action\" \n)\n</code></pre> </li> <li> <p>Also modify the <code>package.xml</code> file (above the <code>&lt;export&gt;</code> line) to include the necessary <code>rosidl</code> dependencies:</p> package.xml<pre><code>&lt;buildtool_depend&gt;rosidl_default_generators&lt;/buildtool_depend&gt;\n&lt;exec_depend&gt;rosidl_default_runtime&lt;/exec_depend&gt;\n&lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;\n</code></pre> <p>Also, add the following <code>msg</code> dependencies too (below the <code>&lt;exec_depend&gt;action_msgs&lt;/exec_depend&gt;</code> line):</p> package.xml<pre><code>&lt;exec_depend&gt;geometry_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;nav_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;sensor_msgs&lt;/exec_depend&gt;\n</code></pre> </li> <li> <p>Now run <code>colcon</code> to generate the necessary source code for this new interface: </p> <ol> <li> <p>First, always make sure you're in the root of the Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Then run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part5_actions --symlink-install \n</code></pre> </li> <li> <p>And finally re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>We can now verify that it worked. </p> <ol> <li> <p>Use <code>ros2 interface</code> to list all available interface types, but use the <code>-a</code> option to display only action-type interfaces:</p> <pre><code>ros2 interface list -a\n</code></pre> <p>Look for a message with the prefix \"<code>part5_actions</code>\".</p> </li> <li> <p>Next, show the data structure of the interface:</p> <pre><code>ros2 interface show part5_actions/action/ExploreForward\n</code></pre> <p>This should match the content of the <code>part5_actions/action/ExploreForward.action</code> file that we created above.</p> </li> </ol> </li> </ol>"},{"location":"course/assignment1/part5/#ex4","title":"Exercise 4: Building the \"ExploreForward\" Action Server","text":"<ol> <li> <p>In TERMINAL 1 navigate to the <code>scripts</code> folder of your <code>part5_actions</code> package then:</p> <ol> <li>Create a Python script called <code>explore_server.py</code></li> <li>Make it executable</li> <li>Declare it as an executable in <code>CMakeLists.txt</code></li> </ol> </li> <li> <p>Open up the file in VS Code.</p> </li> <li> <p>The job of the Server node is as follows:</p> <ul> <li>The action server should make the robot move forwards until it detects an obstacle up ahead.</li> <li>It should use the <code>ExploreForward.action</code> Interface that we created in the previous exercise. </li> <li> <p>The Server will therefore need to be configured to accept two goal parameters:</p> <ol> <li>The speed (in m/s) at which the robot should move forwards when the action server is called. </li> <li> <p>The distance (in meters) at which the robot should stop ahead of any objects or boundary walls that are in front of it.</p> <p>To do this you'll need to subscribe to the <code>/scan</code> topic. Be aware that an object won't necessarily be directly in front of the robot, so you may need to monitor a range of <code>LaserScan</code> data points (within the <code>ranges</code> array) to make the collision avoidance effective (recall the LaserScan callback example from Part 3).</p> </li> </ol> </li> <li> <p>Be sure to do some error checking on the goal parameters to ensure that a valid request is made. This is done by attaching a <code>goal_callback</code> to the Action Server. </p> <ul> <li><code>fwd_velocity</code>: Should be a velocity that is moderate, and within the robot's velocity limits.</li> <li><code>stopping_distance</code>: Should be greater than the minimum detectable limit of the LiDAR Sensor, large enough to safely avoid collisions. </li> </ul> </li> <li> <p>Whilst your server performs its task it should provide the following feedback to a Client:</p> <ol> <li> <p>The distance travelled (in meters) since the current action was initiated.</p> <p>To do this you'll need to subscribe to the <code>/odom</code> topic. Remember the work that you did in Part 2 on this. </p> <p>Tips</p> <ul> <li>The robot's orientation shouldn't change over the course of a single action call, only its <code>linear.x</code> and <code>linear.y</code> positions should vary.</li> <li>Bear in mind however that the robot won't necessarily be moving along the <code>X</code> or <code>Y</code> axis, so you will need to consider the total distance travelled in the <code>X-Y</code> plane.</li> <li>We did this in the Part 2 <code>move_square</code> exercise, so refer to this if you need a reminder.</li> </ul> </li> </ol> </li> <li> <p>Finally, on completion of the action, your server should be configured to provide the following two result parameters:</p> <ol> <li>The total distance travelled (in meters) over the course of the action.</li> <li>The distance to the obstacle that made the robot stop (if the action server has done its job properly, then this should be very similar to the <code>stopping_distance</code> that was provided by the Action Client in the goal).</li> </ol> </li> </ul> <p></p> </li> <li> <p>There's some template code here to help you with this, but you might also want to take a look at the <code>camera_sweep_action_server.py</code> code from the earlier exercises to help you construct this: a lot of the techniques used here will be similar (excluding all the camera related stuff).  </p> </li> </ol> <p>Testing</p> <p>There's a good simulation environment that you can use as you're developing your action server here. When you want to test things out, launch the simulation with the following command: </p> <p>TERMINAL 1: </p><pre><code>ros2 launch turtlebot3_gazebo turtlebot3_dqn_stage4.launch.py\n</code></pre><p></p> <p></p> <p>Don't forget, that in order to launch the server, you'll need to have built everything with <code>colcon</code> by following the usual three stage process.</p> <p>Once you've done this, you'll then be able to run it:</p> <p>TERMINAL 2: </p><pre><code>ros2 run part5_actions explore_server.py\n</code></pre><p></p> <p>Don't forget that you don't need to have developed a Python Client Node in order to test the server. Use the <code>ros2 action send_goal</code> CLI tool to make calls to the server (like we did in Exercise 1).</p>"},{"location":"course/assignment1/part5/#ex5","title":"Exercise 5: Building a Basic \"ExploreForward\" Client","text":"<ol> <li> <p>In TERMINAL 3 navigate to the <code>scripts</code> folder of your <code>part5_actions</code> package, create a Python script called <code>explore_client.py</code>, make it executable, and add this to your <code>CMakeLists.txt</code>.</p> </li> <li> <p>Run <code>colcon build</code> on it now, so you don't have to worry about it later (again, following the three stage process as above)...</p> <p>Here it is again, for good measure:</p> <p>TERMINAL 3:</p> <ol> <li> <p>Step 1:</p> <pre><code>cd ~/ros2_ws\n</code></pre> </li> <li> <p>Step 2:</p> <pre><code>colcon build --packages-select part5_actions --symlink-install\n</code></pre> </li> <li> <p>Step 3:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Open up the <code>explore_client.py</code> file in VS Code.</p> </li> <li> <p>The job of the Action Client is as follows:</p> <ul> <li>The client needs to issue a correctly formatted goal to the server.</li> <li>The client should be programmed to monitor the feedback data from the Server.  If it detects (from the feedback) that the robot has travelled a distance greater than 2 meters without detecting an obstacle, then it should cancel the current action call.</li> </ul> </li> <li> <p>Use the techniques that we used in the Client node from Exercise 2 as a guide to help you with this. </p> </li> <li> <p>Once you have everything in place, launch the action client with <code>ros2 run</code> as below:</p> <p>TERMINAL 3: </p><pre><code>ros2 run part5_actions explore_client.py\n</code></pre><p></p> <p>If all is good, then this client node should call the action server, which will (in turn) make the robot move forwards until it reaches a certain distance from an obstacle up ahead, at which point the robot will stop, and your client node will stop too. Once this happens, reorient your robot (using the <code>teleop_keyboard</code> node) and launch the client node again to make sure that it is robustly stopping in front of obstacles repeatedly, and when approaching them from a range of different angles. </p> <p>Important</p> <p>Make sure that your cancellation functionality works correctly too, ensuring that:</p> <ol> <li>The robot never moves any further than 2 meters during a given action call</li> <li>An action is aborted mid-way through if the client node is shut down with Ctrl+C</li> </ol> </li> </ol>"},{"location":"course/assignment1/part5/#ex6","title":"Exercise 6 (Advanced): Implementing an Exploration Strategy","text":"<p>Up to now, your Action Client node should have the capability to call the <code>ExploreForward.action</code> server to make the robot move forwards by 2 meters, or until it reaches an obstacle (whichever occurs first), but you could build on this now and turn it into a full exploration behaviour:</p> <ul> <li>Between action calls, your client node could make the robot turn on the spot to face a different direction and then issue a further action call to make the robot move forwards once again.</li> <li>The turning process could be done at random (ideally), or by a fixed amount every time.</li> <li>By programming your client node to repeat this process over and over again, the robot would (somewhat randomly) travel around its environment safely, stopping before it crashes into any obstacles and reorienting itself every time it stops moving forwards. </li> </ul>"},{"location":"course/assignment1/part5/#wrapping-up","title":"Wrapping Up","text":"<p>In Part 5 of this course you've learnt:</p> <ul> <li>How ROS Actions work and why they might be useful.</li> <li>How to develop Action Client Nodes in Python which can monitor the action in real-time (via feedback), and which can also cancel the requested action, if required.</li> <li>How to use standard ROS tools to interrogate action interfaces, thus allowing us to build clients to call them</li> <li>How to build custom Action servers, clients and interfaces.</li> <li>How to harness this framework to implement an exploration strategy. </li> </ul>"},{"location":"course/assignment1/part5/#topics-services-or-actions-which-to-choose","title":"Topics, Services or Actions: Which to Choose?","text":"<p>You should now have developed a good understanding of the three communication methods that are available within ROS to facilitate communication between ROS Nodes:</p> <ol> <li>Topic-based messaging.</li> <li>ROS Services.</li> <li>ROS Actions.</li> </ol> <p>Through this course you've gained some practical experience using all three of these, but you may still be wondering how to select the appropriate one for a certain robot task... </p> <p>This ROS.org webpage summarises all of this very nicely (and briefly), so you should have a read through this to make sure you know what's what. In summary though:</p> <ul> <li>Topics: Are most appropriate for broadcasting continuous data-streams such as sensor data and robot state information, and for publishing data that is likely to be required by a range of Nodes across a ROS network.</li> <li>Services: Are most appropriate for very short procedures like quick calculations (inverse kinematics etc.) and performing short discrete actions that are unlikely to go wrong or will not need intervention (e.g. turning on a warning LED when a battery is low).</li> <li>Actions: Are most appropriate for longer running tasks (like moving a robot), or for operations where we might need to change our mind and do something different or cancel an invoked behaviour part way through.</li> </ul>"},{"location":"course/assignment1/part5/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, to save the work you have done in WSL-ROS2 during this session so that you can restore it on a different machine at a later date. Run the following script in any idle WSL-ROS2 Terminal Instance now:</p> <pre><code>wsl_ros backup\n</code></pre> <p>You'll then be able to restore it to a fresh WSL-ROS2 environment next time you fire one up (<code>wsl_ros restore</code>).  </p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment1/part6/","title":"Part 6: Cameras, Machine Vision & OpenCV","text":""},{"location":"course/assignment1/part6/#introduction","title":"Introduction","text":"<p> Exercises: 4 Estimated Completion Time: 2 hours Difficulty Level: Advanced </p>"},{"location":"course/assignment1/part6/#aims","title":"Aims","text":"<p>In this part of the course we'll make use of the Waffle's camera, and look at how to work with images in ROS. Here we'll look at how to build ROS nodes that capture images and process them. We'll explore some ways in which this data can be used to inform decision-making in robotic applications.  </p>"},{"location":"course/assignment1/part6/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Use a range of ROS tools to interrogate camera image topics on a ROS Network and view the images being streamed to them.</li> <li>Use the computer vision library OpenCV with ROS, to obtain camera images and process them in real-time.  </li> <li>Apply filtering processes to isolate objects of interest within an image.</li> <li>Develop object detection nodes and harness the information generated by these processes to control a robot's position.</li> <li>Use camera data as a feedback signal to implement a line following behaviour using proportional control.</li> </ol>"},{"location":"course/assignment1/part6/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Using the <code>rqt_image_view</code> node whilst changing the robot's viewpoint</li> <li>Exercise 2: Object Detection</li> <li>Exercise 3: Using Image Moments for Robot Control</li> <li>Exercise 4: Line following</li> </ul>"},{"location":"course/assignment1/part6/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Initial Object Detection Code (for Exercise 2)</li> <li>A Complete Worked Example of the <code>object_detection.py</code> Node</li> <li>The <code>line_follower</code> Template (for Exercise 4)</li> </ul>"},{"location":"course/assignment1/part6/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>Launch your ROS environment now so that you have access to a Linux terminal instance (aka TERMINAL 1).</p> <p>Step 2: Restore your work (WSL-ROS2 Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS2 Environment will not be preserved between sessions or across different University computers, and so you should be backing up your work to your <code>U:\\</code> drive regularly. When prompted (on first launch of WSL-ROS2 in TERMINAL 1) enter <code>Y</code> to restore this<sup>1</sup>.</p> <p>Step 3: Launch VS Code </p> <p>WSL users remember to check for this.</p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Check that the Course Repo is up-to-date before you start on these exercises. See here for how to update. </p> <p>Step 5: Launch the Robot Simulation</p> <p>In this session we'll start by working with the same mystery world environment from Part 5. In TERMINAL 1, use the following command to load it:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch tuos_simulations coloured_pillars.launch.py\n</code></pre> ...and then wait for the Gazebo window to open:<p></p> <p></p>"},{"location":"course/assignment1/part6/#working-with-cameras-and-images-in-ros","title":"Working with Cameras and Images in ROS","text":""},{"location":"course/assignment1/part6/#camera-topics-and-data","title":"Camera Topics and Data","text":"<p>There are a number of tools that we can use to view the live images that are being captured by a robot's camera in ROS. As with all robot data, these streams are published to topics, so we firstly need to identify those topics.</p> <p>In a new terminal instance (TERMINAL 2), run <code>ros2 topic list</code> to see the full list of topics that are currently active on our system. Conveniently, all the topics related to our robot's camera are prefixed with <code>/camera</code>! Filter the <code>ros2 topic list</code> output using <code>grep</code> (a Linux command), to only show topics with this prefix:</p> <p>TERMINAL 2: </p><pre><code>ros2 topic list | grep /camera\n</code></pre><p></p> <p>This should provide the following filtered list:</p> <pre><code>/camera/camera_info\n/camera/image_raw\n/camera/image_raw/compressed\n/camera/image_raw/compressedDepth\n/camera/image_raw/theora\n</code></pre> <p>The main item that we're interested in here is the raw image data, and the key topic that we'll therefore be using here is:</p> <pre><code>/camera/image_raw\n</code></pre> <p>Run <code>ros2 topic info</code> on this to identify the interface used by this topic.</p> <p>Then, run <code>ros2 interface show</code> on the interface to learn about the data format. You should end up with an output that looks like this (simplified slightly here):</p> <pre><code># This message contains an uncompressed image\n# (0, 0) is at top-left corner of image\n\nstd_msgs/Header header # Header timestamp should be acquisition time of image\n        builtin_interfaces/Time stamp\n                int32 sec\n                uint32 nanosec\n        string frame_id\n\nuint32 height                # image height, that is, number of rows\nuint32 width                 # image width, that is, number of columns\n\nstring encoding       # Encoding of pixels -- channel meaning, ordering, size\n                      # taken from the list of strings in include/sensor_msgs/image_encodings.hpp\n\nuint8 is_bigendian    # is this data bigendian?\nuint32 step           # Full row length in bytes\nuint8[] data          # actual matrix data, size is (step * rows)\n</code></pre> <p></p> <p>Questions</p> <ol> <li>What type of interface is used by this topic, and which package is this derived from?</li> <li>Using <code>ros2 topic echo</code> and the information about the topic message (as shown above) determine the size of the images that our robot's camera will capture (i.e. its dimensions, in pixels).  It will be quite important to know this when we start manipulating these camera images later on. </li> <li>Finally, considering the list above again, which part of the message do you think contains the actual image data?</li> </ol>"},{"location":"course/assignment1/part6/#viz","title":"Visualising Camera Streams","text":"<p>We can view the images being streamed to the above camera topic (in real-time) in a variety of different ways, and we'll explore a couple of these now.</p> <p>One way is to use RViz, which can be launched using the following <code>ros2 launch</code> command:</p> <p>TERMINAL 2: </p><pre><code>ros2 launch tuos_simulations rviz.launch.py\n</code></pre><p></p> <p>Once RViz launches, you should see a camera panel in the bottom-left corner with a live stream of the images being obtained from the robot's camera.</p> <p></p> <p>Close down RViz by entering Ctrl+C in TERMINAL 2.  </p>"},{"location":"course/assignment1/part6/#ex1","title":"Exercise 1: Using RQT whilst changing the robot's viewpoint","text":"<p>Another tool we can use to view camera data-streams is the RQT.</p> <ol> <li> <p>Enter the following command to launch it:</p> <p>TERMINAL 2: </p><pre><code>rqt\n</code></pre> <p></p> </li> <li> <p>From the top menu select <code>Plugins</code> &gt; <code>Vizualisation</code> &gt; <code>Image View</code>.</p> <p></p> <p></p> <p>This allows us to easily view images that are being published to any camera topic on the ROS network. Another useful feature is the ability to save these images (as <code>.jpg</code> files) to the filesystem: See the \"Save as image\" button highlighted in the figure above. This might be useful later on...</p> </li> <li> <p>Click the drop-down box in the top left of the window to select an image topic to display.  Select <code>/camera/image_raw</code> (if it's not already selected).</p> </li> <li> <p>Keep this window open now, and launch a new terminal instance (TERMINAL 3).</p> </li> <li> <p>Launch the <code>teleop_keyboard</code> node. Rotate the robot on the spot, keeping an eye on the RQT Image View window as you do this. Stop the robot once one of the coloured pillars in the arena is roughly in the centre of the robot's field of vision, then close the <code>teleop_keyboard</code> node and RQT Image View by entering Ctrl+C in TERMINAL 3 and TERMINAL 2 respectively.</p> </li> </ol>"},{"location":"course/assignment1/part6/#opencv","title":"OpenCV and ROS","text":"<p>OpenCV is a mature and powerful computer vision library designed for performing real-time image analysis, and it is therefore extremely useful for robotic applications.  The library is cross-platform and there is a Python API (<code>cv2</code>), which we'll be using to do some computer vision tasks of our own during this lab session. While we can work with OpenCV using Python straight away (via the API), the library can't directly interpret the native image format used by the ROS, so there is an interface that we need to use.  The interface is called CvBridge, which is a ROS package that handles the conversion between ROS and OpenCV image formats.  We'll therefore need to use these two libraries (OpenCV and CvBridge) hand-in-hand when developing ROS nodes to perform computer vision related tasks.</p>"},{"location":"course/assignment1/part6/#object-detection","title":"Object Detection","text":"<p>One common job that we often want a robot to perform is object detection, and we will illustrate how this can be achieved using OpenCV tools for colour filtering, to detect the coloured pillar that your robot should now be looking at.  </p>"},{"location":"course/assignment1/part6/#ex2","title":"Exercise 2: Object Detection","text":"<p>In this exercise you will learn how to use OpenCV to capture images, filter them and perform other analysis to confirm the presence and location of features that we might be interested in.</p>"},{"location":"course/assignment1/part6/#step-1-getting-started","title":"Step 1: Getting Started","text":"<ol> <li> <p>First create a new package called <code>part6_vision</code> using the <code>create_pkg.sh</code> helper script (from the <code>tuos_ros</code> course repo). </p> </li> <li> <p>Navigate into the <code>scripts</code> directory of this new package (using <code>cd</code>), create a new file called <code>object_detection.py</code> (using <code>touch</code>), make this executable (<code>chmod</code>) and declare this as an executable in the package's <code>CMakeLists.txt</code> (you've done all this lots of times now, but check back through previous parts of the course if you need a reminder on how to do any of it).</p> </li> <li> <p>Modify the <code>package.xml</code> file (below the <code>&lt;buildtool_depend&gt;ament_cmake_python&lt;/buildtool_depend&gt;</code> line) to include following dependencies:</p> package.xml<pre><code>&lt;depend&gt;opencv2&lt;/depend&gt;\n&lt;depend&gt;cv_bridge&lt;/depend&gt;\n&lt;depend&gt;sensor_msgs&lt;/depend&gt;\n&lt;depend&gt;geometry_msgs&lt;/depend&gt;\n</code></pre> </li> <li> <p>Build the package with <code>colcon</code>:</p> <ol> <li> <p>In TERMINAL 2, make sure you're in the root of the Workspace:</p> <p>TERMINAL 2: </p><pre><code>cd ~/ros2_ws/\n</code></pre><p></p> </li> <li> <p>Run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part6_vision --symlink-install \n</code></pre> </li> <li> <p>And finally re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Open up the <code>object_detection.py</code> Python file in VS Code.</p> </li> <li> <p>Copy the code here into the empty <code>object_detection.py</code> file, save it, then read the annotations so that you understand how the node works and what should happen when you run it. </p> </li> <li> <p>Run the node using <code>ros2 run</code>.</p> <p>Warning</p> <p>This node will capture an image and display it in a pop-up window. Once you've viewed the image in this pop-up window MAKE SURE YOU CLOSE THE POP-UP WINDOW DOWN so that execution can complete!</p> </li> <li> <p>As you should know from reading the explainer, the node has just obtained an image and saved it to a location on the filesystem.  Navigate to this filesystem location and view the image using <code>eog</code>.</p> <p>What you may have noticed from the terminal output when you ran the <code>object_detection.py</code> node is that the robot's camera captures images at a native size of 1080x1920 pixels (you should already know this from interrogating the <code>/camera/rgb/image_raw/width</code> and <code>/height</code> messages using <code>rostopic echo</code> earlier, right?!).  That's over 2 million pixels in total in a single image (2,073,600 pixels per image, to be exact), each pixel having a blue, green and red value associated with it - so that's a lot of data in a single image file! </p> <p>Question</p> <p>The size of the image file (in bytes) was actually printed to the terminal when you ran the <code>object_detection.py</code> node. Did you notice how big it was exactly?</p> </li> </ol> <p>Processing an image of this size is therefore hard work for a robot: any analysis we do will be slow and any raw images that we capture will occupy a considerable amount of storage space. The next step then is to reduce this down by cropping the image to a more manageable size.</p>"},{"location":"course/assignment1/part6/#step-2-cropping","title":"Step 2: Cropping","text":"<p>We're going to modify the <code>object_detection.py</code> node now to:</p> <ul> <li>Capture a new image in its native size</li> <li>Crop it down to focus in on a particular area of interest</li> <li>Save both of the images (the cropped one should be much smaller than the original)  </li> </ul> <p></p> <ol> <li> <p>In your <code>object_detection.py</code> node locate the line:</p> <pre><code>self.show_image(img=cv_img, img_name=\"step1_original\")\n</code></pre> </li> <li> <p>Underneath this, add the following additional lines of code:</p> <pre><code>crop_width = width - 400\ncrop_height = 400\ncrop_y0 = int((width / 2) - (crop_width / 2))\ncrop_z0 = int((height / 2) - (crop_height / 2))\ncropped_img = cv_img[crop_z0:crop_z0+crop_height, crop_y0:crop_y0+crop_width]\n\nself.show_image(img=cropped_img, img_name=\"step2_cropping\")\n</code></pre> </li> <li> <p>Run the node again.  </p> <p>Remember</p> <p>Make sure you close all of these pop-up windows down after viewing them to ensure that all your images are saved to the filesystem and the node completes all of its tasks successfully.</p> <p>The code that you have just added here has created a new image object called <code>cropped_img</code>, from a subset of the original by specifying a desired <code>crop_height</code> and <code>crop_width</code> relative to the original image dimensions.  Additionally, we have also specified where in the original image (in terms of pixel coordinates) we want this subset to start, using <code>crop_y0</code> and <code>crop_z0</code>. This process is illustrated in the figure below:</p> <p></p> <p></p> <p>The original image (<code>cv_img</code>) is cropped using a process called \"slicing\":</p> <p></p><pre><code>cropped_img = cv_img[\n    crop_z0:crop_z0+crop_height,\n    crop_y0:crop_y0+crop_width\n]\n</code></pre> This may seem quite confusing, but hopefully the figure below illustrates what's going on here:<p></p> <p></p> <p></p> </li> </ol>"},{"location":"course/assignment1/part6/#step-3-masking","title":"Step 3: Masking","text":"<p>As discussed above, an image is essentially a series of pixels each with a blue, green and red value associated with it to represent the actual image colours. From the original image that we have just obtained and cropped, we then want to get rid of any colours other than those associated with the pillar that we want the robot to detect.  We therefore need to apply a filter to the pixels, which we will ultimately use to discard any pixel data that isn't related to the coloured pillar, whilst retaining data that is.  </p> <p>This process is called masking and, to achieve this, we need to set some colour thresholds. This can be difficult to do in a standard Blue-Green-Red (BGR) or Red-Green-Blue (RGB) colour space, and you can see a good example of this in this article from RealPython.com.  We will apply some steps discussed in this article to convert our cropped image into a Hue-Saturation-Value (HSV) colour space instead, which makes the process of colour masking a bit easier.</p> <ol> <li> <p>First, analyse the Hue and Saturation values of the cropped image. To do this, first navigate to the <code>~/myrosdata/object_detection</code> directory, where the raw image has been saved:</p> <p>TERMINAL 2: </p><pre><code>cd ~/myrosdata/object_detection\n</code></pre><p></p> <p>Then, run the following ROS Node (from the <code>tuos_examples</code> package), supplying the name of the cropped image as an additional argument:</p> <pre><code>ros2 run tuos_examples image_colours.py step2_cropping.jpg\n</code></pre> </li> <li> <p>The node should produce a scatter plot, illustrating the Hue and Saturation values of each of the pixels in the image. Each data point in the plot represents a single image pixel and each is coloured to match its RGB value:</p> <p></p> <p></p> </li> <li> <p>You should see from the image that all the pixels related to the coloured pillar that we want to detect are clustered together.  We can use this information to specify a range of Hue and Saturation values that can be used to mask our image: filtering out any colours that sit outside this range and thus allowing us to isolate the pillar itself. The pixels also have a Value (or \"Brightness\"), which isn't shown in this plot. As a rule of thumb, a range of brightness values between 100 and 255 generally works quite well.</p> <p></p> <p></p> <p>In this case then, we select upper and lower HSV thresholds as follows:</p> <pre><code>lower_threshold = (115, 225, 100)\nupper_threshold = (130, 255, 255)\n</code></pre> <p>Use the plot that you have generated yourself to determine your own upper and lower thresholds. </p> <p>OpenCV contains a built-in function to detect which pixels of an image fall within a specified HSV range: <code>cv2.inRange()</code>.  This outputs a matrix, the same size and shape as the number of pixels in the image, but containing only <code>True</code> (<code>1</code>) or <code>False</code> (<code>0</code>) values, illustrating which pixels do have a value within the specified range and which don't.  This is known as a Boolean Mask (essentially, a series of ones or zeroes).  We can then apply this mask to the image, using a Bitwise AND operation, to get rid of any image pixels whose mask value is <code>False</code> and keep any flagged as <code>True</code> (or in range).</p> </li> <li> <p>To do this, first locate the following line in your <code>object_detection.py</code> node:</p> <pre><code>self.show_image(img=cropped_img, img_name=\"step2_cropping\")\n</code></pre> </li> <li> <p>Underneath this, add the following:</p> <pre><code>hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\nlower_threshold = (115, 225, 100)\nupper_threshold = (130, 255, 255)\nimg_mask = cv2.inRange(hsv_img, lower_threshold, upper_threshold)\n\nself.show_image(img=img_mask, img_name=\"step3_image_mask\")\n</code></pre> </li> <li> <p>Now, run the node again. A third image should also be generated now. </p> <p>As shown in the figure below, the third image should simply be a black and white representation of the cropped image, where the white regions should indicate the areas of the image where pixel values fall within the HSV range specified earlier. </p> <p>Notice (from the text printed to the terminal) that the cropped image and the image mask have the same dimensions, but the image mask file has a significantly smaller file size.  While the mask contains the same number of pixels, these pixels only have a value of <code>1</code> or <code>0</code>, whereas - in the cropped image of the same pixel size - each pixel has a Red, Green and Blue value: each ranging between <code>0</code> and <code>255</code>, which represents significantly more data.</p> <p></p> <p></p> </li> </ol>"},{"location":"course/assignment1/part6/#bitwise_and","title":"Step 4: Filtering","text":"<p>Finally, we can apply this mask to the cropped image, generating a final version of it where only pixels marked as <code>True</code> in the mask retain their RGB values, and the rest are simply removed.  As discussed earlier, we use a Bitwise AND operation to do this and, once again, OpenCV has a built-in function to do this: <code>cv2.bitwise_and()</code>.</p> <ol> <li> <p>Locate the following line in your <code>object_detection.py</code> node:</p> <pre><code>self.show_image(img=img_mask, img_name=\"step3_image_mask\")\n</code></pre> </li> <li> <p>And, underneath this, add the following:</p> <pre><code>filtered_img = cv2.bitwise_and(cropped_img, cropped_img, mask = img_mask)\n\nself.show_image(img=filtered_img, img_name=\"step4_filtered_image\")\n</code></pre> </li> <li> <p>Run this node again, and a fourth image should also be generated now, this time showing the cropped image taken from the robot's camera, but only containing data related to coloured pillar, with all other background image data removed (and rendered black):</p> <p></p> <p></p> </li> </ol>"},{"location":"course/assignment1/part6/#image-moments","title":"Image Moments","text":"<p>You have now successfully isolated an object of interest within your robot's field of vision, but perhaps we want to make our robot move towards it, or - conversely - make our robot navigate around it and avoid crashing into it!  We therefore also need to know the position of the object in relation to the robot's viewpoint, and we can do this using image moments.</p> <p>The work we have just done above led to us obtaining what is referred to as a colour blob.  OpenCV also has built-in tools to allow us to calculate the centroid of a colour blob like this, allowing us to determine where exactly within an image the object of interest is located (in terms of pixels).  This is done using the principle of image moments: essentially statistical parameters related to an image, telling us how a collection of pixels (i.e. the blob of colour that we have just isolated) are distributed within it.  You can read more about Image Moments here, which tells us that the central coordinates of a colour blob can be obtained by considering some key moments of the image mask that we obtained from thresholding earlier:</p> <ul> <li>\\(M_{00}\\): the sum of all non-zero pixels in the image mask (i.e. the size of the colour blob, in pixels)</li> <li>\\(M_{10}\\): the sum of all the non-zero pixels in the horizontal (y) axis, weighted by row number</li> <li>\\(M_{01}\\): the sum of all the non-zero pixels in the vertical (z) axis, weighted by column number</li> </ul> <p>Remember</p> <p>We refer to the horizontal as the y-axis and the vertical as the z-axis here, to match the terminology that we have used previously to define our robot's principal axes.</p> <p>We don't really need to worry about the derivation of these moments too much though.  OpenCV has a built-in <code>moments()</code> function that we can use to obtain this information from an image mask (such as the one that we generated earlier):</p> <pre><code>m = cv2.moments(img_mask)\n</code></pre> <p>So, using this we can obtain the <code>y</code> and <code>z</code> coordinates of the blob centroid quite simply:</p> <pre><code>cy = m['m10']/(m['m00']+1e-5)\ncz = m['m01']/(m['m00']+1e-5) \n</code></pre> <p>Question</p> <p>We're adding a very small number to the \\(M_{00}\\) moment here to make sure that the divisor in the above equations is never zero and thus ensuring that we never get caught out by any \"divide-by-zero\" errors. Why might this be necessary?</p> <p></p> <p>Once again, there is a built-in OpenCV tool that we can use to add a circle onto an image to illustrate the centroid location within the robot's viewpoint: <code>cv2.circle()</code>.  This is how we produced the red circle that you can see in the figure above.  You can see how this is implemented in a complete worked example of the <code>object_detection.py</code> node from the previous exercise. </p> <p>In our case, we can't actually change the position of our robot in the z axis, so the <code>cz</code> centroid component here might not be that important to us for navigation purposes.  We may however want to use the centroid coordinate <code>cy</code> to understand where a feature is located horizontally in our robot's field of vision, and use this information to turn towards it (or away from it, depending on what we are trying to achieve).  We can then use this as the basis for some real closed-loop control.</p>"},{"location":"course/assignment1/part6/#ex3","title":"Exercise 3: Using Image Moments for Robot Control","text":"<p>Inside the <code>tuos_examples</code> package there is a node that has been developed to illustrate how all the OpenCV tools that you have explored so far could be used to search an environment and stop a robot when it is looking directly at an object of interest. All the tools that are used in this node should be familiar to you by now, and in this exercise you're going to make a copy of this node and modify it to enhance its functionality.</p> <ol> <li> <p>The node is called <code>colour_search.py</code>, and it is located in the <code>scripts</code> folder of the <code>tuos_examples</code> package. Copy this into the <code>scripts</code> folder of your own <code>part6_vision</code> package by first ensuring that you are located in the desired destination folder:</p> <p>TERMINAL 2: </p><pre><code>cd ~/ros2_ws/src/part6_vision/scripts\n</code></pre><p></p> </li> <li> <p>Then, copy the <code>colour_search.py</code> node using <code>cp</code> as follows:</p> <p>TERMINAL 2: </p><pre><code>cp ~/ros2_ws/src/tuos_ros/tuos_examples/scripts/colour_search.py ./\n</code></pre><p></p> </li> <li> <p>Open up the <code>colour_search.py</code> file in VS Code to view the content.  Have a look through it and see if you can make sense of how it works.  The overall structure should be fairly familiar to you by now: we have a Python class structure, a Subscriber with a callback function, a timer with a callback containing all the robot control code, and a lot of the OpenCV tools that you have explored so far in this part of the course.  Essentially this node functions as follows:</p> <ol> <li>The robot turns on the spot whilst obtaining images from its camera (by subscribing to the <code>/camera/image_raw</code> topic).</li> <li>Camera images are obtained, cropped, then a threshold is applied to the cropped images to detect the blue pillar in the simulated environment.</li> <li>If the robot can't see a blue pillar then it turns on the spot quickly.</li> <li>Once detected, the centroid of the blue blob representing the pillar is calculated to obtain its current location in the robot's viewpoint.</li> <li>As soon as the blue pillar comes into view the robot starts to turn more slowly instead.</li> <li>The robot stops turning as soon as it determines that the pillar is situated directly in front of it (determined using the <code>cy</code> component of the blue blob centroid).</li> <li>The robot then waits for a while and then starts to turn again.</li> <li>The whole process repeats until it finds the blue pillar once again.</li> </ol> </li> <li>Run the node as it is to see this in action.  Observe the log messages as they are printed to the terminal throughout execution.</li> <li>Your task is to then modify the node so that it stops in front of every coloured pillar in the arena (there are four in total, each of a different colour, as you know). For this, you may need to use some of the methods that you have explored in the previous exercises.<ol> <li>You might first want to use some of the methods that we used to obtain and analyse some images from the robot's camera:<ol> <li>Use the <code>teleop_keyboard</code> node to manually move the robot, making it look at every coloured pillar in the arena individually.</li> <li>Run the <code>object_detection.py</code> node that you developed in the previous exercise to capture an image, crop it, save it to the filesystem and then feed this cropped image into the <code>image_colours.py</code> node from the <code>tuos_examples</code> package (as you did earlier)</li> <li>From the plot that is generated by the <code>image_colours.py</code> node, determine some appropriate HSV thresholds to apply for each coloured pillar in the arena.</li> </ol> </li> <li>Once you have the right thresholds, then you can add these to your <code>colour_search.py</code> node so that it has the ability to detect every pillar in the same way that it currently detects the blue one.</li> </ol> </li> </ol>"},{"location":"course/assignment1/part6/#pid","title":"PID Control and Line Following","text":"<p>Line following is a handy skill for a robot to have! We can achieve this on our TurtleBot3 using its camera system and the image processing techniques that have been covered so far in this session.</p> <p>COM2009 Lecture 6 introduces a well established algorithm for closed-loop control known as PID Control, and this can be used to achieve such line following behaviour.</p> <p>At the heart of this is the principle of Negative-Feedback control, which considers a Reference Input, a Feedback Signal and the Error between these.</p> <p></p> <p> </p>      Negative-Feedback Control     Adapted from Arturo Urquizo, CC BY-SA 3.0, via Wikimedia Commons    <p>The Reference Input represents a desired state that we would like our system to maintain. If we want our TurtleBot3 to successfully follow a coloured line on the floor, we will need it to keep the colour blob that represents that coloured line in the centre of its view point at all times. The desired state would therefore be to maintain the <code>cy</code> centroid of the colour blob in the centre of its vision.</p> <p>A Feedback Signal informs us of what the current state of the system actually is. In our case, this feedback signal would be the real-time location of the coloured line in the live camera images, i.e. its <code>cy</code> centroid (obtained using processing methods such as those covered in Exercise 3 above). </p> <p>The difference between these two things is the Error, and the PID control algorithm provides us with a means to control this error and minimise it, so that our robot's actual state matches the desired state. i.e.: the coloured line is always in the centre of its viewpoint.</p> <p></p> <p></p> <p></p> <p>The PID algorithm is as follows:</p> \\[ u(t)=K_{P} e(t) + K_{I}\\int e(t)dt + K_{D}\\dfrac{d}{dt}e(t) \\] <p>Where \\(u(t)\\) is the Controlled Output, \\(e(t)\\) is the Error (as illustrated in the figure above) and \\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\) are Proportional, Integral and Differential Gains respectively. These three gains are constants that must be established for any given system through a process called tuning. This tuning process is discussed in COM2009 Lecture 6, but you will also explore this in the practical exercise that follows.</p>"},{"location":"course/assignment1/part6/#ex4","title":"Exercise 4: Line Following","text":""},{"location":"course/assignment1/part6/#ex4a","title":"Part A: Setup","text":"<ol> <li>Make sure that all ROS processes from the previous exercise are shut down now, including the <code>colour_search.py</code> node, and the Gazebo simulation in TERMINAL 1.</li> <li> <p>In TERMINAL 1 launch a new simulation from the <code>tuos_simulations</code> package:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch tuos_simulations line_following_setup.launch.py\n</code></pre><p></p> <p>Your robot should be launched onto a long thin track with a straight pink line painted down the middle of the floor:</p> <p></p> <p></p> </li> <li> <p>In TERMINAL 2 you should still be located in your <code>part6_vision/scripts</code> directory, but if not then go there now:</p> <p>TERMINAL 2: </p><pre><code>cd ~/ros2_ws/src/part6_vision/scripts\n</code></pre><p></p> </li> <li> <p>Perform the necessary steps to create a new empty Python file called <code>line_follower.py</code> and prepare it for execution as a node within your package.</p> </li> <li> <p>Once that's done open up the empty file in VS Code.</p> <p></p> </li> <li> <p>Start with the code template provided here. </p> <p>This template contains three \"TODOs\" that you need to complete, all of which are explained in detail in the code annotations, so read these carefully. Ultimately, you did all of this in Exercise 2, so go back here if you need a reminder on how any of this works. </p> <p>Your aim here is to get the code to generate a cropped image, with the coloured line isolated and located within it, like this:</p> <p></p> <p></p> </li> </ol>"},{"location":"course/assignment1/part6/#ex4b","title":"Part B: Implementing and Tuning a Proportional Controller","text":"<p>Referring back to the equation for the PID algorithm as discussed above, the Proportional, Integral and Differential components all have different effects on a system in terms of its ability to maintain the desired state (the reference input). The gain terms associated with each of these components (\\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\)) must be tuned appropriately for any given system in order to achieve stability of control.</p> <p>A PID Controller can actually take three different forms:</p> <ol> <li>\"P\" Control: Only a Proportional gain (\\(K_{P}\\)) is used, all other gains are set to zero.</li> <li>\"PI\" Control: Proportional and Integral gains (\\(K_{P}\\) and \\(K_{I}\\)) are applied, the Differential gain is set to zero. </li> <li>\"PID\" Control: The controller makes use of all three gain terms (\\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\))</li> </ol> <p>In order to allow our TurtleBot3 to follow a line, we actually only really need a \"P\" Controller, so our control equation becomes quite simple, reducing to:</p> \\[ u(t)=K_{P} e(t) \\] <p>The next task then is to adapt our <code>line_follower.py</code> node to implement this control algorithm and find a proportional gain that is appropriate for our system.</p> <ol> <li> <p>Return to your <code>line_follower.py</code> file. Underneath the line that reads:</p> <pre><code>cv2.waitKey(1)\n</code></pre> <p>Paste the following additional code:</p> <pre><code>kp = 0.01\nreference_input = ?\nfeedback_signal = cy\nerror = feedback_signal - reference_input \n\nang_vel = kp * error\nself.get_logger().info(\n    f\"Error = {error:.1f} pixels | Control Signal = {ang_vel:.2f} rad/s\"\n)\n</code></pre> <p></p> <p>Fill in the Blank!</p> <p>What is the Reference Input to the control system (<code>reference_input</code>)? Refer to this figure from earlier. </p> <p>Here we have implemented our \"P\" Controller. The Control Signal that is being calculated here is the angular velocity that will be applied to our robot (the code won't make the robot move just yet, but we'll get to that bit shortly!) The Controlled Output will therefore be the angular position (i.e. the yaw) of the robot.  </p> </li> <li> <p>Run the code as it is, and consider the following:</p> <ol> <li>What proportional gain (\\(K_{P}\\)) are we applying?</li> <li>What is the maximum angular velocity that can be applied to our robot? Is the angular velocity that has been calculated actually appropriate?</li> <li>Is the angular velocity that has been calculated positive or negative? Will this make the robot turn in the right direction and move towards the line?  </li> </ol> </li> <li> <p>Let's address the third question (c) first...</p> <p>A positive angular velocity should make the robot turn anti-clockwise (i.e. to the left), and a negative angular velocity should make the robot turn clockwise (to the right). The line should currently be to the left of the robot, which means a positive angular velocity would be required in order to make the robot turn towards it. If the value of the Control Signal that is being calculated by our proportional controller (as printed to the terminal) is negative, then this isn't correct, so we need to change the sign of our proportional gain (\\(K_{P}\\)) in order to correct this:</p> <pre><code>kp = -0.01\n</code></pre> </li> <li> <p>Next, let's address the second of the above questions (b)...</p> <p>The maximum angular velocity that can be applied to our robot is \u00b11.82 rad/s. If our proportional controller is calculating a value for the Control Signal that is greater than 1.82, or less than -1.82 then this needs to be limited. In between the following two lines of code:</p> <pre><code>ang_vel = kp * error\nself.get_logger().info(\n    f\"Error = {error:.1f} pixels | Control Signal = {ang_vel:.2f} rad/s\"\n)\n</code></pre> <p>Insert the following: </p><pre><code>if ang_vel &lt; -1.82:\n    ang_vel = -1.82\nelif ang_vel &gt; 1.82:\n    ang_vel = 1.82\n</code></pre><p></p> </li> <li> <p>Finally, we need to think about the actual proportional gain that is being applied. This is where we need to actually tune our system by finding a proportional gain value that controls our system appropriately.</p> <p>Return to your <code>line_follower.py</code> file. Underneath the line that reads:</p> <pre><code>self.get_logger().info(\n    f\"Error = {error:.1f} pixels | Control Signal = {ang_vel:.2f} rad/s\"\n)\n</code></pre> <p>Paste the following:</p> <pre><code>self.vel_cmd.linear.x = 0.1\nself.vel_cmd.angular.z = ang_vel\nself.vel_pub.publish(self.vel_cmd)\n</code></pre> <p>The code should now make the robot move with a constant linear velocity of 0.1 m/s at all times, while its angular velocity will be determined by our proportional controller, based on the controller error and the proportional gain parameter <code>kp</code>.</p> <p>The figure below illustrates the effects different values of proportional gain can have on a system.</p> <p></p>      Courtesy of Prof. Roger Moore     Taken from COM2009 Lecture 6: PID Control    <p></p> <p>Run the code and see what happens. You should find that the robot behaves quite erratically, indicating that <code>kp</code> (at an absolute value of 0.01) is probably too large.</p> </li> <li> <p>Try reducing <code>kp</code> by a factor of 100: <code>kp = -0.0001</code>. Before you run the code again, you can reset the Gazebo simulation by pressing Ctrl+Shift+R so that the robot returns to the starting position.</p> <p>You should find that the robot now gradually approaches the line, but it can take a while for it to do so.</p> </li> <li> <p>Next, increase <code>kp</code> by a factor of 10: <code>kp = -0.001</code>. Once again, reset the robot back to its starting position in Gazebo by using Ctrl+Shift+R to reset the simulation.</p> <p>The robot should now reach the line much quicker, and follow the line well once it reaches it.</p> </li> <li> <p>Could <code>kp</code> be modified any more to improve the control further? Play around a bit more and see what happens. We'll but this to the test on a more challenging track in the next part of this exercise.</p> </li> </ol>"},{"location":"course/assignment1/part6/#ex4c","title":"Part C: Advanced Line Following","text":"<ol> <li> <p>Now, in TERMINAL 1 run a new simulation:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch tuos_simulations line_following.launch.py\n</code></pre><p></p> <p>Your robot should be launched into an environment with a more interesting line to follow:</p> <p></p> <p></p> </li> <li> <p>In TERMINAL 2, run your <code>line_follower.py</code> node and see how it performs. Does your proportional gain need to be adjusted further to optimise the performance?</p> </li> <li> <p>Next, think about conditions where the line can't initially be seen...</p> <p>As you know, the angular velocity is determined by considering the <code>cy</code> component of a colour blob representing the line. What happens in situations where the blob of colour isn't there though?  What influence would this have on the Control Signals that are calculated by the proportional controller? To consider this further, try launching the robot in the same arena but in a different location instead, and think about how you might approach this situation:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch tuos_simulations line_following.launch.py x_pos:=3 y_pos:=-3 yaw:=0\n</code></pre><p></p> </li> <li> <p>Finally, what happens when the robot reaches the finish line? How could you add additional functionality to ensure that the robot stops when it reaches this point? What features of the arena could you use to trigger this?</p> </li> </ol>"},{"location":"course/assignment1/part6/#wrapping-up","title":"Wrapping Up","text":"<p>In this session you have learnt how to use data from a robot's camera to extract further information about its environment.  The camera allows our robot to \"see\" and the information that we obtain from this device can allow us to develop more advanced robotic behaviours such as searching for objects, follow things or - conversely - moving away or avoiding them.  You have learnt how to do some basic tasks with OpenCV, but this is a huge and very capable library of computer vision tools, and we encourage you to explore this further yourselves to enhance some of the basic principles that we have shown you today.</p>"},{"location":"course/assignment1/part6/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, to save the work you have done in WSL-ROS2 during this session so that you can restore it on a different machine at a later date. Run the following script in any idle WSL-ROS2 Terminal Instance now:</p> <pre><code>wsl_ros backup\n</code></pre> <p>You'll then be able to restore it to a fresh WSL-ROS2 environment whenever you need it again (<code>wsl_ros restore</code>).  </p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment1/part1/publisher/","title":"A Simple Publisher Node","text":""},{"location":"course/assignment1/part1/publisher/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>publisher.py</code> file and review the annotations to understand how it all works.</p> <p>Tip</p> <p>Don't forget the Shebang! See below for further details...</p> publisher.py<pre><code>#!/usr/bin/env python3\n# A simple ROS2 Publisher\n\nimport rclpy # (1)!\nfrom rclpy.node import Node\n\nfrom example_interfaces.msg import String # (2)!\n\nclass SimplePublisher(Node): # (3)!\n\n    def __init__(self):\n        super().__init__(\"simple_publisher\") # (4)!\n\n        self.my_publisher = self.create_publisher(\n            msg_type=String,\n            topic=\"my_topic\",\n            qos_profile=10,\n        ) # (5)!\n\n        publish_rate = 1 # Hz\n        self.timer = self.create_timer(\n            timer_period_sec=1/publish_rate, \n            callback=self.timer_callback\n        ) # (6)!\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\" # (7)!\n        )\n\n    def timer_callback(self): # (8)!\n        ros_time = self.get_clock().now().seconds_nanoseconds()\n\n        topic_msg = String()\n        topic_msg.data = f\"The ROS time is {ros_time[0]} (seconds).\"\n        self.my_publisher.publish(topic_msg)\n        self.get_logger().info(f\"Publishing: '{topic_msg.data}'\")\n\ndef main(args=None): # (9)!\n    rclpy.init(args=args)\n    my_simple_publisher = SimplePublisher()\n    rclpy.spin(my_simple_publisher)\n    my_simple_publisher.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__': # (10)!\n    main()\n</code></pre> <ol> <li> <p><code>rclpy</code> is the ROS Client Library for Python. </p> <p>This is a vital import that allows us to create ROS nodes and initialise them on the ROS network.</p> <p>We also import the <code>Node</code> class from the <code>rclpy.node</code> library. This is a ready-made Python Class that contains all the necessary functionality that a Python ROS Node might need, so we'll use this as the basis for our own node (which we'll create shortly).</p> </li> <li> <p>We also need to import the <code>String</code> message type from the <code>example_interfaces.msg</code> library for publishing our messages.</p> </li> <li> <p>We create a Python class called <code>SimplePublisher()</code>, which we'll use to encapsulate all the functionality of our node.</p> <p>The vast majority of the functionality of this node is inherited from the <code>rclpy.node</code>, <code>Node()</code> Class which we imported above. </p> </li> <li> <p>Using the <code>super()</code> method we call the <code>__init__()</code> method from the parent Node class that our <code>SimplePublisher</code> class is derived from.</p> <p>We provide a name here, which is the name that will be used to register our node on the ROS network (we can call the node anything that we want, but it's a good idea to call it something meaningful).</p> </li> <li> <p>We then use the <code>create_publisher()</code> method (inherited from the <code>Node</code> class) in order to provide our node with the ability to publish messages to a ROS Topic. When calling this we provide 3 key bits of information:</p> <ol> <li> <p><code>msg_type</code>: The type of message that we want to publish.</p> <p>In our case, a <code>String</code> message from the <code>example_interfaces.msg</code> module.</p> </li> <li> <p><code>topic</code>: The name of the topic that we want to publish these messages to.</p> <p>This could be an existing topic (in which case, we'd need to make sure we used the correct message type), or a new topic (in which case, the name can be anything we want it to be).</p> <p>In our case, we want to create a new topic on the ROS network called <code>\"my_topic\"</code>.</p> </li> <li> <p><code>qos_profile</code>: A queue size, which is a \"Quality of Service\" (QoS) setting which limits the amount of messages that are queued in a buffer. </p> <p>In our case, we're setting this to <code>10</code>, which is generally appropriate for most of the applications that we'll be working on.</p> </li> </ol> </li> <li> <p>Here, we're calling the <code>create_timer()</code> method, which we'll use to control the rate at which messages are published to our topic. Here we define 2 things:</p> <ol> <li> <p><code>timer_period_sec</code>: The rate at which we want the timer to run. This must be provided as a period, in seconds. In the line above, we have specified a publishing frequency (in Hz):</p> <p></p><code>publish_rate = 1 # Hz</code><p></p> <p>So the associated time period (in seconds) is: </p> <p></p>\\(T = \\frac{1}{f}\\)<p></p> </li> <li> <p><code>callback</code>: This is a function that will be executed every time the timer elapses at the desired rate (1 Hz). We're specifying a function called <code>timer_callback</code>, which we'll define later on in the code...</p> </li> </ol> </li> <li> <p>Finally, we use the <code>get_logger().info()</code> method to send a Log message to the terminal to inform us that the initialisation of our node is complete.</p> </li> <li> <p>Here we define the timer callback function. Anything in here will execute at the rate that we specified when we created the <code>create_timer()</code> instance before. In our case:</p> <ol> <li>Use the <code>get_clock()</code> method to get the current ROS Time.</li> <li>Instantiate a <code>String()</code> message (defined as <code>topic_msg</code>).</li> <li>Populate this message with data. In our case, a statement that includes the ROS Time, as obtained above.</li> <li>Call the <code>publish()</code> method of our <code>my_publisher</code> object, to actually publish this message to the <code>\"my_topic\"</code> topic.</li> <li>Send the message data to the terminal as a log message as well, so that we can see what it is when our Node is actually running.</li> </ol> </li> <li> <p>With the functionality of our <code>SimplePublisher</code> class now established, we define a <code>main()</code> function for the Node. This will be fairly common to most Python Nodes that we create, with the following 5 key processes:</p> <ol> <li>Initialise the <code>rclpy</code> library.</li> <li>Create an instance of our <code>SimplePublisher()</code> node.</li> <li>\"Spin\" the node to keep it alive so that any callbacks can execute as required (in our case here, just the <code>timer_callback()</code>). </li> <li>Destroy the node once termination is requested (triggered by entering Ctrl+C in the terminal).</li> <li>Shutdown the <code>rclpy</code> library.</li> </ol> </li> <li> <p>Finally, we call the <code>main()</code> function to set everything going. We do this inside an <code>if</code> statement, to ensure that our node is the main executable (i.e. it has been executed directly (via <code>ros2 run</code>), and hasn't been called by another script)</p> </li> </ol>"},{"location":"course/assignment1/part1/publisher/#defining-package-dependencies","title":"Defining Package Dependencies","text":"<p>We're importing a couple of Python libraries into our node here, which means that our package has two dependencies: <code>rclpy</code> and <code>example_interfaces</code>:</p> <pre><code>import rclpy \nfrom rclpy.node import Node\n\nfrom example_interfaces.msg import String\n</code></pre> <p>Its good practice to add these dependencies to your <code>package.xml</code> file. Locate this file (<code>ros2_ws/src/part1_pubsub/package.xml</code>), open it up and find the following line:</p> <pre><code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;\n</code></pre> <p><code>rclpy</code> is therefore already defined as an execution dependency (which means that our package needs this library in order to execute our code), but we need to add <code>example_interfaces</code> as well, so add the following additional line underneath:</p> <pre><code>&lt;exec_depend&gt;example_interfaces&lt;/exec_depend&gt;\n</code></pre> <p>Job done. Save the file and close it.</p>"},{"location":"course/assignment1/part1/publisher/#shebang","title":"The Shebang","text":"<p>The very first line of code looks like a comment, but it is actually a very crucial part of the script:</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>This is called the Shebang, and it tells the operating system which interpreter to use to execute the code. In our case here, it tells the operating system where to find the right Python interpreter that should be used to actually run the code.</p> <p> \u2190 Back to Part 1 </p>"},{"location":"course/assignment1/part1/subscriber/","title":"A Simple Subscriber Node","text":""},{"location":"course/assignment1/part1/subscriber/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>subscriber.py</code> file and (again) make sure you read the annotations to understand how it all works!</p> subscriber.py<pre><code>#!/usr/bin/env python3\n# A simple ROS2 Subscriber\n\nimport rclpy # (1)!\nfrom rclpy.node import Node\n\nfrom example_interfaces.msg import String\n\nclass SimpleSubscriber(Node): # (2)! \n\n    def __init__(self): \n        super().__init__(\"simple_subscriber\") # (3)!\n\n        self.my_subscriber = self.create_subscription(\n            msg_type=String,\n            topic=\"??\",\n            callback=self.msg_callback,\n            qos_profile=10,\n        ) # (4)!\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\"\n        ) # (5)!\n\n    def msg_callback(self, topic_message: String): # (6)!\n        # (7)!\n        self.get_logger().info(\n            f\"\\nThe '{self.get_name()}' node heard:\\n\"\n            f\"  '{topic_message.data}'\"\n        )\n\ndef main(args=None): # (8)!\n    rclpy.init(args=args)\n    my_simple_subscriber = SimpleSubscriber()\n    rclpy.spin(my_simple_subscriber)\n    my_simple_subscriber.destroy_node()\n    rclpy.shutdown() \n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>As with our publisher node, we need to import the <code>rclpy</code> client library and the <code>String</code> message type from the <code>example_interfaces.msg</code> library in order to write a Python ROS Node and use the relevant ROS messages:</p> </li> <li> <p>This time, we create a Python Class called <code>SimpleSubscriber()</code> instead, but which still inherits the <code>Node</code> class from <code>rclpy</code> as we did with the Publisher before.</p> </li> <li> <p>Once again, using the <code>super()</code> method we call the <code>__init__()</code> method from the parent Node class that our <code>SimpleSubscriber</code> class is derived from, and provide a name to use to register in on the network.</p> </li> <li> <p>We're now using the <code>create_subscription()</code> method here, which will allow this node to subscribe to messages on a ROS Topic. When calling this we provide 4 key bits of information:</p> <ol> <li> <p><code>msg_type</code>: The type of message that the topic uses (which we could obtain by running the <code>ros2 topic info</code> command).</p> <p>We know (having just created the publisher), that our topic uses <code>String</code> messages (from <code>example_interfaces</code>).</p> </li> <li> <p><code>topic</code>: The name of the topic that we want to listen (or subscribe) to.</p> <p>Fill in the Blank!</p> <p>Replace the <code>??</code> in the code above with the name of the topic that our <code>publisher.py</code> node was set up to publish to!</p> </li> <li> <p><code>callback</code>: When building a subscriber, we need a callback function, which is a function that will execute every time a new message is received from the topic.</p> <p>At this stage, we define what this callback function is called (<code>self.msg_callback</code>), and we'll actually define the function itself further down within the Class.</p> </li> <li> <p><code>qos_profile</code>: As before, a queue size to limit the amount of messages that are queued in a buffer. </p> </li> </ol> </li> <li> <p>Print a Log message to the terminal to indicate that the initialisation process has taken place.</p> </li> <li> <p>Here we're defining what will happen each time our subscriber receives a new message. This callback function must have only one argument (other than <code>self</code>), which will contain the message data that has been received:</p> <p>We're also using a Python Type Annotation here too, which informs the interpreter that the <code>topic_message</code> that is received by the <code>msg_callback</code> function will be of the <code>String</code> data type.</p> <p>(All this really does is allow autocomplete functionality to work within our text editor, so that whenever we want to pull an attribute from the <code>toic_message</code> object it will tell us what attributes actually exist within the object.)</p> </li> <li> <p>In this simple example, all we're going to do on receipt of a message is to print a couple of log messages to the terminal, to include: </p> <ol> <li> <p>The name of this node (using the <code>self.get_name()</code> method)</p> </li> <li> <p>The message that has been received (<code>topic_mesage.data</code>)</p> </li> </ol> </li> <li> <p>The rest of this is exactly the same as before with our publisher.</p> </li> </ol>"},{"location":"course/assignment1/part1/subscriber/#dfts","title":"Don't Forget the Shebang!","text":"<p>Remember: don't forget the shebang, it's very important!</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p> \u2190 Back to Part 1 </p>"},{"location":"course/assignment1/part2/move_circle/","title":"A Simple Velocity Control Node (Move Circle)","text":""},{"location":"course/assignment1/part2/move_circle/#the-initial-code","title":"The Initial Code","text":"<p>Start by returning to the <code>publisher.py</code> file from your <code>part1_pubsub</code> package (or return here), and copy the contents into your new <code>move_circle.py</code> file. Then, adapt the code as follows.</p>"},{"location":"course/assignment1/part2/move_circle/#creating-the-move-circle-node","title":"Creating the \"Move Circle\" Node","text":""},{"location":"course/assignment1/part2/move_circle/#imports","title":"Imports","text":"<p>Once again, <code>rclpy</code> and the <code>Node</code> class from the <code>rclpy.node</code> library are vital for any node we create, so the first two imports will remain the same.</p> <p>We need to import the message type used by the <code>/cmd_vel</code> topic here though, in order to be able to format velocity command messages appropriately. We can find all the necessary information about this message type by using the <code>ros2 topic info</code> command:</p> <pre><code>$ ros2 topic info /cmd_vel\nType: geometry_msgs/msg/Twist\n...\n</code></pre> <p>And so our message import becomes:</p> <pre><code>from geometry_msgs.msg import Twist # (1)!\n</code></pre> <ol> <li>In place of: <code>from example_interfaces.msg import String</code></li> </ol>"},{"location":"course/assignment1/part2/move_circle/#change-the-class-name","title":"Change the Class Name","text":"<p>Previously our publisher class was called <code>SimplePublisher()</code>, change this to <code>Circle</code>:</p> <pre><code>class Circle(Node):\n</code></pre>"},{"location":"course/assignment1/part2/move_circle/#initialising-the-class","title":"Initialising the Class","text":"<ol> <li> <p>Initialise the node with an appropriate name:</p> <pre><code>super().__init__(\"move_circle\")\n</code></pre> </li> <li> <p>Change the <code>create_publisher()</code> parameters:</p> <pre><code>self.my_publisher = self.create_publisher(\n    msg_type=??, # (1)!\n    topic=??, # (2)!\n    qos_profile=10,\n)\n</code></pre> <ol> <li>What message type are we using here?</li> <li>What's the topic name?</li> </ol> </li> <li> <p>We'll need to publish velocity commands at a rate of at least 10 Hz, so set this here, and then set up a timer accordingly:</p> <pre><code>publish_rate = 10 # Hz\nself.timer = self.create_timer(\n    timer_period_sec=1/publish_rate, \n    callback=self.timer_callback\n)\n</code></pre> </li> </ol>"},{"location":"course/assignment1/part2/move_circle/#modifying-the-timer-callback","title":"Modifying the Timer Callback","text":"<p>Here, we'll publish our velocity commands:</p> <pre><code>def timer_callback(self):\n    radius = 0.5 # meters\n    linear_velocity = 0.1 # meters per second [m/s]\n    angular_velocity = ?? # radians per second [rad/s] # (1)!\n\n    topic_msg = Twist() # (2)!\n    topic_msg.linear.x = linear_velocity\n    topic_msg.angular.z = angular_velocity\n    self.my_publisher.publish(topic_msg) # (3)!\n\n    self.get_logger().info( # (4)!\n        f\"Linear Velocity: {topic_msg.linear.x:.2f} [m/s], \"\n        f\"Angular Velocity: {topic_msg.angular.z:.2f} [rad/s].\",\n        throttle_duration_sec=1, # (5)!\n    )\n</code></pre> <ol> <li> <p>Having defined the radius of the circle, and the linear velocity that we want the robot to move at, how would we calculate the angular velocity that should be applied?</p> <p>Consider the equation for angular velocity:</p> <p></p> <p></p> \\[ \\omega=\\frac{v}{r} \\] </li> <li> <p><code>/cmd_vel</code> uses <code>Twist</code> messages, so we instantiate one here, and assign the linear and angular velocity values (as set above) to the relevant message fields. Remember, we talked about all this here.</p> </li> <li> <p>Once the appropriate velocity fields have been set, publish the message.</p> </li> <li> <p>Publish a ROS Log Message to inform us (in the terminal) of the velocity control values that are being published by the node.</p> </li> <li> <p>Remember in the Odometry Subscriber how we used a counter (<code>self.counter</code>) and an <code>if()</code> statement to control the rate at which these log messages were generated?</p> <p>We can actually achieve exactly the same thing by simply supplying a <code>throttle_duration_sec</code> argument to the <code>get_logger().info()</code> call. Much easier, right?</p> </li> </ol>"},{"location":"course/assignment1/part2/move_circle/#updating-main","title":"Updating \"Main\"","text":"<p>Once again, don't forget to update any relevant parts of the <code>main</code> function to ensure that we're instantiating the <code>Circle()</code> class, spinning and shutting it down correctly.</p> <p> \u2190 Back to Part 2 </p>"},{"location":"course/assignment1/part2/move_square/","title":"Odometry-based Navigation (Move Square)","text":"<p>A combined publisher-subscriber node to achieve odometry-based control...</p> <p>Below you will find a template Python script to show you how you can both publish to <code>/cmd_vel</code> and subscribe to <code>/odom</code> in the same node.  This will help you build a closed-loop controller to make your robot follow a square motion path of size: 1m x 1m. </p> <p>You can publish velocity commands to <code>/cmd_vel</code> to make the robot move, monitor the robot's position and orientation in real-time, determine when the desired movement has been completed, and then update the velocity commands accordingly.  </p>"},{"location":"course/assignment1/part2/move_square/#suggested-approach","title":"Suggested Approach","text":"<p>Moving in a square can be achieved by switching between two different movement states sequentially: Moving forwards and turning on the spot. At the start of each movement step we can read the robot's current odometry, and then use this as a reference to compare to, and to tell us when the robot's position/orientation has changed by the required amount, e.g.:</p> <ol> <li>With the robot stationary, read the odometry to determine its current X and Y position in the environment.</li> <li>Move forwards until the robot's X and Y position indicate that it has moved linearly by 0.5m.</li> <li>Stop moving forwards.</li> <li>Read the robot's odometry to determine its current orientation (\"yaw\"/<code>\u03b8<sub>z</sub></code>).</li> <li>Turn on the spot until the robot's orientation changes by 90\u00b0.</li> <li>Stop turning.</li> <li>Repeat.  </li> </ol>"},{"location":"course/assignment1/part2/move_square/#the-code","title":"The Code","text":"move_square.py<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom rclpy.signals import SignalHandlerOptions\n\nfrom geometry_msgs.msg import Twist # (1)!\nfrom nav_msgs.msg import Odometry # (2)!\n\nfrom part2_navigation_modules.tb3_tools import quaternion_to_euler # (3)!\nfrom math import sqrt, pow, pi # (4)!\n\nclass Square(Node):\n\n    def __init__(self):\n        super().__init__(\"move_square\")\n\n        self.first_message = False\n        self.turn = False \n\n        self.vel_msg = Twist() # (5)! \n        # (6)!\n        self.x = 0.0; self.y = 0.0; self.theta_z = 0.0\n        self.xref = 0.0; self.yref = 0.0; self.theta_zref = 0.0\n        # (7)!\n        self.yaw = 0.0 \n        self.displacement = 0.0 \n\n        self.vel_pub = self.create_publisher(\n            msg_type=Twist,\n            topic=\"cmd_vel\",\n            qos_profile=10,\n        )\n\n        self.odom_sub = self.create_subscription(\n            msg_type=Odometry,\n            topic=\"odom\",\n            callback=self.odom_callback,\n            qos_profile=10,\n        )\n\n        ctrl_rate = 10 # hz\n        self.timer = self.create_timer(\n            timer_period_sec=1/ctrl_rate,\n            callback=self.timer_callback,\n        )\n\n        self.shutdown = False\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\"\n        )\n\n    def on_shutdown(self):\n        print(\"Stopping the robot...\")\n        self.vel_pub.publish(Twist())\n        self.shutdown = True\n\n    def odom_callback(self, msg_data: Odometry):\n        pose = msg_data.pose.pose \n\n        (roll, pitch, yaw) = quaternion_to_euler(pose.orientation) # (8)!\n\n        self.x = pose.position.x # (9)!\n        self.y = pose.position.y\n        self.theta_z = abs(yaw) # abs(yaw) makes life much easier!!\n\n        if not self.first_message: # (10)!\n            self.first_message = True\n            self.xref = self.x\n            self.yref = self.y\n            self.theta_zref = self.theta_z\n\n    def timer_callback(self):\n        # here is where the code to control the motion of the robot \n        # goes. Add code here to make the robot move in a square of\n        # dimensions 1 x 1m...\n        if self.turn:\n            # turn by 90 degrees...\n\n\n        else:\n            # move forwards by 1m...\n\n\n        # publish whatever velocity command has been set above:\n        self.vel_pub.publish(self.vel_msg)\n\ndef main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO,\n    )\n    move_square = Square()\n    try:\n        rclpy.spin(move_square)\n    except KeyboardInterrupt:\n        print(\n            f\"{move_square.get_name()} received a shutdown request (Ctrl+C).\"\n        )\n    finally:\n        move_square.on_shutdown()\n        while not move_square.shutdown:\n            continue\n        move_square.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <ol> <li>Import the <code>Twist</code> message for publishing velocity commands to <code>/cmd_vel</code>.</li> <li>Import the <code>Odometry</code> message, for use when subscribing to the <code>/odom</code> topic.</li> <li>Import the <code>quaternion_to_euler</code> function from <code>tb3_tools.py</code> to convert orientation from quaternions to Euler angles (about the principal axes).</li> <li> <p>Finally, import some useful mathematical operations (and <code>pi</code>), which may prove useful for this task:</p> <p></p><p></p> Mathematical Operation Python Implementation \\(\\sqrt{a+b}\\) <code>sqrt(a+b)</code> \\(a^{2}+(bc)^{3}\\) <code>pow(a, 2) + pow(b*c, 3)</code> \\(\\pi r^2\\) <code>pi * pow(r, 2)</code> <p></p><p></p> </li> <li> <p>Here we establish a <code>Twist</code> message, which we can populate with velocities and then publish to <code>/cmd_vel</code> within the <code>timer_callback()</code> method (in order to make the robot move).</p> </li> <li> <p>Here, we define some variables that we can use to store relevant bits of odometry data while our node is running (and read it back to implement feedback control):</p> <ul> <li><code>self.x</code>, <code>self.y</code> and <code>self.theta_z</code> will be used by the <code>odom_callback()</code> to store the robot's current pose</li> <li><code>self.xref</code>, <code>self.yref</code> and <code>self.theta_zref</code> can be used in the <code>timer_callback()</code> method to keep a record of where the robot was at a given moment in time (and determine how far it has moved since that point)</li> </ul> </li> <li> <p>We'll also need to keep track of how far the robot has travelled (or turned) in order to determine when sufficient movement has taken place to trigger a switch to the alternative state, i.e.:</p> <ul> <li><code>if</code> travelled 1 meter, <code>then</code>: turn</li> <li><code>if</code> turned 90\u00b0, <code>then</code>: move forward</li> </ul> </li> <li> <p>Here we obtain the robot's current orientation (in quaternions) and convert it to Euler angles (in radians) about the principal axes, where:</p> <ul> <li>\"roll\" = <code>\u03b8<sub>x</sub></code></li> <li>\"pitch\" = <code>\u03b8<sub>y</sub></code></li> <li>\"yaw\" = <code>\u03b8<sub>z</sub></code></li> </ul> </li> <li> <p>We're only interested in <code>x</code>, <code>y</code> and <code>\u03b8<sub>z</sub></code>, so we assign these to class variables <code>self.x</code>, <code>self.y</code> and <code>self.theta_z</code>, so that we can access them elsewhere within our <code>Square()</code> class.</p> </li> <li> <p>Sometimes, it can take a few moments for the first topic message to come through, and it's useful to know when that's happened so that you know you are dealing with actual topic data! Here, we're just setting a flag to <code>True</code> once the callback function has executed for the first time (i.e. the first topic message has been received).</p> </li> </ol>"},{"location":"course/assignment1/part2/move_square/#alternative-approach-waypoint-tracking","title":"Alternative Approach: Waypoint Tracking","text":"<p>A square motion path can be fully defined by the coordinates of its four corners, and we can make the robot move to each of these corners one-by-one, using its odometry system to monitor its real-time position, and adapting linear and angular velocities accordingly.</p> <p>This is slightly more complicated, and you might want to wait until you have a bit more experience with ROS before tackling it this way.</p> <p> \u2190 Back to Part 2 </p>"},{"location":"course/assignment1/part2/odom_subscriber/","title":"An Odometry Subscriber Node","text":""},{"location":"course/assignment1/part2/odom_subscriber/#the-initial-code","title":"The Initial Code","text":"<p>Having copied the <code>subscriber.py</code> file from your <code>part1_pubsub</code> package, you'll start out with the code discussed here.</p> <p>Let's look at what we need to change now.</p>"},{"location":"course/assignment1/part2/odom_subscriber/#from-simple-subscriber-to-odom-subscriber","title":"From Simple Subscriber to Odom Subscriber","text":""},{"location":"course/assignment1/part2/odom_subscriber/#imports","title":"Imports","text":"<p>We will generally rely on <code>rclpy</code> and the <code>Node</code> class from the <code>rclpy.node</code> library, for most nodes that we will create, so our first two imports will remain the same. </p> <p>We won't be working with <code>String</code> type messages any more however, so we need to replace this line in order to import the correct message type. As we know from earlier in Part 2, the <code>/odom</code> topic uses messages of the type <code>nav_msgs/msg/Odometry</code>:</p> <pre><code>$ ros2 topic info /odom\nType: nav_msgs/msg/Odometry\n...\n</code></pre> <p>This tells us everything we need to know to construct the Python import statement correctly:</p> <pre><code>from nav_msgs.msg import Odometry\n</code></pre> <p>We'll also need to import a handy function that should already exist as an importable module in your <code>part2_navigation</code> package called <code>tb3_tools</code>:</p> <pre><code>from part2_navigation_modules.tb3_tools import quaternion_to_euler\n</code></pre> <p>As the name suggests, we'll use this to convert the raw orientation values from <code>/odom</code> into their Euler Angle representation.</p> Info <p>This module can be found here: <code>part2_navigation/part2_navigation_modules/tb3_tools.py</code>, if you want to have a look.</p>"},{"location":"course/assignment1/part2/odom_subscriber/#change-the-class-name","title":"Change the Class Name","text":"<p>Previously our class was called <code>SimpleSubscriber()</code>, change this to something more appropriate now, e.g.: <code>OdomSubscriber()</code>:</p> <pre><code>class OdomSubscriber(Node):\n</code></pre>"},{"location":"course/assignment1/part2/odom_subscriber/#initialising-the-class","title":"Initialising the Class","text":"<p>The structure of this remains largely the same, we just need to modify a few things: </p> <ol> <li> <p>Change the name that is used to register the node on the ROS Network:</p> <pre><code>super().__init__(\"odom_subscriber\")\n</code></pre> </li> <li> <p>Change the subscription parameters:</p> <pre><code>self.my_subscriber = self.create_subscription(\n    msg_type=Odometry, # (1)!\n    topic=\"odom\", # (2)!\n    callback=self.msg_callback, \n    qos_profile=10,\n)\n</code></pre> <ol> <li><code>/odom</code> uses the Odometry message type (as imported above)</li> <li>The topic name is <code>\"odom\"</code>, of course!</li> </ol> </li> <li> <p>The final thing we'll do inside our class' <code>__init__</code> method (after we've set up the subscriber) is initialise a counter:</p> <pre><code>self.counter = 0 \n</code></pre> <p>The reason for this will be explained shortly...</p> </li> </ol>"},{"location":"course/assignment1/part2/odom_subscriber/#modifying-the-message-callback","title":"Modifying the Message Callback","text":"<p>This is where the changes are a bit more significant:</p> <pre><code>def msg_callback(self, topic_message: Odometry): # (1)!\n\n    pose = topic_message.pose.pose # (2)!\n\n    # (3)!\n    pos_x = pose.position.x\n    pos_y = pose.position.y\n    pos_z = pose.position.z\n\n    roll, pitch, yaw = quaternion_to_euler(pose.orientation) # (4)!\n\n    if self.counter &gt; 10: # (5)!\n        self.counter = 0\n        self.get_logger().info(\n            f\"x = {pos_x:.3f} (m), y = ? (m), theta_z = ? (radians)\"\n        ) # (6)!\n    else:\n        self.counter += 1\n</code></pre> <ol> <li>When defining the message callback, modify the type annotation for the <code>topic_message</code> input.</li> <li> <p>There are two key parts to an odometry message: Pose and Twist.</p> <p>We're only really interested in the Pose part of the message here, so grab this first.</p> </li> <li> <p>As we know by now, Pose contains information about both the \"position\" and \"orientation\" of the robot, we extract the position values first and assign them to the variables <code>pos_x</code>, <code>pos_y</code> and <code>pos_z</code>.</p> <p>Position data is provided in meters, so we don't need to do any conversion on this and can use the data directly.</p> </li> <li> <p>Orientation data is in quaternions, so we convert this by passing it to the <code>quaternion_to_euler</code> function that we imported from <code>tb3_tools</code> earlier.</p> <p>This function provides us with the orientation of the robot about its 3 principal axes:</p> <ul> <li><code>\u03b8<sub>x</sub></code>: \"Roll\"</li> <li><code>\u03b8<sub>y</sub></code>: \"Pitch\"</li> <li><code>\u03b8<sub>z</sub></code>: \"Yaw\"</li> </ul> </li> <li> <p>Here we print out the values that we're interested in to the terminal.</p> <p>This callback function will execute every time a new message is published to the <code>odom</code> topic, which occurs at a rate of around 20 times per second (20 Hz).</p> Tip <p>We can use he <code>ros2 topic hz</code> function to tell us this:</p> <pre><code>$ ros2 topic hz /odom\naverage rate: 18.358\nmin: 0.037s max: 0.088s std dev: 0.01444s window: 20\n</code></pre> <p>That's a lot of messages to be printed to the terminal every second! We therefore use an <code>if</code> statement and a <code>counter</code> to ensure that our <code>print</code> statement only executes for 1 in every 10 topic messages instead.</p> </li> <li> <p>Task: Continue formatting the <code>print</code> message to display the three odometry values that are relevant to our robot!  </p> </li> </ol>"},{"location":"course/assignment1/part2/odom_subscriber/#updating-main","title":"Updating \"Main\"","text":"<p>The only thing left to do now is update any relevant parts of the <code>main</code> function to ensure that you are instantiating, spinning and shutting down your node correctly.</p> <p> \u2190 Back to Part 2 </p>"},{"location":"course/assignment1/part3/lidar_subscriber/","title":"Building a Basic LaserScan Subscriber Node","text":"<p>Copy all the code below into your <code>lidar_subscriber.py</code> file and then review the annotations to understand how it all works.</p> lidar_subscriber.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.signals import SignalHandlerOptions # (1)!\n\nfrom sensor_msgs.msg import LaserScan # (2)!\n\nimport numpy as np # (3)!\n\nclass LidarSubscriber(Node): \n\n    def __init__(self): \n        super().__init__(\"lidar_subscriber\")\n\n        self.lidar_sub = self.create_subscription(\n            msg_type=LaserScan,\n            topic=\"/scan\",\n            callback=self.lidar_callback,\n            qos_profile=10,\n        ) # (4)!\n\n        self.get_logger().info(f\"The '{self.get_name()}' node is initialised.\")\n\n    def lidar_callback(self, scan_data: LaserScan): \n        left_20_deg = scan_data.ranges[0:21]\n        right_20_deg = scan_data.ranges[-20:] # (5)!\n        front = np.array(left_20_deg + right_20_deg) # (6)!\n\n        valid_data = front[front != float(\"inf\")] # (7)!\n        if np.shape(valid_data)[0] &gt; 0: # (8)!\n            single_point_average = valid_data.mean() # (9)!\n        else:\n            single_point_average = float(\"nan\") # (10)!\n\n        self.get_logger().info(\n            f\"LiDAR Reading (front): {single_point_average:.3f} meters.\",\n            throttle_duration_sec = 1,\n        ) # (11)!\n\ndef main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    )\n    node = LidarSubscriber()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print(\"Shutdown request (Ctrl+C) detected...\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown() \n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li>None of this should be new to you by now. Remember from Part 2 that we're using <code>SignalHandlerOptions</code> to handle shutdown requests (triggered by Ctrl+C).</li> <li>We're building a <code>/scan</code> subscriber here, and we know that this topic uses the <code>sensor_msgs/msg/LaserScan</code> interface type, so we import this here.</li> <li><code>numpy</code> is a Python library that allows us to work with numeric data, very useful for big arrays like <code>ranges</code>.</li> <li>We construct a subscriber in much the same way as we have done in Parts 1 and 2, this time targetting the <code>/scan</code> topic though.</li> <li> <p>From the front of the robot, we obtain a 20\u00b0 arc of scan data either side of the x-axis (see the figure below).</p> </li> <li> <p>Then, we combine the <code>left_20_deg</code> and <code>right_20_deg</code> data arrays, and convert this from a Python list to a <code>numpy</code> array (see the figure below).</p> </li> <li> <p>This illustrates one of the great features of <code>numpy</code> arrays: we can filter them.</p> <p>Remember that <code>front</code> is now a <code>numpy</code> array containing 40 data points.</p> <p>Remember also, that there will typically be several <code>inf</code> values scattered around the LaserScan array, resulting from sensor readings that are outside the sensor's measurement range (i.e. greater than <code>range_max</code> or less than <code>range_min</code>). We need to get rid of these, so we ask <code>numpy</code> to filter our array as follows:</p> <ol> <li> <p>Of all the values in the <code>front</code> array, determine which ones are not equal to <code>inf</code>: </p> <p><code>front != float(\"inf\")</code></p> </li> <li> <p>Use this filter to remove these <code>inf</code> values from the <code>front</code> array:</p> <p><code>front[front != float(\"inf\")]</code></p> </li> <li> <p>Return this as a new <code>numpy</code> array called <code>valid_data</code>:</p> <p><code>valid_data = front[front != float(\"inf\")]</code></p> </li> </ol> </li> <li> <p>In certain situations (i.e. in very sparse environments) all values could be equal to<code>inf</code> (imagine the \"empty world\" simulation). Here we're checking the size of the <code>valid_data</code> array to make sure that we haven't just removed all values through the above filtering process!</p> </li> <li> <p>If the array is not empty, then use the <code>mean()</code> method to determine the average of all readings within the dataset</p> </li> <li>If the array is empty, then return \"not a number\" (aka \"nan\") instead </li> <li> <p>Print this value to the terminal, but throttle the messages so that only one is displayed every second</p> <p>Question</p> <p>If we didn't throttle this, what rate would the messages be printed at?</p> </li> </ol> <p>The data processing is illustrated in the figure below:</p> <p></p> <p> \u2190 Back to Part 3 </p>"},{"location":"course/assignment1/part4/number_game_client/","title":"Creating a Python Service Client","text":"<p>Copy all the code below into your <code>number_game_client.py</code> file and then review the annotations to understand how it all works.</p> number_game_client.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\n\nfrom part4_services.srv import MyNumberGame\n\nimport argparse # (1)!\n\nclass NumberGameClient(Node):\n\n    def __init__(self):\n        super().__init__('number_game_client')\n\n        self.client = self.create_client(\n            srv_type=MyNumberGame, \n            srv_name='guess_the_number'\n        ) # (2)!\n\n        cli = argparse.ArgumentParser() # (3)!\n        cli.add_argument(\n            \"-g\", \"--guess\", default=0, type=int\n        ) # (4)!\n        cli.add_argument(\n            \"-c\", \"--cheat\", action=\"store_true\"\n        ) # (5)!\n        self.args = cli.parse_args() # (6)!\n\n        while not self.client.wait_for_service(timeout_sec=1.0):\n            self.get_logger().info(\n                \"Waiting for service...\"\n            ) # (7)!\n\n    def send_request(self, guess, cheat): # (8)!\n        request = MyNumberGame.Request()\n        request.guess = guess\n        request.cheat = cheat\n\n        return self.client.call_async(request)\n\ndef main():\n    rclpy.init()\n    client = NumberGameClient()\n\n    client.get_logger().info(\n        f\"Sending the request:\\n\"\n        f\" - guess: {client.args.guess}\\n\"\n        f\" - cheat: {client.args.cheat}\\n\"\n        f\"   Awaiting response...\"\n    ) # (9)!\n\n    future = client.send_request(client.args.guess, client.args.cheat) # (10)!\n    rclpy.spin_until_future_complete(client, future) # (11)!\n    response = future.result() # (12)!\n\n    client.get_logger().info(\n        f\"The server has responded with:\\n\"\n        f\" - {'You guessed correctly! :)' if response.correct else 'Incorrect guess :('}\\n\"\n        f\" - Number of attempts so far: {response.num_guesses}\\n\"\n        f\" - A hint: '{response.hint}'.\"\n    ) # (13)!\n\n    client.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>Nothing above this should be new to you. </p> <p>Here however, we're importing a standard Python module called <code>argparse</code>, which we'll use to build a command-line interface for our node.</p> </li> <li> <p>Creating a Service Client is done using the <code>create_client()</code> class method, providing the name of the service that we want to call (<code>srv_name</code>), and specifying the interface type used by it (<code>srv_type</code>). </p> <p><code>srv_type</code> and <code>srv_name</code> must match the definition in the server, in order to be able to communicate and send requests to it. </p> </li> <li> <p>Here we're building a simple Command-line Interface (CLI) for the node, using <code>argparse</code>.</p> </li> <li> <p>We add an argument here called \"guess\", which will be used to pass our guesses for the number game from the command line, into this node.</p> <p>The node itself can then access the value that we've passed to it (an <code>int</code>) and use this to construct a service request.</p> <p>We've assigned a default value of <code>0</code> here, for cases where we don't pass a guess from the CLI.</p> </li> <li> <p>Here we're adding a second Command-line Argument (CLA) called \"cheat\".</p> <p><code>action=\"store_true\"</code> ensures that if we don't specify this argument as a CLA then the value will be set to <code>False</code>. If we do pass in this argument then the value will be <code>True</code>. </p> <p>(You'll see how this all works shortly, when we actually run the node.)</p> </li> <li> <p>Here we \"parse\" the arguments that have been passed to the node from the CLI, so that we can access them from <code>self.args</code>.</p> </li> <li> <p>We use a <code>while</code> loop here to halt the execution of the code at this point and wait for the service to become available (if it isn't already). </p> <p>We can't send a request to a service that isn't actually running!</p> </li> <li> <p>In this class method we construct a service request, based on the values that have been passed via the CLI.</p> <p>(This method is called in the <code>main()</code> function below.)</p> <p>We know what the request attributes are called, because we defined them in the <code>MyNumberGame.srv</code> file, and we can also use <code>ros2 interface show</code> to recall them at any point:</p> <pre><code>$ ros2 interface show part4_services/srv/MyNumberGame\n\nint32 guess\nbool cheat\n---\nint32 num_guesses\nstring hint\nbool correct\n</code></pre> <p><code>call_async(request)</code> then actually sends this request to the server.</p> </li> <li> <p>Here we're grabbing the values passed via the CLI and printing them as a log message to the terminal, in order to verify exactly what request will be sent.</p> </li> <li> <p>We then call our client's <code>send_request()</code> class method, supplying the <code>guess</code> and <code>cheat</code> values from the CLI to this too, in order for the request to be constructed accordingly, and sent to the server. </p> <p>The output of this function is the output of the <code>call_async(request)</code> call, which we assign to a variable called <code>future</code>.</p> </li> <li> <p>We use the <code>rclpy.spin_until_future_complete()</code> method here, which (as the name suggests) will allow our node (<code>client</code>) to spin only until our service request (<code>future</code>) has completed. </p> </li> <li> <p>Once we've reached this point then the service has completed and returned its Response. </p> <p>We obtain the response from our <code>future</code> object so that we can read its values...</p> </li> <li> <p>To finish off, we construct another log message to contain all the values returned by the Server (i.e. the Response). </p> <p>We know what these attributes are called, because we defined them in the <code>MyNumberGame.srv</code> file, which we can recall at any point using <code>ros2 interface show</code>:</p> <pre><code>$ ros2 interface show part4_services/srv/MyNumberGame\n\nint32 guess\nbool cheat\n---\nint32 num_guesses\nstring hint\nbool correct\n</code></pre> </li> </ol> <p> \u2190 Back to Part 4 </p>"},{"location":"course/assignment1/part5/explore_server/","title":"The Part 5 Explore Server Template","text":""},{"location":"course/assignment1/part5/explore_server/#the-part-5-explore-server-template","title":"The Part 5 Explore Server Template","text":"explore_server.py<pre><code>#!/usr/bin/env python3\n\n# Import the core Python libraries for ROS and to implement ROS Actions:\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\n\n# Import additional rclpy libraries for multithreading and shutdown\nfrom rclpy.executors import MultiThreadedExecutor\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.signals import SignalHandlerOptions\n\n# Import our package's action interface:\nfrom part5_actions.action import ExploreForward\n# Import other key ROS interfaces that this server will use:\nfrom geometry_msgs.msg import Twist\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import LaserScan\n\n# Import some other useful Python Modules\nfrom math import sqrt, pow\nimport numpy as np\n\nclass ExploreForwardServer(Node):\n\n    def __init__(self):\n        super().__init__(\"explore_forward_server_node\")\n\n        self.posx = 0.0 # (1)!\n        self.posy = 0.0\n        self.lidar_reading = 0.0\n\n        self.shutdown = False # (2)!\n\n        self.await_odom = True # (3)!\n        self.await_lidar = True\n\n        self.loop_rate = self.create_rate(\n            frequency=5, \n            clock=self.get_clock()\n        ) # (4)!\n\n        ## TASK: create a /cmd_vel publisher\n        self.vel_pub = self.create_publisher(...)\n\n        ## TASK: create an /odom subscriber \n        self.odom_sub = self.create_subscription(...)\n\n        ## TASK: create a /scan subscriber\n        self.lidar_sub = self.create_subscription(...)\n\n        # Creating the action server:\n        self.actionserver = ActionServer(\n            node=self, \n            action_type=ExploreForward,\n            action_name=\"explore_forward\",\n            execute_callback=self.server_execution_callback,\n            callback_group=ReentrantCallbackGroup(),\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback\n        )\n\n    def odom_callback(self, odom_msg: Odometry):\n        \"\"\"\n        Callback for the /odom subscriber\n        \"\"\"\n        ## TASK: obtain the robot's X and Y position:\n        self.posx = # (5)!\n        self.posy = \n\n        self.await_odom = False\n\n    def lidar_callback(self, scan_msg: LaserScan):\n        \"\"\"\n        Callback for the /scan subscriber\n        \"\"\"\n        ## TASK: Obtain the front 40 degrees of LiDAR\n        # data, filter it and return the average distance\n        # (or 'nan'):\n        self.lidar_reading = # (6)!\n\n        self.await_lidar = False\n\n    def goal_callback(self, goal: ExploreForward.Goal):\n        \"\"\"\n        A callback to check that the goal inputs are valid        \n        \"\"\"\n        goal_ok = True\n        ## TASK: check the 'fwd_velocity' goal parameter is valid\n        # to ensure that the robot moves forward but that it \n        # doesn't exceed is max velocity limit:\n        if goal.fwd_velocity ...\n\n\n\n            goal_ok = False\n\n        ## TASK: check the 'stopping_distance' goal parameter is valid\n        # (it should be at least 0.2m to make sure it doesn't get too \n        # close!)\n        if goal.stopping_distance ...\n\n\n\n            goal_ok = False\n\n        return GoalResponse.ACCEPT if goal_ok else GoalResponse.REJECT\n\n    def cancel_callback(self, goal):\n        \"\"\"\n        A callback to trigger cancellation of the action\n        \"\"\"\n        self.get_logger().info('Received a cancel request...')\n        return CancelResponse.ACCEPT\n\n    def on_shutdown(self):\n        \"\"\"\n        A method to stop the robot on shutdown\n        \"\"\"\n        for i in range(5):\n            self.vel_pub.publish(Twist())\n        self.shutdown = True\n\n    def server_execution_callback(self, goal):\n        \"\"\"\n        A callback to encapsulate the action that is performed\n        when the server is called (i.e. a valid goal is issued)\n        \"\"\"\n        result = ExploreForward.Result()\n        feedback = ExploreForward.Feedback()\n        fwd_vel = goal.request.fwd_velocity\n        stop_dist = goal.request.stopping_distance\n\n        self.get_logger().info(\n            f\"\\n#####\\n\"\n            f\"The '{self.get_name()}' has been called.\\n\"\n            f\"Goal:\\n\"\n            f\"  - explore at {fwd_vel:.2f} m/s\\n\"\n            f\"  - stop {stop_dist:.2f} m in front of something\\n\" \n            f\"Here we go...\"\n            f\"\\n#####\\n\")\n\n        # set the robot's velocity (based on the request):\n        vel_cmd = Twist()\n        vel_cmd...\n\n        # Get the robot's current position:\n        while self.await_odom or self.await_lidar:\n            continue\n        ref_posx = self.posx\n        ref_posy = self.posy\n        dist_travelled = 0.0\n\n        ## TASK: establish a while loop that executes until lidar readings \n        # indicate that an object is closer than the requested stopping distance\n        while SOMETHING &gt; SOMETHING_ELSE:\n\n            ## TASK: publish a velocity command to make the robot move forward \n            # at the requested velocity...\n\n            # check if there has been a request to cancel the action:\n            if goal.is_cancel_requested:\n                # stop the robot:\n                for i in range(5):\n                    self.vel_pub.publish(Twist())\n                goal.canceled()\n                self.get_logger().info(\n                    f\"Cancelled.\"\n                )\n                result.total_distance_travelled = dist_travelled\n                result.closest_obstacle = float(self.lidar_reading)\n                return result\n\n            ## TASK: calculate the distance travelled so far:\n            dist_travelled = \n\n            feedback.current_distance_travelled = dist_travelled\n            goal.publish_feedback(feedback)\n\n            self.loop_rate.sleep() # (7)!\n\n        for i in range(5):\n            self.vel_pub.publish(Twist())\n\n        self.get_logger().info(\n            f\"{self.get_name()} complete.\"\n        )\n        goal.succeed()\n        return result\n\ndef main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    )\n    node = ExploreForwardServer()\n    executor = MultiThreadedExecutor()\n    executor.add_node(node)\n    try:\n        node.get_logger().info(\n            \"Starting the Server (shut down with Ctrl+C)\"\n        )\n        executor.spin()\n    except KeyboardInterrupt:\n        node.get_logger().info(\n            \"Server shut down with Ctrl+C\"\n        )\n    finally:\n        ## TASK: complete all necessary shutdown operations.\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li>Some variables to store data from the <code>/odom</code> (<code>posx</code> and <code>posy</code>) and <code>/scan</code> (<code>lidar_reading</code>) subscribers, and share this data across the class.</li> <li>A shutdown flag (you've seen this before)</li> <li>Some flags to determine when we've received data from the <code>/odom</code> and <code>/scan</code> subscribers, to ensure that our action server's main execution callback doesn't begin until we have some valid data to work with (see usage below).</li> <li>Here we're creating an object that we can use in our action server's main execution callback to control the rate of execution inside a <code>while</code> loop (see usage below).</li> <li>Refer back to the Part 2 Odometry Subscriber for help with this (if you need it).</li> <li>Refer back to the Part 3 Lidar Subscriber for help with this (if you need it).</li> <li> <p>Calling this here will block any further code execution until enough time has elapsed. </p> <p>This time is dictated by the <code>frequency</code> parameter that we defined when we set this up earlier:</p> <pre><code>self.loop_rate = self.create_rate(\n    frequency=5, \n    clock=self.get_clock()\n)\n</code></pre> </li> </ol> <p> \u2190 Back to Part 5 </p>"},{"location":"course/assignment1/part6/line_follower/","title":"Part 6 Line Following (Setup)","text":""},{"location":"course/assignment1/part6/line_follower/#part-6-line-following-setup","title":"Part 6 Line Following (Setup)","text":"<p>Use this code as a starting point for Part A of the Line Following exercise.</p> line_follower.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.signals import SignalHandlerOptions\n\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Twist\n\nclass LineFollower(Node):\n\n    def __init__(self):\n        super().__init__(\"line_follower\")\n\n        self.camera_sub = self.create_subscription(\n            msg_type=Image,\n            topic=\"/camera/image_raw\",\n            callback=self.camera_callback,\n            qos_profile=10,\n        )\n\n        self.vel_pub = self.create_publisher(\n            msg_type=Twist,\n            topic=\"/cmd_vel\",\n            qos_profile=10\n        )\n\n        self.vel_cmd = Twist()\n        self.shutdown = False\n\n    def shutdown_ops(self):\n        self.get_logger().info(\n            \"Shutting down...\"\n        )\n        cv2.destroyAllWindows()\n        for i in range(5):\n            self.vel_pub.publish(Twist())\n        self.shutdown = True\n\n    def camera_callback(self, img_data):\n        cvbridge_interface = CvBridge()\n        try:\n            cv_img = cvbridge_interface.imgmsg_to_cv2(\n                img_data, desired_encoding=\"bgr8\")\n        except CvBridgeError as e:\n            self.get_logger().warn(f\"{e}\")\n\n        cv2.imshow(\"camera image\", cv_img)\n\n        height, width, _ = cv_img.shape\n        ## TODO 1 (1)\n\n        ## TODO 2 (2)\n\n        ## TODO 3 (3)\n\n        cv2.waitKey(1)\n\ndef main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    )\n    node = LineFollower()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\n            f\"{node.get_name()} received a shutdown request (Ctrl+C)\"\n        )\n    finally:\n        node.shutdown_ops()\n        while not node.shutdown:\n            continue\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>Image Cropping </p> <p>Apply some cropping to the raw camera image (<code>cv_img</code>). </p> <p>Crop it to around 1/5 of its original height, and to a width so that the pink line is just visible at the edge of the image. </p> <p>Call your new cropped image something like <code>cropped_img</code>. You could then use the <code>cv2.imshow()</code> method to display this in an additional pop-up window when the node is run: </p> <pre><code>cv2.imshow(\"cropped_image\", cropped_img)\n</code></pre> <p></p> <p></p> </li> <li> <p>Colour Detection</p> <p>Filter the cropped image by selecting appropriate HSV values so that the pink line can be isolated from the rest of the image.</p> <p>You may need to use the <code>tuos_examples\\image_colours.py</code> node again to help you identify the correct Hue and Saturation value range.</p> <p>Use <code>cv2.cvtColor()</code> to convert your <code>cropped_img</code> into an HSV colour representation:</p> <pre><code>hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\n</code></pre> <p>Use <code>cv2.inRange()</code> to create a mask with the HSV value range that you have determined:</p> <pre><code>line_mask = cv2.inRange(\n    hsv_img, {lower_hsv_values}, {upper_hsv_values}\n)\n</code></pre> <p>And then use <code>cv2.bitwise_and()</code> to create a new image with the mask applied, so that the coloured line is isolated:</p> <pre><code>line_isolated = cv2.bitwise_and(\n    cropped_img, cropped_img, mask = line_mask\n)\n</code></pre> <p></p> <p></p> </li> <li> <p>Locating the line</p> <p>Finally, find the horizontal position of the line in the robot's viewpoint.</p> <p>Calculate the image moments of the pink colour blob that represents the line (<code>line_mask</code>) using the <code>cv2.moments()</code> method. Remember that it's the \\(c_{y}\\) component that we're interested in here:</p> \\[ c_{y}=\\dfrac{M_{10}}{M_{00}} \\] <p>Ultimately, this will provide us with the feedback signal that we can use for a proportional controller that we will implement in the next part of the exercise.</p> <p>Once you've obtained the image moments (and <code>cy</code>), use <code>cv2.circle()</code> to mark the centroid of the line on the filtered image (<code>line_isolated</code>) with a circle. For this, you'll also need to calculate the \\(c_{z}\\) component of the centroid:</p> \\[ c_{z}=\\dfrac{M_{01}}{M_{00}} \\] <p>Remember that once you've done all this you can display the filtered image of the isolated line (with the circle to denote the centroid location) using <code>cv2.imshow()</code> again:</p> <pre><code>cv2.imshow(\"filtered line\", line_isolated)\n</code></pre> <p></p> <p></p> </li> </ol> <p> \u2190 Back to Part 6 - Exercise 4 (Part A) </p>"},{"location":"course/assignment1/part6/object_detection/","title":"Part 6 Object Detection Node","text":""},{"location":"course/assignment1/part6/object_detection/#part-6-object-detection-node","title":"Part 6 Object Detection Node","text":"<p>Copy all the code below into your <code>object_detection.py</code> file, and make sure you read the annotations!</p> object_detection.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy \nfrom rclpy.node import Node # (1)!\n\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError # (2)!\n\nfrom sensor_msgs.msg import Image # (3)!\n\nfrom pathlib import Path # (4)!\n\nclass ObjectDetection(Node):\n\n    def __init__(self): # (5)!\n        super().__init__(\"object_detection\")\n\n        self.camera_sub = self.create_subscription(\n            msg_type=Image,\n            topic=\"/camera/image_raw\",\n            callback=self.camera_callback,\n            qos_profile=10\n        )\n\n        self.waiting_for_image = True # (6)!\n\n    def camera_callback(self, img_data): # (7)!\n        cvbridge_interface = CvBridge() # (8)!\n        try:\n            cv_img = cvbridge_interface.imgmsg_to_cv2(\n                img_data, desired_encoding=\"bgr8\"\n            ) # (9)!\n        except CvBridgeError as e:\n            self.get_logger().warning(f\"{e}\")\n\n        if self.waiting_for_image: # (10)!\n            height, width, channels = cv_img.shape\n\n            self.get_logger().info(\n                f\"Obtained an image of height {height}px and width {width}px.\"\n            )\n\n            self.show_image(img=cv_img, img_name=\"step1_original\")\n\n            self.waiting_for_image = False # (15)!\n            cv2.destroyAllWindows() # (16)!\n\n    def show_image(self, img, img_name, save_img=True): # (11)!\n\n        self.get_logger().info(\"Opening the image in a new window...\")\n        cv2.imshow(img_name, img) # (12)!\n\n        if save_img: # (13)!\n            self.save_image(img, img_name)\n\n        self.get_logger().info(\n            \"IMPORTANT: Close the image pop-up window to exit.\"\n        )\n\n        cv2.waitKey(0) # (14)!\n\n    def save_image(self, img, img_name): # (17)!\n        self.get_logger().info(f\"Saving the image...\")\n\n        base_image_path = Path.home().joinpath(\"myrosdata/object_detection/\")\n        base_image_path.mkdir(parents=True, exist_ok=True) # (18)!\n        full_image_path = base_image_path.joinpath(\n            f\"{img_name}.jpg\") # (19)!\n\n        cv2.imwrite(str(full_image_path), img) # (20)!\n\n        self.get_logger().info(\n            f\"\\nSaved an image to '{full_image_path}'\\n\"\n            f\"  - image dims: {img.shape[0]}x{img.shape[1]}px\\n\"\n            f\"  - file size: {full_image_path.stat().st_size} bytes\"\n        ) # (21)!\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetection()\n    while node.waiting_for_image:\n        rclpy.spin_once(node) # (22)!\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>Nothing new here, moving on...</p> </li> <li> <p>We're importing the OpenCV library for Python (remember the Python API that we talked about earlier), which is called <code>cv2</code>, and also that ROS-to-OpenCV bridge interface that we talked about earlier too: <code>cv_bridge</code>.</p> <p>From <code>cv_bridge</code> we're importing the <code>CvBridge</code> and <code>CvBridgeError</code> classes from the <code>cv_bridge</code> library specifically.</p> </li> <li> <p>We need to subscribe to an image topic in order to obtain the data being published to it. You should've already identified the type of interface that is published to the <code>/camera/image_raw</code> topic, so we import that interface type here (from the <code>sensor_msgs</code> package) so that we can build a subscriber to the topic later.</p> </li> <li> <p>We're also importing the Python <code>Path</code> class from the <code>pathlib</code> module. A very handy tool for doing file operations.</p> </li> <li> <p>Initialising our <code>ObjectDetection()</code> Class (should be very familiar to you by now):</p> <ol> <li>Giving our node a name.</li> <li>Creating a subscriber to the <code>/camera/image_raw</code> topic, providing the interface type used by the topic (<code>sensor_msgs/msg/Image</code> - as imported above), and pointing it to a callback function (<code>camera_callback</code>, in this case), to define the processes that should be performed every time a message is obtained on this topic (in this case, the messages will be our camera images)</li> </ol> </li> <li> <p>We're creating a flag to indicate whether the node has obtained an image yet or not. For this exercise, we only want to obtain a single image, so we will set the <code>waiting_for_image</code> flag to <code>False</code> once an image has been obtained and processed, to avoid capturing any more. </p> <p>This flag will then be used to shut down the node when it's done its job.</p> </li> <li> <p>Here, we're defining a callback function for our <code>self.camera_sub</code> subscriber...</p> </li> <li> <p>Here, we create an instance of the <code>CvBridge</code> class that we imported earlier, and which we'll use later on to convert ROS image data into a format that OpenCV can understand.</p> </li> <li> <p>We're using the CvBridge interface to take our ROS image data and convert it to a format that OpenCV will be able to understand.  In this case we are specifying conversion (or \"encoding\") to an 8-bit BGR (Blue-Green-Red) image format: <code>\"bgr8\"</code>.</p> <p>We contain this within a <code>try</code>-<code>except</code> block though, which is the recommended procedure when doing this.  Here we try to convert an image using the desired encoding, and if a <code>CvBridgeError</code> is raised then we print a warning message to the terminal.  Should this happen, this particular execution of the camera callback function will stop.</p> </li> <li> <p>Then we check the <code>waiting_for_image</code> flag to see if this is the first image that has been received by the node.  If so, then:</p> <ol> <li>Obtain the height and width of the image (in pixels), as well as the number of colour channels.</li> <li>Print a log message containing the image dimensions.</li> <li>Pass the image data to the <code>show_image()</code> function (defined below). We also pass a descriptive name for the image to this function too (<code>img_name</code>).</li> </ol> </li> <li> <p>This class method presents the image to us in a pop-up window and also calls another method which saves the image to file for us.    </p> </li> <li> <p>Display the actual image in a pop-up window:</p> <ol> <li>The image data is passed into the function via the <code>img</code> argument,</li> <li>We need to give the pop-up window a name, so in this case we are using the <code>img_name</code> argument that is passed into this class method.</li> </ol> </li> <li> <p>The <code>show_image()</code> class method has a <code>save_img</code> argument, which is set to <code>True</code> by default, so that this <code>if</code> condition is triggered, and another class method is called to save the image to file.    </p> </li> <li> <p>We're supplying a value of <code>0</code> here, which tells this function to keep this window open indefinitely and wait until it is closed manually before allowing our <code>show_image()</code> class method to complete.</p> <p>If we had supplied a value here (say: <code>1</code>) then the function would simply wait for 1 millisecond and then close the pop-up window down. In our case however, we want some time to actually look at the image and then close the window down ourselves, manually. </p> <p>Once the window has been closed, the execution of our code is able to continue...    </p> </li> <li> <p>We then set the <code>waiting_for_image</code> flag to <code>False</code> so that we only ever perform these processing steps once (we only want to capture a single image).  This will then trigger the main <code>while</code> loop to stop (see below), thus causing the overall execution of the node to stop too.</p> </li> <li> <p><code>cv2.destroyAllWindows()</code> ensures that any OpenCV image pop-up windows that may still be active or in memory are destroyed before the class method exits (and the node shuts down).     </p> </li> <li> <p>This class method handles the saving of the image to a file using <code>cv2</code> tools and <code>pathlib</code>.</p> </li> <li> <p>Here, we define a filesystem location to save images to. </p> <p>We want this to exist in a folder called \"<code>myrosdata/object_detection</code>\" in the home directory, so we can use Pathlib's <code>Path.home().joinpath(...)</code> to define it (a handy way to access the User's home directory, without needing to know the Users name).</p> <p>Then, we use the Pathlib <code>Path.mkdir()</code> method to create this directory if it doesn't exist already.    </p> </li> <li> <p>A full file path is constructed for the image here (using the <code>Path.joinpath()</code> method), based on:</p> <ol> <li>The <code>base_image_path</code> that we defined above </li> <li>An image name that is passed into this class method via the <code>img_name</code> argument.</li> </ol> </li> <li> <p>This saves the image to a <code>.jpg</code> file.  We're supplying the <code>full_image_path</code> that was created above, and also the actual image data (<code>self.cv_img</code>) so that the function knows what image we want to save.</p> </li> <li> <p>We're printing a log message to the terminal to inform us of:</p> <ol> <li>Where the image has been saved to</li> <li>How big the image is (in terms of its pixel dimensions)</li> <li>How big the image file is (in bytes).</li> </ol> </li> <li> <p>We're using <code>spin_once()</code> inside a <code>while</code> loop here so that we can keep an eye on the value of the <code>wait_for_image</code> flag, and stop spinning (i.e. break out of the <code>while</code> loop) once it turns <code>False</code>.</p> </li> </ol> <p> \u2190 Back to Part 6 - Exercise 2 </p>"},{"location":"course/assignment1/part6/object_detection_complete/","title":"Part 6 Object Detection Node (Complete)","text":""},{"location":"course/assignment1/part6/object_detection_complete/#part-6-object-detection-node-complete","title":"Part 6 Object Detection Node (Complete)","text":"<p>Here's a full example of the <code>object_detection.py</code> node that you should have developed during Part 6 Exercise 2.  Also included here is an illustration of how to use the <code>cv2.circle()</code> method to create a marker on an image illustrating the centroid of the detected feature, as discussed here.</p> object_detection_complete.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy \nfrom rclpy.node import Node\n\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError\n\nfrom sensor_msgs.msg import Image\n\nfrom pathlib import Path\n\nclass ObjectDetection(Node):\n\n    def __init__(self):\n        super().__init__(\"object_detection\")\n\n        self.camera_sub = self.create_subscription(\n            msg_type=Image,\n            topic=\"/camera/image_raw\",\n            callback=self.camera_callback,\n            qos_profile=10\n        )\n\n        self.waiting_for_image = True\n\n    def camera_callback(self, img_data):\n        cvbridge_interface = CvBridge()\n        try:\n            cv_img = cvbridge_interface.imgmsg_to_cv2(\n                img_data, desired_encoding=\"bgr8\"\n            )\n        except CvBridgeError as e:\n            self.get_logger().warning(f\"{e}\")\n\n        if self.waiting_for_image:\n            height, width, channels = cv_img.shape\n\n            self.get_logger().info(\n                f\"Obtained an image of height {height}px and width {width}px.\"\n            )\n\n            self.show_image(img=cv_img, img_name=\"step1_original\")\n\n            crop_width = width - 400\n            crop_height = 400\n            crop_y0 = int((width / 2) - (crop_width / 2))\n            crop_z0 = int((height / 2) - (crop_height / 2))\n            cropped_img = cv_img[\n                crop_z0:crop_z0+crop_height, \n                crop_y0:crop_y0+crop_width\n            ]\n\n            self.show_image(img=cropped_img, img_name=\"step2_cropping\")\n\n            hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\n            lower_threshold = (115, 225, 100)\n            upper_threshold = (130, 255, 255)\n            img_mask = cv2.inRange(hsv_img, lower_threshold, upper_threshold)\n\n            self.show_image(img=img_mask, img_name=\"step3_image_mask\")\n\n            filtered_img = cv2.bitwise_and(cropped_img, cropped_img, mask = img_mask)\n\n            # Finding the Image Centroid: (1) \n            m = cv2.moments(img_mask) # (2)!\n            cy = m['m10'] / (m['m00'] + 1e-5)\n            cz = m['m01'] / (m['m00'] + 1e-5) # (3)!\n            cv2.circle(\n                filtered_img,\n                (int(cy), int(cz)),\n                10, (0, 0, 255), 2\n            ) # (4)!\n\n            self.show_image(img=filtered_img, img_name=\"step4_filtered_image\")\n\n            self.waiting_for_image = False\n            cv2.destroyAllWindows()\n\n    def show_image(self, img, img_name, save_img=True):\n\n        self.get_logger().info(\"Opening the image in a new window...\")\n        cv2.imshow(img_name, img)\n\n        if save_img:\n            self.save_image(img, img_name)\n\n        self.get_logger().info(\n            \"IMPORTANT: Close the image pop-up window to exit.\"\n        )\n\n        cv2.waitKey(0)\n\n    def save_image(self, img, img_name):\n        self.get_logger().info(f\"Saving the image...\")\n\n        base_image_path = Path.home().joinpath(\"myrosdata/object_detection/\")\n        base_image_path.mkdir(parents=True, exist_ok=True)\n        full_image_path = base_image_path.joinpath(\n            f\"{img_name}.jpg\")\n\n        cv2.imwrite(str(full_image_path), img)\n\n        self.get_logger().info(\n            f\"\\nSaved an image to '{full_image_path}'\\n\"\n            f\"  - image dims: {img.shape[0]}x{img.shape[1]}px\\n\"\n            f\"  - file size: {full_image_path.stat().st_size} bytes\"\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetection()\n    while node.waiting_for_image:\n        rclpy.spin_once(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>Everything here should be familiar to you from earlier in this exercise, except for this section...</p> </li> <li> <p>Here, we obtain the moments of our colour blob by providing the boolean representation of it (i.e. the <code>img_mask</code>) to the <code>cv2.moments()</code> function.</p> </li> <li> <p>Then, we are determining where the central point of this colour blob is located by calculating the <code>cy</code> and <code>cz</code> coordinates of it.  This provides us with pixel coordinates relative to the top left-hand corner of the image.</p> </li> <li> <p>Finally, this function allows us to draw a circle on our image at the centroid location so that we can visualise it.  Into this function we pass:</p> <ol> <li>The image that we want the circle to be drawn on.  In this case: <code>filtered_img</code>.</li> <li>The location that we want the circle to be placed, specifying the horizontal and vertical pixel coordinates respectively: <code>(int(cy), int(cz))</code>.</li> <li>How big we want the circle to be: here we specify a radius of 10 pixels.</li> <li>The colour of the circle, specifying this using a Blue-Green-Red colour space: <code>(0, 0, 255)</code> (i.e.: pure red in this case)</li> <li>Finally, the thickness of the line that will be used to draw the circle, in pixels.</li> </ol> </li> </ol> <p> \u2190 Back to Part 6 - Exercise 2 </p>"},{"location":"course/assignment2/","title":"Assignment #2: Team Robotics Project","text":""},{"location":"course/assignment2/#overview","title":"Overview","text":"<p>In Assignment #2 you will put into practice everything that you are learning about ROS in Assignment #1, and explore the capabilities of the framework further.</p> <p>You will attend a 2-hour lab session per week in Diamond Computer Room 5 for the full 12-week semester. You will work in teams to develop ROS Nodes for our TurtleBot3 Waffles that allow them to successfully complete a number of robotics tasks in a real-world environment. </p> <p>Assignment #2 is split into two parts: Part A and Part B. You will complete Part A in the first half of the semester (Weeks 1-6) and then move on to Part B in the second half of the semester (Weeks 7-12). </p>"},{"location":"course/assignment2/#the-tasks","title":"The Tasks","text":"Part Tasks Marks(/100) Submission A Task 1: Velocity ControlTask 2: Avoiding Obstacles 2020 Friday of Week 6 at 10pm (GMT) B Task 3: Exploration &amp; SearchTask 4: Documentation 4020 Friday of Week 12 at 10pm (BST) <p>As shown above, there are four tasks in total that you must complete for Assignment #2, worth a total of 100 marks overall. Exact submission deadlines will be stated on Blackboard.</p> <p>Tasks 1, 2 &amp; 3 are programming tasks, requiring you to develop ROS nodes for the Waffles in order for them to complete certain real-world objectives in the robot arena in Computer Room 5. All three of these tasks will be marked based on how well the robot completes each of the objectives. Task 4 will require you to produce some documentation to describe your approach to Task 3.</p>"},{"location":"course/assignment2/#assessment","title":"Assessment","text":"<p>This assignment is worth 30% of the overall mark for COM2009. As a team you will be assessed on a ROS package that you develop to satisfy the above tasks.</p> <p>Your ROS package will be assessed by the teaching team off-line in the weeks following the submission deadlines (as above). You will receive your marks, plus video recordings of the assessment within 3 weeks of submission<sup>1</sup>.</p>"},{"location":"course/assignment2/#tasks-1-2-3","title":"Tasks 1, 2 &amp; 3","text":"<p>Each submission will be assessed by deploying your ROS package on one of the robotics laptops used extensively throughout the lab sessions. Nodes within your package will then be executed on the laptop to control a real robot in the Diamond Computer Room 5 Robot Arena.</p>"},{"location":"course/assignment2/#task-4","title":"Task 4","text":"<p>Your package must contain a <code>README.md</code> file in the root of your package directory. This README file must document the application that you have developed for Task 3. Further details can be found on the Task 4 page. </p>"},{"location":"course/assignment2/#submissions","title":"Submissions","text":"<p>Before you get started on any of the tasks, as a team you'll need to create a single ROS package and host this on GitHub (which you'll do in the Week 1 Lab Session). You can then add all the necessary functionality for each task as you go along. On each of the submission deadlines (as summarised above and detailed on Blackboard) we will pull your team's work from your GitHub repo. See here for further details.</p> <p>Note</p> <p>You should work on each task as a team, and create only one ROS package/GitHub repo per team for this assignment.</p>"},{"location":"course/assignment2/#your-ros-package","title":"Your ROS Package","text":""},{"location":"course/assignment2/#launching-your-code","title":"Launching Your Code","text":"<p>In order to launch the necessary functionality within your package for a given task you will need to include correctly named launch files: <code>task1.launch.py</code>, <code>task2.launch.py</code>, etc. This will allow you to ensure that all the required functionality is executed when your submission is assessed, and also ensures that we know exactly how to launch this functionality in order to assess it. Full details of the requirements for each launch file are provided on the associated task page.</p> <p>For more information on how to create <code>.launch.py</code> files, refer to the following resources:</p> <ol> <li>Assignment #1 Part 3</li> <li>Launch File Arguments (Additional Resources) </li> </ol>"},{"location":"course/assignment2/#preparing-for-the-deadlines","title":"Preparing for the Deadlines","text":"<p>You can find all the Key Information regarding assessment of the programming tasks on this page. It's extremely important that you follow all the Key Requirements outlined here regarding the structure, content and configuration of your ROS package, so please be sure to read this page in full at your earliest convenience!</p> <ol> <li> <p>If this falls over a holiday period (i.e. Easter), then the deadline for marking and the return of marks extends to 5 weeks (as per standard University policy).\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment2/assessment/","title":"Key Assessment Info & Requirements","text":"<p>Warning</p> <p>Failure to follow all the requirements listed on this page could result in penalties being applied to your mark, or zero marks being awarded for a submission point and/or assignment task!</p> <p>Your ROS package should be hosted on GitHub, it should be setup as a private repository, and you should have added <code>tom-howard</code> as a collaborator. </p> <p>In addition to this, you should have registered your package with the teaching team (via the Google Form), so that we know where to find it on the submission deadlines.</p> <p>All the above was covered in the Getting Started section, which you should have completed in the Week 1 Lab.</p> <p>Having completed all of this successfully, we'll be able to pull your package on each of the submission dates so that your team's Assignment #2 work can be assessed. If you haven't completed all this, then you could receive zero marks!</p> <p>Note</p> <p>At some point within the first few weeks of the course a <code>hello.md</code> file will be pushed to your repo (by Tom) to confirm that it has been registered correctly.</p>"},{"location":"course/assignment2/assessment/#submission-points","title":"Submission Points","text":"<p>As discussed here, there are two submission points for Assignment #2 and four tasks to complete overall: </p> <p></p><p></p> Part Tasks Marks(/100) Submission A Tasks 1 &amp; 2 40 Friday of Week 6 at 10pm (GMT) B Tasks 3 &amp; 4 60 Friday of Week 12 at 10pm (BST) <p></p><p></p> <p>See the task pages for full details on each of the four tasks.</p>"},{"location":"course/assignment2/assessment/#dependencies","title":"Dependencies","text":"<p>You may draw upon any pre-existing Python libraries or ROS 2 packages in your own work for Assignment #2 as long as they are pre-installed on the real robotics hardware (i.e. the Linux laptops in the lab). The WSL-ROS2 environment is equivalent to the software setup on the real robotics hardware, so any packages that exist in one will also exist in the other.</p> <p>Note</p> <p>You will not be able to request for any additional libraries/packages to be installed.</p>"},{"location":"course/assignment2/assessment/#key-requirements","title":"Key Requirements","text":"<p>In addition to registering your package correctly (as above), you must also ensure that the following Key Requirements are met for each of the submission points (A and B): </p> <ul> <li> <p> The name of your ROS package must be:</p> <pre><code>com2009_teamXX_2025\n</code></pre> <p>... where <code>XX</code> should be replaced with your team number.</p> </li> <li> <p> It must be possible to build your package by running the following command from the root of the local ROS2 Workspace, and this must build without errors:</p> <pre><code>colcon build --packages-select com2009_teamXX_2025\n</code></pre> </li> <li> <p> You must ensure that a launch file exists for each of the programming tasks (Tasks 1, 2 &amp; 3) and that these are callable (after having run the above <code>colcon build</code> command) so that we are able to launch your work using <code>ros2 launch</code> as follows<sup>1</sup>:</p> <pre><code>ros2 launch com2009_teamXX_2025 taskY.launch.py\n</code></pre> <p>... where <code>XX</code> will be replaced by your team number, and <code>Y</code> will be replaced by the appropriate task number.</p> <p>Important</p> <p>You must ensure that your launch files are named correctly (as detailed in each of the task pages). We won't use any other method to launch your ROS nodes during the assessment of each programming task. </p> </li> <li> <p> Any nodes within your package that are executed by the above launch files must have been correctly defined as package executables (i.e. in your <code>CMakeLists.txt</code>) and must also have been assigned the appropriate execute permission (i.e. with <code>chmod</code>).  </p> <p>Important</p> <p>It's up to you to ensure that your code launches as intended for a given task. If it doesn't, then you'll be awarded zero marks, so make sure you test it all out prior to submission!</p> <p></p> </li> <li> <p> Your package must contain no build files (<code>build/</code>, <code>install/</code>, <code>log/</code>) that would be generated as a result of (incorrectly) running <code>colcon build</code> from inside your package.</p> <p>Remember</p> <p>Always run <code>colcon build</code> from the root of the ROS workspace (e.g. <code>~/ros2_ws/</code>), to ensure that all build files are generated in the right location in the filesystem (<code>~/ros2_ws/build/</code>, <code>~/ros2_ws/install/</code>, <code>~/ros2_ws/log/</code>).</p> </li> <li> <p> On each of the deadlines, we will pull your work from the <code>main</code> branch of your package repository. We will ONLY assess work on your <code>main</code> branch!</p> </li> <li> <p> Your package's <code>package.xml</code> file must contain a <code>&lt;maintainer&gt;</code> tag for each member of your team. Add these as necessary, e.g.:</p> <p></p><pre><code>&lt;maintainer email=\"member.1@sheffield.ac.uk\"&gt;Member 1's Name&lt;/maintainer&gt;\n&lt;maintainer email=\"member.2@sheffield.ac.uk\"&gt;Member 2's Name&lt;/maintainer&gt;\n...\n</code></pre> (providing each team member's full name and Sheffield email address.)<p></p> </li> </ul> <p>For the assessment of each Assignment #2 Task, your package will be built and deployed on one of the Robotics Laptops that you'll have been working with extensively during the lab sessions. We will use the standard <code>student</code> user account, and your package will be downloaded to the <code>~/ros2_ws/src/</code> directory. </p>"},{"location":"course/assignment2/assessment/#other-important-information","title":"Other Important Information","text":"<ul> <li> <p>The <code>tuos_ros</code> Course Repo will be installed and up-to-date on the Robotics Laptop that we use to assess your work with.</p> </li> <li> <p>The Robotics Laptop that we use for the assessment will be selected at random.</p> </li> <li> <p>This laptop will have been paired with a robot prior to us attempting to run your submission.</p> </li> <li> <p>The robot will also be selected at random.</p> </li> <li> <p>We will have already launched the bringup on the robot, so ROS will be up and running, and the robot will be ready to go in the arena.</p> </li> <li> <p>A bridge between the robot and laptop will have already been established, and communications will be tested, prior to us attempting to launch your work for each task.</p> </li> </ul> <ol> <li> <p>Make sure you have defined an appropriate <code>install</code> directory in your package's <code>CMakeLists.txt</code> \u21a9</p> </li> </ol>"},{"location":"course/assignment2/getting-started/","title":"Week 1: Getting Started","text":"<p>Before you get started on Assignment #2 (as detailed in the pages that follow), you should work through the following tasks in your teams during the first lab in Week 1. </p> <ul> <li> Set Up Your Team's ROS Package</li> <li> Get to Know the Robots</li> </ul> <p>The instructions below will guide you through these key first steps.</p>"},{"location":"course/assignment2/getting-started/#set-up-your-teams-ros-package","title":"Set Up Your Team's ROS Package","text":"<p>As discussed on the Assignment #2 Overview, everything that your team submit for this lab assignment must be contained within a single ROS package. Inside this you will develop all the necessary nodes to make a TurtleBot3 Waffle complete each of the assignment tasks, as well as some documentation to describe your approach. Each task will be assessed by the Teaching Team via launch files that you must also provide within your package. </p> <p>The first step however is to create your team's ROS Package.</p> <p>Your team's package will need to be hosted on GitHub, so each team member will need a GitHub account. Head to GitHub and set up an account if you don't already have one<sup>1</sup>.</p>"},{"location":"course/assignment2/getting-started/#creating-your-teams-package-repo-on-github","title":"Creating Your Team's Package Repo (on GitHub)","text":"<p>Nominate only one member of your team to do this bit.</p> <ol> <li>Ensure that you are signed in to your account on GitHub, then go to the <code>ros2_pkg_template</code> Repo. </li> <li> <p>Click on the green <code>Use this template</code> button, and then select <code>Create a new repository</code> from the dropdown menu. </p> <p></p> <p></p> <p>You should then be presented with a Create a new repository screen.</p> </li> <li> <p>The name for your repository must be as follows:</p> <pre><code>com2009_teamXX_2025\n</code></pre> <p>... where <code>XX</code> should be replaced with your COM2009 Assignment #2 Team Number. Enter this in the <code>Repository name</code> box.</p> <p>If your team number is less than 10: put a zero before the number, so that the team number is always 2 digits long, e.g.: </p> <ul> <li><code>com2009_team03_2025</code> for Team 3</li> <li><code>com2009_team08_2025</code> for Team 8</li> <li><code>com2009_team15_2025</code> for Team 15</li> </ul> <p>Important</p> <p>Your repository name should match the above format exactly:</p> <ul> <li>The name should be 19 characters long in total.</li> <li>All characters should be lower case (e.g. <code>com2009</code>, NOT <code>COM2009</code>)</li> </ul> </li> <li> <p>Select <code>Private</code> to make the repository private, then click the green <code>Create repository</code> button. </p> <p></p> <p></p> </li> <li> <p>You'll then be directed to your main repository page. From here, click on <code>Settings</code>, then under <code>Access</code> click <code>Collaborators</code>:</p> <p></p> <p></p> <p>(You may be prompted for 2FA.)</p> </li> <li> <p>In the <code>Manage access</code> area, click the green <code>Add people</code> button and add <code>tom-howard</code>: </p> <p></p> <p></p> </li> <li> <p>Finally, click on the <code>Add people</code> button and add the rest of your team members as collaborators to this repo too.</p> </li> </ol>"},{"location":"course/assignment2/getting-started/#register-your-ros-package-url-with-the-teaching-team","title":"Register Your ROS Package URL with the Teaching Team","text":"<p>Having created your package, you'll need to tell us your GitHub username and the URL to your team's GitHub repository, so that we can access it and pull download your work when the submissions are due.</p> <p>There is a form that you must complete (as a team), to register your ROS package with us for Assignment #2. </p> <p>Access the form here (also available on Blackboard). You must be signed in to your university email account (<code>...@sheffield.ac.uk</code>) to access this. </p> <p>The team member who created the Repo (in the step above) should fill in this form now.</p> <p>Warning</p> <p>Failure to do this (and do it properly) could result in you receiving 0 marks for the assignment tasks!</p> <p>At some point within the first few weeks of the course a <code>hello.md</code> file will be pushed to your repo (by Tom) to confirm that it has been registered correctly.</p>"},{"location":"course/assignment2/getting-started/#initialising-your-teams-ros-package-locally","title":"Initialising Your Team's ROS Package (Locally)","text":"<p>Nominate only one member of your team to do this bit too.</p> <p>You should do this from within your own ROS installation (or WSL-ROS2), rather than on the robotics laptop that you will use to work with the real robots in the lab. Select a team member who has access to their own ROS installation in the lab now (i.e. via a personal laptop), or access WSL-ROS2 using one of the \"WSL-ROS laptops\" that are also available in the lab.</p> <ol> <li> <p>On GitHub, go back to your repository's main page by clicking the <code>&lt;&gt; Code</code> tab at the top-left.</p> </li> <li> <p>Click the green <code>Code</code> button and then, from the dropdown menu, click the  button to copy the remote HTTPS URL of your repo. </p> <p></p> <p></p> </li> <li> <p>From your local ROS installation, open a terminal instance and navigate to the <code>src</code> directory of the ROS Workspace:</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> </li> <li> <p>Clone your repo here using the remote HTTPS URL:</p> <pre><code>git clone REMOTE_HTTPS_URL\n</code></pre> <p>You'll then be asked to enter your GitHub username, followed by a password. This password is not your GitHub account password!  </p> <p>Warning</p> <p>Your GitHub account password won't work here! You'll need to generate a personal access token and use this instead!</p> </li> <li> <p>Navigate into the package directory using the <code>cd</code> command:</p> <pre><code>cd com2009_teamXX_2025\n</code></pre> <p>(...replacing <code>XX</code> with your COM2009 Assignment #2 Team Number.)</p> </li> <li> <p>Then, run an initialisation script to configure your ROS package appropriately:</p> <pre><code>./init_pkg.sh\n</code></pre> </li> </ol>"},{"location":"course/assignment2/getting-started/#git","title":"Configure Git","text":"<p>Next, you'll need to make sure Git is configured properly in your local ROS installation before you do anything else.</p> <ol> <li> <p>From the same terminal instance as above run the following commands to update your personal details in the global Git config file on your machine:</p> <p></p><pre><code>git config --global user.name \"your_name\"\n</code></pre> ...replacing <code>your_name</code> with your actual name! E.g.: <code>git config --global user.name \"John Smith\"</code><p></p> <p></p><pre><code>git config --global user.email \"your_email_address\"\n</code></pre> ...replacing <code>your_email_address</code> with your actual email address!<p></p> </li> <li> <p>If you're working in WSL-ROS2 on a University machine, don't forget to run <code>wsl_ros backup</code> to save these changes to your external WSL-ROS2 backup file, so that they will always be restored whenever you run <code>wsl_ros restore</code> in a fresh WSL-ROS2 instance on another machine. </p> <p>Note</p> <p>All team members will actually need to do this bit before interacting with Git!</p> <p>Regardless of which team member is setting up your team's ROS package to begin with, you'll all need to interact with Git for this assignment, and you should therefore each set up your own individual Git configurations (via the steps above) before working individually on your team's ROS package.</p> </li> </ol>"},{"location":"course/assignment2/getting-started/#git-push","title":"Push Your Local ROS Package Back to GitHub","text":"<p>Again, only one member of your team needs to do this bit.</p> <p>Having initialised your team's ROS package, it's now ready for you to start populating with code for the Assignment #2 Tasks! The first step though is to push the changes made in the initialisation step (above) back to GitHub, so that everyone in your team is working from the right starting point. </p> <ol> <li> <p>From the same terminal as above, use the <code>git status</code> command to show you all the changes that have been made to the repo in the initialisation process:</p> <pre><code>git status\n</code></pre> </li> <li> <p>Use <code>git add</code> to stage all these changes for an initial commit:</p> <pre><code>git add .\n</code></pre> <p>Warning</p> <p>Don't forget the <code>.</code> at the end there!</p> </li> <li> <p>Then commit them:</p> <pre><code>git commit -m \"ROS package initialisations complete.\"\n</code></pre> </li> <li> <p>Finally, push the local changes back up the \"remote\" repository on GitHub:</p> <pre><code>git push origin main\n</code></pre> <p>You'll then be asked to enter your GitHub username and password again. </p> <p>Remember</p> <p>This is not your GitHub account password... Use the personal access token that you created earlier.  </p> </li> <li> <p>All team members should then be able to pull the remote repo into their own ROS Workspaces (<code>cd ~/ros2_ws/src/ &amp;&amp; git clone REMOTE_HTTPS_URL</code>), make contributions and push these back to the remote repo as required (using their own GitHub account credentials and personal access tokens).</p> </li> </ol> <p>You'll need to copy your ROS package onto the Robot Laptops when working on the Real-Robot based tasks, which we'll cover in more detail later. </p>"},{"location":"course/assignment2/getting-started/#getting-to-know-the-real-robots","title":"Getting to Know the Real Robots","text":"<p>Assignment #2 involves extensive work with our real robots, and you'll therefore have access to the robots for every lab session so that you can work on these tasks as you wish. All the details on how the robots work, how to get them up and running and start programming them can be found in the \"Waffles\" section of this course site. You should proceed now as follows (in your teams):</p> <ol> <li>Each team has been assigned a specific robot (there's a list on Blackboard). When you're ready, speak to a member of the teaching team who will provide you with the robot that has been assigned to you.</li> <li> <p>Work through each page of the \"Waffles\" section of this site (in order):</p> <ul> <li> Read about the hardware.</li> <li> Learn how to launch ROS and get the robots up and running.</li> <li> Work through the Waffle (&amp; ROS) Basics, which will help to get you started and understand how ROS and the robots work.</li> <li> There is also some further Essential Information that you must all be aware of when working with the real robots. Work through the further exercises here now.</li> <li> Finally, review the Shutdown Procedures. Follow these steps to shut down the robot and power off the robotics laptop at the end of each lab session.</li> </ul> </li> </ol> <ol> <li> <p>As a University of Sheffield student, you can apply for the GitHub Student Developer Pack, which gives you access to a range of developer tools including GitHub Pro. GitHub Pro allows you to have unlimited collaborators on your repositories, which might help you to collaborate on your ROS package with your team.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/assignment2/ros-pkg-tips/","title":"Working with your ROS Package (in the Lab)","text":"<p>Having followed the instructions on the Getting Started page in Week 1, your team's ROS package will be hosted on GitHub, which makes it much easier to collaborate, and transfer between simulation (e.g. WSL-ROS2, for example) and the real hardware in the lab. </p> <p>You'll need to transfer your ROS package to a robot laptop whenever you want to work on a real robot during the labs. </p>"},{"location":"course/assignment2/ros-pkg-tips/#working-with-git-and-github","title":"Working with Git and GitHub","text":"<p>You'll be working with Git and GitHub quite extensively throughout Assignment #2. Hopefully a lot of you will already be quite familiar with these tools, but if not, we would strongly recommend that you have a look at This Course by the University of Sheffield's Research Software Engineering (RSE) Team.</p>"},{"location":"course/assignment2/ros-pkg-tips/#setting-up-ssh-keys","title":"Setting Up SSH Keys","text":"<p>Using SSH keys, you can clone your team's ROS package to the robot laptops, make commits and push these back up to GitHub during the labs, without needing to provide your GitHub username and a personal access token every time. This makes life a lot easier! The following steps describe the process you should follow to achieve this (adapted from GitHub Docs).</p>"},{"location":"course/assignment2/ros-pkg-tips/#step-0-check-if-you-already-have-an-ssh-key-on-the-laptop","title":"Step 0: Check if you already have an SSH Key on the Laptop","text":"<p>These instructions are adapted from this GitHub Docs page.</p> <p>If you're generating an SSH key for the first time, then you can skip this step and go straight to the next section: \"Step 1: Generating an SSH key (on the Laptop)\". If, however, you've already generated an SSH Key on the laptop previously then check it's still there before you go any further by following these steps...</p> <ol> <li> <p>Open a terminal on the laptop.</p> Tip <p>You can use the Ctrl+Alt+T keyboard combination to open a terminal!</p> </li> <li> <p>Use the <code>ls</code> command as follows, to see if your SSH key already exists on the laptop:</p> <pre><code>ls -al ~/.ssh\n</code></pre> <p>This will provide you with a list of all the SSH keys on the laptop. Your team's key should have the same name as your ROS package (if you followed the steps correctly when you created the key previously), and so you should see your key in the list, i.e.: <code>com2009_teamXX_2025.pub</code>.</p> </li> <li> <p>If your key is there, then you're good to go... Either clone your ROS package onto the Laptop if you deleted it the last time you were in the lab, or just navigate back to it and pull down any updates (<code>git pull</code>) if you left it there.</p> <p>Warning</p> <p>We strongly recommend that you delete your team's package from the laptop at the end of each lab session.</p> </li> <li> <p>If you can't see your key in the list, then you'll need to follow all the steps on this page, starting with Step 1: Generating an SSH key (on the Laptop).</p> </li> </ol>"},{"location":"course/assignment2/ros-pkg-tips/#ssh-keygen","title":"Step 1: Generating an SSH key (on the Laptop)","text":"<ol> <li> <p>From a terminal instance on the laptop navigate to the <code>~/.ssh</code> folder:</p> <pre><code>cd ~/.ssh\n</code></pre> </li> <li> <p>Create a new SSH key on the laptop, using your GitHub email address:</p> <pre><code>ssh-keygen -t ed25519 -C \"your.email@sheffield.ac.uk\"\n</code></pre> <p>Replacing <code>your.email@sheffield.ac.uk</code> with your GitHub email address.</p> <p></p> </li> <li> <p>You'll then be asked to \"Enter a file in which to save the key\". This needs to be unique, so enter the name of your ROS package. For the purposes of this example, let's assume yours is called <code>com2009_team99_2025</code>.</p> </li> <li> <p>You'll then be asked to enter a passphrase. This is how you make your SSH key secure, so that no other teams using the same laptop can access and make changes to your team's package/GitHub repo. You'll be asked to enter this whenever you try to commit/push new changes to your ROS package on GitHub. Decide on a passphrase and share this ONLY with your fellow team members. </p> </li> <li> <p>Next, start the laptop's ssh-agent:</p> <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> </li> <li> <p>Add your SSH private key to the laptop's ssh-agent. You'll need to enter the name of the SSH key file that you created in the earlier step (e.g.: <code>com2009_team99_2025</code>)</p> <pre><code>ssh-add ~/.ssh/com2009_team99_2025\n</code></pre> <p>Replacing <code>com2009_team99_2025</code> with the name of your own SSH key file, of course!</p> </li> <li> <p>Then, you'll need to add the SSH key to your account on GitHub...</p> </li> </ol>"},{"location":"course/assignment2/ros-pkg-tips/#step-2-adding-an-ssh-key-to-your-github-account","title":"Step 2: Adding an SSH key to your GitHub account","text":"<p>These instructions are replicated from this GitHub Docs page.</p> <ol> <li> <p>On the laptop, copy the SSH public key that you created in the previous steps to your clipboard.</p> <p>Do this from a terminal on the laptop, using <code>cat</code>:</p> <pre><code>cat ~/.ssh/com2009_team99_2025.pub\n</code></pre> <p>...replacing <code>com2009_team99_2025</code> with the name of your SSH key file.</p> <p>The content of the file will then be displayed in the terminal... copy it from here.</p> <p>Tips</p> <ol> <li>To copy text from inside a terminal window use Ctrl+Shift+C</li> <li> <p>You could also open the file in VS Code and copy it from there:</p> <pre><code>code ~/.ssh/com2009_team99_2025.pub\n</code></pre> </li> </ol> </li> <li> <p>Go to your GitHub account in a web browser. In the upper-right corner of any page, click your profile photo, then click Settings.</p> </li> <li> <p>In the \"Access\" section of the sidebar, click SSH and GPG keys.</p> </li> <li> <p>Click New SSH key.</p> </li> <li> <p>Enter a descriptive name for the key in the \"Title\" field, e.g. <code>com2009_dia-laptop1</code>.</p> </li> <li> <p>Select <code>Authentication Key</code> as the \"Key Type.\"</p> </li> <li> <p>Paste the text from your SSH Public Key file into the \"Key\" field.</p> </li> <li> <p>Finally, click the \"Add SSH Key\" button.</p> </li> </ol>"},{"location":"course/assignment2/ros-pkg-tips/#ssh-clone","title":"Cloning your ROS package onto the Laptop","text":"<p>With your SSH keys all set up, you can now clone your ROS package onto the laptop. </p> <p>There's a ROS Workspace on each of the robot laptops and (much the same as in your own local ROS environment) your package must reside within this workspace!</p> <ol> <li> <p>From a terminal on the laptop, navigate to the ROS Workspace <code>src</code> directory:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Go to your ROS package on GitHub. Click the Code button and then select the SSH option to reveal the SSH address of your repo. Copy this. </p> </li> <li> <p>Head back to the terminal instance on the laptop to then clone your package into the <code>ros2_ws/src/</code> directory using <code>git</code>:</p> <pre><code>git clone REMOTE_SSH_ADDRESS\n</code></pre> <p>Where <code>REMOTE_SSH_ADDRESS</code> is the SSH address that you have just copied from GitHub.</p> </li> <li> <p>Run Colcon to build your package, which is a three-step process:</p> <ol> <li> <p>Navigate into the root of the ROS Workspace:</p> <pre><code>cd ~/ros2_ws\n</code></pre> </li> <li> <p>Run the <code>colcon build</code> command, targetting your package only:</p> <p></p><pre><code>colcon build --packages-select com2009_team99_2025 --symlink-install\n</code></pre> (...again, replacing <code>com2009_team99_2025</code> with your team's package name.)<p></p> </li> <li> <p>Then, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Navigate into your package and run the following commands to set your identity, to allow you to make commits to your package repo:</p> <p></p><pre><code>cd com2009_team99_2025/\n</code></pre> <pre><code>git config user.name \"your name\"\n</code></pre> <pre><code>git config user.email \"your email address\"\n</code></pre><p></p> </li> </ol> <p>You should then be able to commit and push any updates that you make to your ROS package while working on the laptop, back to your remote repository using the secret passphrase that you defined earlier!</p>"},{"location":"course/assignment2/ros-pkg-tips/#deleting-your-ros-package-after-a-lab-session","title":"Deleting your ROS package after a lab session","text":"<p>Remember that the Robotics Laptops use an account that everyone in the class has access to. You might therefore want to delete your package from the laptop at the end of each lab session. It's very easy to clone it back onto the laptop again by following the steps above at the start of each lab session. Deleting your package (by following the instructions below) won't delete your SSH key from the laptop though, so you won't need to do all that again, and your SSH key will still be protected with the secret passphrase that you set up when generating the SSH Key to begin with (assuming that you are working on the same laptop, of course!) </p> <p>Warning</p> <p>Make sure you've pushed any changes to GitHub before deleting your package!</p> <p>Delete your package by simply running the following command from any terminal on the laptop:</p> <pre><code>rm -rf ~/ros2_ws/src/com2009_teamXX_2025\n</code></pre> <p>... replacing <code>XX</code> with your own team's number!</p>"},{"location":"course/assignment2/ros-pkg-tips/#returning-in-a-subsequent-lab-session","title":"Returning in a Subsequent Lab Session","text":"<p>Your team will be provided with the same Robotics Laptop for each lab session. Having completed all the steps above in a previous lab session, you should be able to return to the laptop, re-clone your package and continue working with relative ease...</p> <ol> <li> <p>The private SSH key that you created in a previous lab session (and secured with a passphrase) should still be saved on the laptop. Check that this is the case by first running the following command:</p> <p></p><pre><code>ls -al ~/.ssh\n</code></pre> If you can see your team's ssh key in the list then you're good to go. If not you'll need to go back here and follow the steps to create it again.<p></p> </li> <li> <p>Next, start the laptop's ssh-agent and re-add your team's private key:</p> <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> <pre><code>ssh-add ~/.ssh/com2009_teamXX_2025\n</code></pre> <p>Replacing <code>XX</code> with your team number.</p> </li> <li> <p>Then, navigate to the ROS Workspace <code>src</code> directory:</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> </li> <li> <p>Clone your package into here, using the SSH address of your package on GitHub:</p> <pre><code>git clone REMOTE_SSH_ADDRESS\n</code></pre> <p>You'll be asked for your secret passphrase, hopefully you remember it!</p> </li> </ol>"},{"location":"course/assignment2/part-a/","title":"Assignment #2 Part A","text":""},{"location":"course/assignment2/part-a/#assignment-2-part-a","title":"Assignment #2 Part A","text":"<p>Total Marks: 40/100</p> <p>Submission: Friday of Week 6 at 10pm (GMT) </p> <p>Tasks:</p> <ul> <li>Task 1: Velocity Control</li> <li>Task 2: Avoiding Obstacles</li> </ul> <p>Ensure that you have prepared appropriately for the Part A submission by following all the instructions here.</p>"},{"location":"course/assignment2/part-a/task1/","title":"Task 1: Velocity Control","text":"<p>Develop a working ROS application to making a real TurtleBot3 Waffle follow a prescribed motion profile, whilst printing key information to the terminal.</p> <p>Course Checkpoints</p> <p>You should aim to have completed the following additional parts of the COM2009 ROS Course to support your work on this task: </p> <ul> <li>Assignment #1: Part 2, up to (and including) Exercise 5.</li> <li>Real Waffle Essential Exercises: Exercise 1 (Publishing Velocity Commands).</li> </ul>"},{"location":"course/assignment2/part-a/task1/#summary","title":"Summary","text":"<p>The main objective of this task is to create a ROS node (or multiple nodes) that make your robot follow a figure-of-eight pattern on the robot arena floor. The figure-of-eight trajectory should be generated by following two loops, both 1 meter in diameter, as shown below. </p> <p> </p> The figure-of-eight path for Task 1. <p>Whilst doing this, you will also need to print some robot odometry data to the terminal at regular intervals (see below for the specifics). In order to get the terminal message formatting right, you might want to have a look at the documentation on Python String Formatting, and refer to any of the code examples that involve printing messages to the terminal in Assignment #1.</p>"},{"location":"course/assignment2/part-a/task1/#details","title":"Details","text":"<ol> <li>The robot must start by moving anti-clockwise, following a circular motion path of 1 m diameter (\"Loop 1,\" as shown in the figure above).</li> <li>Once complete, the robot must then turn clockwise to follow a second circular path, again of 1 m diameter (\"Loop 2\").</li> <li>After Loop 2 the robot must stop, at which point it should be located back at its starting point.</li> <li> <p>The velocity of the robot should be defined to ensure that the whole sequence takes 60 seconds to complete (5 seconds).</p> <p>Note: The timer will start as soon as the robot starts moving.</p> </li> <li> <p>The robot's real-time pose should be printed to the terminal throughout, where messages should be of the following format (exactly): </p> <pre><code>x={x} [m], y={y} [m], yaw={yaw} [degrees].\n</code></pre> <p>Where <code>{x}</code>, <code>{y}</code> and <code>{yaw}</code> should be replaced with the correct real-time odometry data as follows:</p> <ol> <li><code>{x}</code>: the robot's linear position in the X axis, quoted in meters to two decimal places.</li> <li><code>{y}</code>: the robot's linear position in the Y axis, quoted in meters to two decimal places.</li> <li><code>{yaw}</code>: the robot's orientation about the Z axis, quoted in degrees to one decimal place.</li> </ol> <p>The data should be quoted relative to its starting position at the beginning of the task, e.g. at the start of the task (before the robot has moved) the terminal messages should read:</p> <pre><code>x=0.00 [m], y=0.00 [m], yaw=0.0 [degrees].\n</code></pre> <p>These message should be printed to the terminal at a rate of 1Hz. It doesn't matter if the messages continue to be printed to the terminal after the robot has stopped (i.e. after the figure-of-eight has been completed).</p> <p>Important</p> <p>You should use <code>get_logger().info()</code> method calls within your node to print these terminal messages.</p> </li> </ol>"},{"location":"course/assignment2/part-a/task1/#a-note-on-odometry","title":"A note on Odometry","text":"<p>When the robot is placed in the arena at the start of the task its odometry may not necessarily read zero, so you will need to compensate for this. You'll therefore need to grab the robot pose from the <code>/odom</code> topic before your robot starts moving, and then use that as the zero-reference to convert all the subsequent odometry readings that you obtain throughout the task.</p> <p>Odometry and keeping track of the robot's pose is discussed in detail in Assignment #1 Part 2.</p>"},{"location":"course/assignment2/part-a/task1/#launch","title":"Executing Your Code","text":"<p>Your ROS package must contain a launch file called <code>task1.launch.py</code>. When assessing your team's package, the teaching team will use the following command to execute all the necessary functionality from within your package:</p> <pre><code>ros2 launch com2009_teamXX_2025 task1.launch.py\n</code></pre> <p>... where <code>XX</code> will be replaced with your team number.</p> <p>Note</p> <p>ROS will already be running on the robot before we attempt to execute your launch file, and a bridge between the robot and laptop will have already been established.</p>"},{"location":"course/assignment2/part-a/task1/#simulation-resources","title":"Simulation Resources","text":"<p>You might find it helpful to develop your node(s) basic functionality in simulation before testing things out on a real robot. You can use the standard \"Empty World\" environment to do this, which can be launched in using the following command:</p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>For the real task, there will be cylindrical objects placed at the centre of each of the figure-of-eight loops, so your robot will need to move around these as it completes the task. We have therefore also created a simulation environment that is representative of the real world environment during the assessment. This is available in a package called <code>com2009_simulations</code>, which is part of the <code>tuos_ros</code> Course Repo. The instructions for downloading and installing this within your own local ROS installation are available here.</p> <p>If you've already installed this (as part of Assignment #1 perhaps), then it's worth making sure that you have the most up-to-date version (as discussed here).</p> <p>Once you've done all this, then you should be able to launch the Task 1 development arena with the following <code>ros2 launch</code> command:</p> <pre><code>ros2 launch com2009_simulations task1.launch.py\n</code></pre> <p> </p> The Task 1 development arena. <p>Note</p> <p>There won't be any loop markers on the real robot arena floor during the assessment.</p>"},{"location":"course/assignment2/part-a/task1/#marking","title":"Marking","text":"<p>This task will be assessed by the teaching team as part of Part A (i.e. along with Task 2). This will be assessed during the Easter Holiday period, with feedback returned to you before the semester resumes.</p> <p>There are 20 marks available for this task in total, summarised as follows:</p> <p></p><p></p> Criteria Marks Details A: The Motion Path 10/20 How closely the real robot follows a true figure-of-eight path in the robot arena, based on the criteria table below. B: Terminal Messages 10/20 The correct formatting of your odometry messages, and the validity of the data that is presented in the terminal as the robot performs the task, based on the criteria table below. <p></p><p></p>"},{"location":"course/assignment2/part-a/task1/#criterion-a-the-motion-path","title":"Criterion A: The Motion Path","text":"<p>Marks: 10/20</p> <p></p><p></p> Criteria Details Marks A1: Direction of travel The robot must move anticlockwise for the first loop (\"Loop 1\") and then clockwise for the second (\"Loop 2\"). 2 A2: Loop 1 The loop must be ~1 m in diameter, centred about the red beacon. 2 A3: Loop 2 The loop must be ~1 m in diameter, centred about the blue beacon. 2 A4: Stopping Once the robot completes its figure of eight, it must stop with both wheels within 10 cm of the start line. 2 A5: Timing The robot must complete the full figure of eight and stop in 55-65 seconds. 2 <p></p><p></p>"},{"location":"course/assignment2/part-a/task1/#criterion-b-terminal-messages","title":"Criterion B: Terminal Messages","text":"<p>Marks: 10/20</p> <p></p><p></p> Criteria Details Marks B1: Rate Messages should be printed to the terminal at a rate of 1 Hz. 2 B2: Format The messages printed to the terminal should be formatted exactly as detailed above, and must be printed using <code>get_logger().info()</code> method calls. 2 B3: Data Each message value (<code>x</code>, <code>y</code> and <code>yaw</code>) should be plausible, that is: they represent the actual pose of the robot, based on all readings being set to zero at the start/finish point (as illustrated above). In addition, each value must be quoted in the correct units (meters or degrees, as appropriate). 6 <p></p><p></p>"},{"location":"course/assignment2/part-a/task2/","title":"Task 2: Avoiding Obstacles","text":"<p>Develop the ROS node(s) to allow a TurtleBot3 Waffle to autonomously explore an environment containing various obstacles. The robot must explore as much of the environment as possible in 90 seconds without crashing into anything!</p> <p>Course Checkpoints</p> <p>You should aim to have completed the following additional parts of the COM2009 ROS Course to support your work on this task: </p> <ul> <li>Assignment #1: Up to and including Part 5 (in full).</li> <li>Real Waffle Essentials:<ul> <li>Exercise 1 (Publishing Velocity Commands),</li> <li>Exercise 2 (Out of Range LiDAR Data).</li> </ul> </li> </ul>"},{"location":"course/assignment2/part-a/task2/#summary","title":"Summary","text":"<p>Assignment #1 Part 3 introduces the Waffle's LiDAR sensor. This sensor is very useful, as it tells us the distance to any objects that are present in the robot's environment. In Assignment #1 Part 5 we look at how this data, in combination with the ROS Action framework, can be used as the basis for a basic exploration strategy that would incorporate obstacle avoidance. Building on this in Part 5 Exercise 6, we discuss how this could be developed further by developing an action client that could make successive calls to the action server to keep the robot moving randomly, and indefinitely, around an arena whilst avoiding obstacles.</p> <p>This is one approach that you could use for this task, but there are other (and potentially simpler) ways that this could be achieved too. </p> <p>In COM2009 Lecture 3 (\"Sensing, Actuation &amp; Control\"), for instance, you are introduced to Cybernetic Control Principles and some of Braitenberg's \"Vehicles,\" which are discussed and implemented on a Lego robot during the lecture! In particular, \"Vehicle 3b\" might well be relevant to consider as a simple method to achieve an obstacle avoidance behaviour.</p> <p>Another aspect of this task is exploration: your robot will be awarded more marks for navigating around more of the environment. Consider the search strategies that are discussed in Lecture 8 (\"Local Guidance Strategies\"), such as \"Brownian Motion\" and \"Levy Walks.\" Could something along these lines be implemented on the  Waffle?</p>"},{"location":"course/assignment2/part-a/task2/#details","title":"Details","text":"<p>The environment that your robot will need to explore for this will (again) be the Diamond Computer Room 5 Robot Arena, which is a square arena of 4x4m. For the task, the arena will contain a number of \"obstacles,\" i.e.: short wooden walls and coloured cylinders. Your robot will need to be able to detect these obstacles and navigate around them in order to fully explore the space.</p> <ol> <li>The robot will start in the centre of the arena, perpendicular to one of the four outer arena walls.</li> <li> <p>It must explore the environment for 90 seconds without touching any of the arena walls or the obstacles within it.</p> <p>Note: The 90-second timer will start as soon as the robot starts moving within the arena.</p> </li> <li> <p>If the robot makes contact with anything before the time has elapsed then the attempt is over, and this time will be recorded to determine the \"Run Time\" mark (see below).</p> </li> <li>The arena floor will be divided into 16 equal-sized zones and the robot must enter as many of the outer 12 zones as possible during the attempt.</li> <li> <p>The robot must be moving for the entire duration of the task. Simply just turning on the spot for the whole time doesn't count!</p> <ul> <li>What we want to see here is that the robot is constantly making an effort to explore.</li> <li>It is however OK for the robot to stop moving and turn on the spot for a few seconds whenever required though.</li> <li>If the robot explores for a while and then stops and doesn't move again for the remainder of the 90-second run, then Run Time marks will be awarded up to the point at which the robot ceased to be active.</li> <li>Further details on the eligibility for Run Time marks are provided in the Marking Section below.</li> </ul> </li> </ol>"},{"location":"course/assignment2/part-a/task2/#launch","title":"Executing Your Code","text":"<p>The ROS package that you submit must contain a launch file called <code>task2.launch.py</code>, such that the functionality that you develop for Task 2 can be launched from your package via the command:</p> <pre><code>ros2 launch com2009_teamXX_2025 task2.launch.py\n</code></pre> <p>... where <code>XX</code> will be replaced with your team number.</p> <p>Note</p> <p>ROS will already be running on the robot before we attempt to execute your launch file, and a bridge between the robot and laptop will have already been established.</p>"},{"location":"course/assignment2/part-a/task2/#simulation-resources","title":"Simulation Resources","text":"<p>Within the <code>com2009_simulations</code> package there is an example arena which can be used to develop and test out your team's obstacle avoidance node(s) for this task.</p> <p>Info</p> <p>Make sure you check for updates to the Course Repo to ensure that you have the most up-to-date version of these simulations.</p> <p>The simulation can be launched using the following <code>ros2 launch</code> command:</p> <pre><code>ros2 launch com2009_simulations task2.launch.py\n</code></pre> <p></p> <p> </p> The Obstacle Avoidance development arena. <p>This is an example of what the real-world environment might look like. ALL objects (i.e. the four coloured cylinders and the four inner wooden wall assemblies) could be in different positions entirely. The wooden walls may not be touching the outer edges of the arena! The only things that will remain the same are the arena size, the location of the outer arena walls and the floor layout (i.e. the location of the exploration zones).</p>"},{"location":"course/assignment2/part-a/task2/#marking","title":"Marking","text":"<p>There are 20 marks available for Task 2 in total, awarded as follows:</p> <p></p><p></p> Criteria Marks Details A: Run Time 8/20 You will be awarded marks for the amount of time that your robot spends exploring the environment before 90 seconds has elapsed, or the robot makes contact with anything in its environment (as per the table below). The robot must leave the central red zone (a 1x1m area) in order to be eligible for any of these marks. If the robot does not explore beyond the central orange zone then a \\(0.5\\times\\) multiplication factor will be applied to the run time marks. B: Exploration 12/20 You will be awarded 1 mark for each of the outer 12 arena zones that the robot manages to enter (i.e. excluding the four zones in the middle). The robot only needs to enter each of the 12 zones once, but its full body must be inside the zone marking to be awarded the mark. <p></p><p></p>"},{"location":"course/assignment2/part-a/task2/#run-time","title":"Criterion A: Run Time","text":"<p>Marks: 8/20</p> <p>Marks will be awarded as follows:</p> <p></p><p></p> Time (Seconds) Marks 0-9 0 10-19 1 20-29 2 30-39 3 40-49 4 50-59 5 60-89 6 The full 90! 8 <p></p><p></p>"},{"location":"course/assignment2/part-b/","title":"Assignment #2 Part B","text":""},{"location":"course/assignment2/part-b/#assignment-2-part-a","title":"Assignment #2 Part A","text":"<p>Total Marks: 60/100</p> <p>Submission: Friday of Week 12 at 10pm (BST)</p> <p>Tasks:</p> <ul> <li>Task 3: Exploration &amp; Search</li> <li>Task 4: Documentation</li> </ul> <p>Ensure that you have prepared appropriately for the Part B submission by following all the instructions here.</p>"},{"location":"course/assignment2/part-b/task3/","title":"Task 3: Exploration & Search","text":"<p>Develop the ROS node(s) to allow a TurtleBot3 Waffle to autonomously explore a simulated environment equivalent to the Computer Room 5 robot arena, navigating through a series of rooms as quickly as possible whilst documenting its exploration with a photo of a beacon and a map of the environment as it goes! </p> <p>Course Checkpoints</p> <p>You should aim to have completed the following additional parts of the COM2009 ROS Course to support your work on this task: </p> <ul> <li>Assignment #1: All of it, in full!</li> <li>Real Waffle Essentials: Exercises 1-5 (i.e. all of them!)</li> </ul>"},{"location":"course/assignment2/part-b/task3/#summary","title":"Summary","text":"<p>For this task the robot arena will contain a series of \"rooms\" each with a coloured, cylindrical beacon in it. The main aim is to safely explore each of the rooms in the shortest time possible (emphasis on \"safely\" here, meaning you need to also try not to crash into anything in the process!) At the same time, you'll need to search for a beacon of a particular colour as well as documenting your exploration by building a map of the environment (with SLAM).</p>"},{"location":"course/assignment2/part-b/task3/#simulation-resources","title":"Simulation Resources","text":"<p>As with Tasks 1 &amp; 2, there's a simulation to help you develop your code for this task outside the lab sessions. This also helps to illustrate the nature of the task.</p> <p>You can launch the simulation from the <code>com2009_simulations</code> package with the following <code>ros2 launch</code> command:</p> With RobotWithout Robot <pre><code>ros2 launch com2009_simulations task3.launch.py\n</code></pre> <p></p> <p></p> <pre><code>ros2 launch com2009_simulations task3.launch.py with_robot:=false\n</code></pre> <p></p> <p></p> <p>Make sure you check for updates to the course repo to ensure that you have the most up-to-date version of this.</p> <p>Once again, this is just an example of what the real-world simulation environment could look like:</p> <ul> <li>\"Rooms\" will be constructed of 'wooden' walls 180 mm tall, 10 mm thick and either 440 mm or 880 mm in length</li> <li>Each room will contain a cylindrical beacon of 200 mm diameter and 250 mm height</li> <li>Rooms will be different shapes and sizes and in different locations (but there will always be four of them)</li> <li>The robot might not necessarily be located at the same starting point as in the example simulation above, it could start anywhere</li> <li> <p>Beacons will be the same shape, size and colour as those in the example simulation (yellow, red, green and blue). </p> <p>But: detecting colours is a lot harder in the real-world than it is in simulation, so you'll need to do a lot of testing on a real robot if you want to get this working robustly (you will have access to all the beacons during the lab sessions).</p> </li> </ul>"},{"location":"course/assignment2/part-b/task3/#details","title":"Details","text":"<ol> <li>The robot will have 3 minutes (180 seconds) in total to complete this task. The timer will start as soon as the robot starts moving within the arena.</li> <li>The arena floor will be marked out into 9 equal-sized zones. You will be awarded marks for each of the zones that the robot enters within the time available (excluding the one it starts in).</li> <li>In addition to this, the robot will need to try to explore the four rooms that will also be present in the arena. There will be marks available not only for the number of rooms that the robot manages to explore, but also the speed at which it manages to explore them all (see the marking section below for more details).</li> <li> <p>Your robot will need to do this whilst avoiding contact with anything in the environment. </p> <p>Any contact with the environment is referred to as an \"incident.\" Once an incident has taken place, we'll move the robot away slightly so that it is free to move again, but after five incidents have occurred the assessment will be stopped.</p> </li> </ol> <p>Having developed the core functionality for the task, you'll then need to think about a couple of more advanced features...</p>"},{"location":"course/assignment2/part-b/task3/#advanced-feature-1-a-photo-of-a-beacon","title":"Advanced Feature 1: A photo of a beacon","text":"<p>As with the previous 2 tasks, we will launch the ROS node(s) from within your package for this task using <code>ros2 launch</code> (as discussed below). For this one however, we will also attempt to supply an additional argument when we make the command-line call:</p> <pre><code>ros2 launch com2009_teamXX_2025 task3.launch.py target_colour:=COLOUR\n</code></pre> <p>...where <code>COLOUR</code> will be replaced with either <code>yellow</code>, <code>red</code>, <code>green</code> or <code>blue</code> (the target colour will be selected randomly). Based on this input, your robot will need to capture an image of the beacon in the arena of the same colour!</p> <p>At the root of your package there must be a directory called <code>snaps</code>, and the image must be saved into this directory with the file name: <code>target_beacon.jpg</code>. The image that is saved must be the raw image from the robot's camera, and should not include any filtering that you may have applied in post-processing.  </p> <p>You will therefore need to define your launch file to accommodate the <code>target_colour</code> command-line argument. In addition to this, inside your launch file you'll also need to pass the value of this to a ROS node within your package, so that the node knows which beacon to actually look for (i.e. your node needs to know whether to look for a yellow, red, green or blue beacon). This kind of launch file functionality wasn't covered in Assignment #1, so there are some additional resources available here, to help you with this.</p> <p></p> <p>We will test whether your launch file has been correctly built to accept the <code>target_colour</code> command-line argument using the <code>-s</code> option with <code>ros2 launch</code>, which provides a list of all arguments that may be given to the launch file: </p> <pre><code>ros2 launch com2009_teamXX_2025 task3.launch.py -s\n</code></pre> <p>Having built your <code>task3.launch.py</code> file correctly, the <code>target_colour</code> argument should be listed in the output of the above command, e.g.:</p> <pre><code>$ ros2 launch com2009_teamXX_2025 task3.launch.py -s\nArguments (pass arguments as '&lt;name&gt;:=&lt;value&gt;'):\n\n    'target_colour':\n        The colour of the beacon to search for (yellow|red|green|blue). \n</code></pre> <p></p> <p>To illustrate that the value of the <code>target_colour</code> command-line argument has been correctly passed to a ROS Node within your package, you should configure your Node (or any one of your nodes, if you have multiple) to print a message to the terminal as soon as your launch file is called. </p> <p>The message should be formatted exactly as follows:</p> <pre><code>TARGET BEACON: Searching for COLOUR.\n</code></pre> <p>...where <code>COLOUR</code> must be replaced with the actual colour that was passed to your <code>task3.launch.py</code> file (either <code>yellow</code>, <code>red</code>, <code>green</code> or <code>blue</code>). You should use a <code>get_logger().info()</code> method call to print this terminal message.</p>"},{"location":"course/assignment2/part-b/task3/#advanced-feature-2-mapping-with-slam","title":"Advanced Feature 2: Mapping with SLAM","text":"<p>Marks are also available if, whilst your robot is completing this task, you can use SLAM to generate a map of the environment in the background.</p> <p>In Part 3 of Assignment #1 we launched SLAM using the following <code>ros2 launch</code> command:</p> <pre><code>ros2 launch tuos_simulations cartographer.launch.py\n</code></pre> <p>Also in Part 3 we discussed how to use launch files to launch other launch files! Consider how you could take a similar approach to run SLAM from your <code>task3.launch.py</code> file.</p> <p>When it comes to saving the map that has been generated by SLAM, recall how we did this from the command-line in Part 3 Exercise 5, using the following command: </p> <pre><code>ros2 run nav2_map_server map_saver_cli -f MAP_NAME\n</code></pre> <p>It is also possible however to do this programmatically using the ROS 2 Service framework. Consider Assignment #1 Part 4 Exercise 6 for how this could be done from within your Task 3 application.</p> <p>The root of your package directory must contain a directory called <code>maps</code>, and the map file that you obtain must be saved into this directory with the name: <code>arena_map.png</code>.</p>"},{"location":"course/assignment2/part-b/task3/#launch","title":"Executing Your Code","text":"<p>Your team's ROS package must contain a launch file named <code>task3.launch.py</code>, such that (for the assessment) we are able to launch all the nodes that you have developed for this task via the following command:</p> <p></p><pre><code>ros2 launch com2009_teamXX_2025 task3.launch.py target_colour:=COLOUR\n</code></pre> ... where <code>XX</code> will be replaced with your team number and <code>COLOUR</code> will be replaced with either <code>yellow</code>, <code>red</code>, <code>green</code> or <code>blue</code>.<p></p> <p>Note</p> <p>ROS will already be running on the robot before we attempt to execute your launch file, and a bridge between the robot and laptop will have already been established.</p> <p>The simulation environment will already be running before we attempt to execute your launch file.</p>"},{"location":"course/assignment2/part-b/task3/#marking","title":"Marking","text":"<p>There are 40 marks available for this task in total, awarded based on the criteria outlined below.</p> <p></p><p></p> Criteria Marks Details A: Arena exploration 8/40 For this task, the arena will be divided into nine equal-sized zones. You will be awarded 1 mark for each zone that your robot manages to enter, excluding the one it starts within. The robot only needs to enter each zone once, but its full body must be inside the zone marking to be awarded the associated mark. B: Room exploration 12/40 Marks will be awarded based on the maximum number of rooms that your robot manages to explore within the 180-second time limit and the speed by which it does this. The marking details are outlined here. C: An 'incident-free run' 5/40 If your robot completes the task (or the 180 seconds elapses) without it making contact with anything in the arena then you will be awarded full marks here for an incident-free-run! You will however be deducted 1 mark per unique \"incident\" that occurs during the assessment. Your robot must at least leave the zone that it starts in to be eligible for these marks and once five incidents have been recorded in total then the assessment will be stopped. D1: A Photo of a Beacon 10/40 Further details below. D2: Mapping with SLAM 5/40 Further details below. <p></p><p></p>"},{"location":"course/assignment2/part-b/task3/#room-explore","title":"Criterion B: Room exploration","text":"<p>The marks available per room explored will be awarded as follows:</p> <p></p><p></p> Time (seconds) 1 room 2 rooms 3 rooms 4 rooms 150-180 1.0 4.0 7.0 12.0 120-149 1.5 4.5 7.5 12.0 90-129 2.0 5.0 8.0 12.0 60-89 2.5 5.5 9.0 12.0 &lt;60 3.0 6.0 9.0 12.0 <p></p><p></p> <p>The 180-second timer starts as soon as the robot starts moving within the arena.</p>"},{"location":"course/assignment2/part-b/task3/#crit-d1","title":"Criterion D1: A Photo of a Beacon","text":"Criteria Details Marks D1.a Your <code>task3.launch.py</code> file has been built to accept <code>target_colour</code> argument (assessed by running <code>ros2 launch -s</code> on your launch file) and a message is printed to the terminal (using a <code>get_logger().info()</code> method call) to indicate that the correct target colour has been passed to a node in your package (this must occur straight away on executing your launch file and the message format must be exactly as specified here). 2 D1.b At the end of the assessment a single image file, called <code>target_beacon.jpg</code>, has been obtained from the robot's camera (during the course of the assessment), and this is located in a folder called <code>snaps</code> at the root of your package directory i.e.: <code>com2009_teamXX_2025/snaps/target_beacon.jpg</code>. 2 D1.c Your <code>com2009_teamXX_2025/snaps/target_beacon.jpg</code> image file contains any part of the correct beacon. 3 D1.d Your <code>com2009_teamXX_2025/snaps/target_beacon.jpg</code> image file has captured the full width of the correct beacon. 3"},{"location":"course/assignment2/part-b/task3/#crit-d2","title":"Criterion D2: Mapping with SLAM","text":"Criteria Details Marks D2.a By the end of the assessment a map of the robot arena (or any part of it) must have been generated. Two files should exist: a <code>.png</code> and a <code>.yaml</code>, both of which must be called <code>arena_map</code>, and both must be located in a <code>maps</code> folder at the root of your package directory i.e. <code>com2009_teamXX_2025/maps/arena_map.png</code> and <code>com2009_teamXX_2025/maps/arena_map.yaml</code>. 2 D2.b Your <code>com2009_teamXX_2025/maps/arena_map.png</code> file that is created during the assessment is a map that depicts at least one of the rooms of the arena, in full. 3"},{"location":"course/assignment2/part-b/task4/","title":"Task 4: Documentation","text":""},{"location":"course/assignment2/part-b/task4/#summary","title":"Summary","text":"<p>For this task you will write some documentation to describe the application that you have developed for Task 3.</p> <p>Your documentation should be contained within a single <code>README.md</code> file which must sit in the root of your team's package directory, i.e.: </p> <pre><code>com2009_teamXX_2025/README.md\n</code></pre> <p>Important</p> <p>Your documentation file MUST be called <code>README.md</code>, it MUST be in the root of your package directory, and it must be created with Markdown Formatted Text (see below).</p>"},{"location":"course/assignment2/part-b/task4/#content-of-your-readme","title":"Content of your README","text":"<p>Your documentation should contain the following information.</p>"},{"location":"course/assignment2/part-b/task4/#overview","title":"Overview","text":"<p>A brief explanation of what your application does (no more than 100 words).</p>"},{"location":"course/assignment2/part-b/task4/#installation-and-execution","title":"Installation and Execution","text":"<p>Explain (to someone who may not already be familiar) how to install and execute your package on one of the Robotics Laptops in the lab.</p>"},{"location":"course/assignment2/part-b/task4/#exceptions","title":"Exceptions","text":"<ol> <li>In your documentation you can assume that Steps 1-4 of the Robot/Laptop Setup process have already been carried out, so you don't need to discuss any of this.</li> <li>You can also assume (for the purposes of this documentation) that your package is public, so there's no need to include any information about creating ssh keys etc (assume anyone could download your package through a simple <code>git clone ...</code>).</li> </ol>"},{"location":"course/assignment2/part-b/task4/#dependencies","title":"Dependencies","text":"<p>This section should also provide details on all the external packages that your application depends upon in order to function (i.e. any Python/ROS 2 libraries that you are using that exist outside your own package). </p>"},{"location":"course/assignment2/part-b/task4/#functional-description","title":"Functional Description","text":"<p>Explain how your application works. This will form the bulk of the documentation, and should include a Functional Block Diagram (FBD) (or multiple FBDs if you wish) to aid the explanation and illustrate the control logic. This should be more than simply a ROS node/topic graph. </p> <p>See here for information on how to include images in your README.md to ensure that they are rendered correctly (see the information on using relative links to images that exist within your repository).</p>"},{"location":"course/assignment2/part-b/task4/#contributors","title":"Contributors","text":"<p>List all contributing team members and provide links to their GitHub profiles.</p>"},{"location":"course/assignment2/part-b/task4/#word-count","title":"Word Count","text":"<p>Your documentation should be 600-800 words in length, any content after the 800-word limit won't be read (and therefore won't be considered in the assessment either).</p>"},{"location":"course/assignment2/part-b/task4/#formatting","title":"Formatting","text":"<p>Your <code>README.md</code> file should be formatted using GitHub Flavoured Markdown. You can find the basic formatting syntax here, which should be sufficient for the purposes of this task. If you really want to do more however, then the full GitHub Flavoured Markdown Specification can be found here.</p> <p>When marking this task we will view your <code>README.md</code> file on GitHub, so it's important that you check the formatting yourself prior to the Part B deadline. You will lose marks for formatting errors!</p>"},{"location":"course/assignment2/part-b/task4/#marking","title":"Marking","text":"<p>There are 20 marks available for this task in total, distributed as follows.</p> <p></p><p></p> Criteria Marks Details A: Overview 5/20 A clear and concise summary of the application, and a full and correct explanation of how to install and execute it. B: Functional Description 10/20 A detailed and accurate description of how the application works (or was intended to work<sup>1</sup>). A clear yet detailed Functional Block Diagram should be included to support the discussion. C: Formatting and Writing Standard 5/20 Clear, concise and professional writing throughout, which uses technical language appropriately but which is accessible to a none experienced reader. The <code>README.md</code> file must be correctly formatted, i.e. headings, styling, code blocks, other text formatting, figures etc. should all be rendered correctly when viewed directly on GitHub. <p></p><p></p> <ol> <li> <p>The marking of this task is not dependent on your team's performance in Task 3, so even if you score 0/40 marks in Task 3, you could still score 20/20 marks for this task if you document your work well.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/extras/","title":"Additional Resources","text":""},{"location":"course/extras/#additional-resources","title":"Additional Resources","text":"<p>Additional resources to help you in your work on Course Assignments #1 and #2.</p>"},{"location":"course/extras/course-repo/","title":"The TUoS ROS Course Repo","text":"<p>A ROS Metapackage called <code>tuos_ros</code> has been put together to support these courses. This package is available on GitHub here. This repo contains the following ROS packages:</p> Package Name Description <code>tuos_examples</code> Some example scripts to support certain exercises in COM2009 Assignment #1 <code>tuos_interfaces</code> Some custom ROS interfaces to support certain exercises in COM2009 Assignment #1 <code>tuos_simulations</code> Some Gazebo simulations to support certain exercises in COM2009 Assignment #1 <code>tuos_tb3_tools</code> Scripts that run on the real Waffles, and some RViz configs to support real robot work too <code>com2009_simulations</code> Simulation resources to support your development work in COM2009 Assignment #2"},{"location":"course/extras/course-repo/#installing","title":"Installing","text":"<p>The <code>tuos_ros</code> course repo is already installed and ready to go in WSL-ROS, for those who use it, and on the Robotics Laptops. To install the packages in your own local ROS installation, follow the steps here.</p> <ol> <li> <p>Navigate to your ROS Workspace <code>src</code> directory:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Clone the repo from GitHub:</p> <pre><code>git clone -b humble https://github.com/tom-howard/tuos_ros.git\n</code></pre> </li> <li> <p>Navigate back one directory, into the root* of the ROS workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Run <code>colcon build</code> to compile the packages:</p> <pre><code>colcon build --packages-up-to tuos_ros\n</code></pre> </li> <li> <p>Then re-source your <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol>"},{"location":"course/extras/course-repo/#verify","title":"Verify","text":"<p>Once you've installed it, then you can verify that the build process has worked using the following command:</p> <pre><code>colcon_cd tuos_ros\n</code></pre> <p>Which should take you to a directory within the repo that's also called <code>tuos_ros</code>, i.e.:</p> <pre><code>~/ros2_ws/src/tuos_ros/tuos_ros\n</code></pre>"},{"location":"course/extras/course-repo/#updating","title":"Updating","text":"<p>The course repo may be updated every now and again, so its worth checking regularly that you have the most up-to-date version. You can do this by pulling down the latest updates from GitHub using <code>git pull</code>:</p> <pre><code>cd ~/ros2_ws/src/tuos_ros/ &amp;&amp; git pull\n</code></pre> <p>If you see the following message:</p> <pre><code>-bash: cd: tuos_ros/: No such file or directory\n</code></pre> <p>... then go back and make sure you've installed the repo first!</p> <p>Then, run <code>colcon build</code> and re-source your environment:</p> <pre><code>cd ~/ros2_ws &amp;&amp; colcon build --packages-up-to tuos_ros &amp;&amp; source ~/.bashrc\n</code></pre>"},{"location":"course/extras/launch-file-args/","title":"Launch File Arguments","text":""},{"location":"course/extras/launch-file-args/#introduction","title":"Introduction","text":"<p>As we know from the work we've done in Assignment #1, ROS applications can be executed in two different ways:  </p> <ol> <li> <p>Using the <code>ros2 run</code> command:</p> <pre><code>ros2 run {Package name} {Node name}\n</code></pre> </li> <li> <p>Using the <code>ros2 launch</code> command:</p> <pre><code>ros2 launch {Package name} {Launch file}\n</code></pre> </li> </ol> <p>The <code>ros2 launch</code> command, used in combination with launch files, offers a few advantages over <code>ros2 run</code>, for example:</p> <ol> <li>Multiple nodes can be executed simultaneously.</li> <li>From within one launch file, we can call other launch files.</li> <li>We can pass in additional arguments to launch things conditionally, or to change the behaviour of our ROS applications dynamically.</li> </ol> <p>Points 1 and 2 above are explored in Assignment #1 Part 3 (Exercises 1 &amp; 2). In this section we'll explore Point 3 further<sup>1</sup>.</p> <p>Note</p> <p>Make sure you check for updates to the course repo before moving on!</p>"},{"location":"course/extras/launch-file-args/#identifying-launch-arguments","title":"Identifying Launch Arguments","text":"<p>We can use the <code>-s</code> option with <code>ros2 launch</code> to discover the additional arguments that can be supplied to any given launch file. Take the <code>waffle.launch.py</code> launch file from <code>tuos_simulations</code>, for example:</p> <pre><code>ros2 launch tuos_simulations waffle.launch.py -s\n</code></pre> <p>You should be presented with a range of arguments here, starting with:</p> <pre><code>$ ros2 launch tuos_simulations waffle.launch.py -s\nArguments (pass arguments as '&lt;name&gt;:=&lt;value&gt;'):\nurdf_file_name : turtlebot3_waffle.urdf\n\n    'with_gui':\n        Select whether to launch Gazebo with or without Gazebo Client (i.e. the GUI).\n        (default: 'true')\n</code></pre> <p>Scroll to the bottom of the list, and you should see the following:</p> <pre><code>    'x_pose':\n        Starting X-position of the robot\n        (default: '0.0')\n\n    'y_pose':\n        Starting Y-position of the robot\n        (default: '0.0')\n\n    'yaw':\n        Starting orientation of the robot (radians)\n        (default: '0.0')\n</code></pre> <p>Using these arguments, we can control the position and orientation of the Waffle when it is spawned into the simulated world. Try this:</p> <pre><code>ros2 launch tuos_simulations waffle.launch.py x_pose:=1 y_pose:=0.5\n</code></pre> <p>The robot should spawn into an empty world, but at coordinate position \\(x=1.0\\), \\(y=0.5\\), rather than \\(x=0\\), \\(y=0\\), as would normally be the case.</p>"},{"location":"course/extras/launch-file-args/#launching-launch-files-from-within-launch-files","title":"Launching Launch Files from Within Launch Files!","text":"<p>This was covered in Assignment #1 Part 3 Exercise 2, where we learnt how to launch an \"Empty World\" simulation from within our own launch file (and also launch a velocity control node from one of our own packages alongside this).</p>"},{"location":"course/extras/launch-file-args/#passing-launch-arguments","title":"Passing Launch Arguments","text":"<p>How do we pass an argument to a launch file (<code>tuos_simulations/waffle.launch.py</code>, for example) that we are executing from within another launch file? </p> <p>As per Assignment #1 Part 3 Exercise 2, a basic launch file would look like this (in this case configured to launch <code>tuos_simulations/waffle.launch.py</code>):</p> launch_args_example.launch.py<pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\nimport os\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    return LaunchDescription([\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(\n                    get_package_share_directory(\"tuos_simulations\"), \n                    \"launch\", \"waffle.launch.py\"\n                )\n            )\n        )\n    ])\n</code></pre> <p>To launch this and supply the <code>x_pose</code> and <code>y_pose</code> launch arguments to it as well, we need to add the following:</p> launch_args_example.launch.py<pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\nimport os\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    return LaunchDescription([\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(\n                    get_package_share_directory(\"tuos_simulations\"), \n                    \"launch\", \"waffle.launch.py\"\n                )\n            ),\n            launch_arguments={ # (1)!\n                'x_pose': '1.0',\n                'y_pose': '0.5' # (2)!\n            }.items() \n        )\n    ])\n</code></pre> <ol> <li>Arguments are passed to the launch file via the <code>launch_arguments</code> option of <code>IncludeLaunchDescription()</code>.</li> <li> <p>Arguments are passed as a dictionary, which can contain multiple key value pairs separated by commas: <code>dict = {key1:value1, key2:value2, ... }</code>. </p> <p>In this case, keys are the names of the launch arguments to be passed to the <code>waffle.launch.py</code> launch file, and values are the actual values we want to assign to those arguments (and which can be changed as required).</p> </li> </ol>"},{"location":"course/extras/launch-file-args/#a-very-brief-introduction-to-ros-parameters","title":"A Very Brief Introduction to ROS Parameters","text":"<p>To create arguments for our own launch files and to be able to pass these arguments into our own Nodes, we need to use the ROS parameter System.</p> <p>There's a node in the <code>tuos_examples</code> package called <code>param_publisher.py</code>. This node looks for a parameter on the ROS network called <code>word</code>, reads its value and then publishes this to a topic called <code>/chatter</code>.</p> <p>Parameters are used as a way to configure nodes, and can be further used to change their behaviour dynamically during run time. You can read more about ROS Parameters here, and you can also take a look at the code for the <code>param_publisher.py</code> file here, where you'll see that it's fairly straightforward to declare and read parameter values from within a node. </p> <p>If we run this node using <code>ros2 run</code> a default value is used for the <code>word</code> parameter:</p> <pre><code>$ ros2 run tuos_examples param_publisher.py\n[INFO] [####] [param_publisher]: The 'param_publisher' node is running...\n[INFO] [####] [param_publisher]: Publishing the word 'hello'.\n</code></pre> <p>In a different terminal, we can read the values that are being published to the <code>/chatter</code> topic using <code>ros2 topic echo</code>:</p> <pre><code>$ ros2 topic echo /chatter\ndata: hello\n---\ndata: hello\n---\n</code></pre>"},{"location":"course/extras/launch-file-args/#defining-command-line-arguments-for-launch-files","title":"Defining Command Line Arguments for Launch Files","text":"<p>We can create a basic launch file to launch the <code>param_publisher.py</code> file as shown below:</p> <pre><code>from launch import LaunchDescription \nfrom launch_ros.actions import Node \n\ndef generate_launch_description(): \n    return LaunchDescription([ \n        Node( \n            package='tuos_examples', \n            executable='param_publisher.py', \n            name='the_param_publisher_node' \n        )\n    ])\n</code></pre> <p>In order to follow along here, you'll need to create this launch file within one of your own packages. Refer back to Assignment #1 Part 3 for a reminder on how all this works.</p> <p>To define an argument for this launch file, we use the <code>DeclareLaunchArgument</code> action, which must be included as an item in the <code>LaunchDescription</code>:</p> <pre><code>from launch import LaunchDescription \nfrom launch_ros.actions import Node \nfrom launch.actions import DeclareLaunchArgument # (1)!\n\ndef generate_launch_description(): \n    return LaunchDescription([ \n        DeclareLaunchArgument(\n            name='some_word', \n            description=\"A word, any word.\",\n            default_value='Hi'\n        ), # (2)!\n        Node( \n            package='tuos_examples', \n            executable='param_publisher.py', \n            name='the_param_publisher_node' \n        )\n    ])\n</code></pre> <ol> <li>We need to import <code>DeclareLaunchArgument</code> so that we can use it in the launch file.</li> <li>Don't forget the comma to separate the two launch description items: <code>DeclareLaunchArgument()</code> and <code>Node()</code>! </li> </ol> <p>We're defining three things when declaring the launch argument:</p> <ol> <li><code>name</code>: The name of the argument.</li> <li><code>description</code>: A description of what this argument is used for.</li> <li><code>default_value</code>: A value that will be assigned to <code>name</code> if we don't provide one when executing the launch file.</li> </ol>"},{"location":"course/extras/launch-file-args/#passing-launch-file-arguments-to-python-nodes-via-parameters","title":"Passing Launch File Arguments to Python Nodes (via Parameters)","text":"<p>We defined a launch argument in the step above, but (currently) this argument isn't actually being used anywhere. We want to pass this value to our <code>param_publisher.py</code> node via a parameter (as discussed earlier).</p> <p>To do this, we can add an argument to the <code>Node()</code> launch description item:</p> <pre><code>from launch import LaunchDescription \nfrom launch_ros.actions import Node \nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration # (1)!\n\ndef generate_launch_description(): \n    return LaunchDescription([ \n        DeclareLaunchArgument(\n            name='some_word', \n            description=\"A word, any word.\",\n            default_value='Hi'\n        ),\n        Node( \n            package='tuos_examples', \n            executable='param_publisher.py', \n            name='param_publisher_node',\n            parameters=[{'word': LaunchConfiguration('some_word')}] \n        )\n    ])\n</code></pre> <ol> <li>Another new import here!!</li> </ol> <p>Remember that our <code>param_publisher.py</code> node is looking for a parameter called <code>word</code>, and we are passing this in using the value supplied by the launch file argument <code>some_word</code>. So, if we call this launch file without supplying the <code>some_word</code> argument, a default value of <code>Hi</code> will be set (instead of the default value of <code>Hello</code> set by the node itself). You can test this out by running the pre-made <code>cli_example.launch.py</code> launch file also available in the <code>tuos_examples</code> package:</p> <pre><code>$ ros2 launch tuos_examples cli_example.launch.py\n[INFO] [launch]: All log files can be found below xxx\n[INFO] [launch]: Default logging verbosity is set to INFO\n[INFO] [param_publisher.py-1]: process started with pid [####]\n[param_publisher.py-1] [INFO] [####] [param_publisher_node]: The 'param_publisher_node' node is running...\n[param_publisher.py-1] [INFO] [####] [param_publisher_node]: Publishing the word 'Hi'.\n[param_publisher.py-1] [INFO] [####] [param_publisher_node]: Publishing the word 'Hi'.\n</code></pre> <p>Now, do this again but this time supplying the <code>some_word</code> command line argument to the launch file:</p> <pre><code>$ ros2 launch tuos_examples cli_example.launch.py some_word:=goodbye\n[INFO] [launch]: All log files can be found below xxx\n[INFO] [launch]: Default logging verbosity is set to INFO\n[INFO] [param_publisher.py-1]: process started with pid [####]\n[param_publisher.py-1] [INFO] [####] [param_publisher_node]: The 'param_publisher_node' node is running...\n[param_publisher.py-1] [INFO] [####] [param_publisher_node]: Publishing the word 'goodbye'.\n[param_publisher.py-1] [INFO] [####] [param_publisher_node]: Publishing the word 'goodbye'.\n</code></pre> <p>Here, we've seen how we can build a launch file that accepts a command line argument, and how we can pass the value of that command line argument into a ROS node.</p> <ol> <li> <p>For more advanced launch file features, have a look at this guide.\u00a0\u21a9</p> </li> </ol>"},{"location":"ros/","title":"Accessing ROS for this Course","text":"<p>In order to engage with the COM2009 Course Assignments you'll need access to a ROS environment. As discussed here, the course is designed around the eighth version of ROS 2: Humble Hawksbill. </p> <p>ROS can be a bit tricky to install, and is primarily supported on only a handful of \"Tier 1\" operating systems (as listed here), the first choice being Ubuntu 22.04. While Windows 10 is also a supported OS (natively), we strongly recommend actually installing via the Windows Subsystem for Linux (WSL) running an Ubuntu 22.04 distro. In order to install or access ROS for this course, we recommend one of the following options. Click on the relevant link below to access the associated instructions:</p>"},{"location":"ros/#wsl-ros2-windows","title":"WSL-ROS2 (Windows)","text":"<p>We've created our own custom ROS2 (Humble) and Ubuntu (22.04) environment for WSL specifically for this course, which we call \"WSL-ROS2\". The environment contains all the tools and ROS packages that you will need for COM2009. </p> <p>This is our recommended option, and there are two ways to access this: </p> <ol> <li>Install it on your own Windows 10 or 11 machine via the IT Services Software Download Service.</li> <li>Access it on a range of managed desktop computers across the University campus.</li> </ol>"},{"location":"ros/#mac-linux","title":"Mac &amp; Linux","text":"<p>New for 2025</p> <p>We also have a couple of options for accessing ROS on Mac or Linux Machines:</p> <ol> <li>Installing with Docker (Mac, Linux &amp; Windows).</li> <li>UTM (Mac Only).</li> </ol>"},{"location":"ros/other-options/","title":"Other Installation Options","text":""},{"location":"ros/other-options/#other-installation-options","title":"Other Installation Options","text":"<p>WSL-ROS2 is our recommended way to access a ROS2 environment for this course, but if you have a Mac or Linux machine, or if you want to try an alternative option on Windows, then you can find further details here.</p> <ul> <li>docker-ros2 (Linux, Mac, Windows)</li> <li>UTM (Mac)</li> </ul> <p>Note</p> <p>These options aren't (currently) well tested, so we can't guarantee their success... </p>"},{"location":"ros/other-options/docker-ros2/","title":"Installing \"docker-ros2\" for Linux, Mac (and Windows)","text":"<p>Applicable to: Linux, Mac (and Windows) Users</p> <p>Windows Users</p> <p>While this should work on Windows machines, it's currently untested so proceed with caution! (and remember that there's always the WSL-ROS2 option)</p>"},{"location":"ros/other-options/docker-ros2/#acknowledgments","title":"Acknowledgments","text":"<p>This solution has been put together by Atri Hegde, a COM2009 student from 2024 (and a course demonstrator in 2025!)</p> <p>Thanks Atri!!</p>"},{"location":"ros/other-options/docker-ros2/#setup","title":"Setup","text":"<p>See here for the <code>docker-ros2</code> repo and the instructions on how to install it: </p> <p></p>https://github.com/hegde-atri/ros2-docker<p></p>"},{"location":"ros/other-options/docker-ros2/#launch","title":"Launching the ROS Environment","text":""},{"location":"ros/other-options/docker-ros2/#mac-linux","title":"Mac &amp; Linux","text":"<p>Instructions on how to launch the ROS2 docker container are provided in the README, so please consult this for all the details. Essentially though (if you're on a Mac or Linux machine) then you first need to fire up the docker container using:</p> <pre><code>ros_start\n</code></pre> <p>... and once that's done, you need to run the following command to enter the ROS2 environment:</p> <pre><code>ros_shell\n</code></pre> <p>The above assumes that you have already set up your shell appropriately (again see the README).</p>"},{"location":"ros/other-options/docker-ros2/#windows","title":"Windows","text":"<p>The process for Windows users is slightly different. </p>"},{"location":"ros/other-options/utm/","title":"Installing with UTM (on a Mac)","text":"<p>Applicable to: Mac Users</p> <p>This page will be available soon...</p>"},{"location":"ros/using-wsl-ros/","title":"Using WSL-ROS2","text":""},{"location":"ros/using-wsl-ros/#using-wsl-ros2","title":"Using WSL-ROS2","text":"<p>This section contains some information on how to use WSL-ROS2... </p>"},{"location":"ros/using-wsl-ros/linux-term/","title":"A Quick Introduction to the Linux Terminal","text":"<p>You'll work extensively with the Linux Terminal throughout this course. An idle WSL-ROS2 terminal instance will look like this:</p> <p></p> <p>Here, the presence of the <code>$</code> symbol indicates that the terminal is ready to accept a command. Text before the <code>$</code> symbol has two parts separated by the <code>:</code> symbol:</p> <ul> <li> <p>Text to the left of the <code>:</code> tells us the name of the Linux user (\"student\" in this case) followed by the WSL-ROS2 version that you are working with.</p> <p>Note</p> <p>The current WSL-ROS2 version is <code>2425</code>.</p> </li> <li> <p>Text to the right of the <code>:</code> tells us where in the Linux Filesystem we are currently located (<code>~</code> means \"The Home Directory\", which is an alias for the path: <code>/home/student/</code>).</p> </li> </ul> <p>If you don't see the <code>$</code> symbol at all, then this means that a process is currently running. To stop any running process enter Ctrl+C simultaneously on your keyboard.</p>"},{"location":"ros/using-wsl-ros/man-win/","title":"University Managed Desktops","text":""},{"location":"ros/using-wsl-ros/man-win/#prerequisites","title":"Prerequisites","text":"<ul> <li>Accessing WSL-ROS2 on a University Managed Desktop Computer</li> </ul>"},{"location":"ros/using-wsl-ros/man-win/#launching-wsl-ros2","title":"Launching WSL-ROS2","text":"<ol> <li> <p>Click the Windows Start Menu button: </p> </li> <li> <p>Then, start typing <code>wsl-ros</code> and click on the application shortcut that should appear in the list:</p> <p></p> <p></p> <p>Make sure you select WSL-ROS2! </p> <p>You'll then be presented with the following screen:</p> <p></p> <p></p> <p>WSL-ROS2 is now being installed from our custom OS image, which may take a couple of minutes to complete.  Once it's done, the Windows Terminal should automatically launch:</p> <p></p> <p></p> </li> </ol> <p>This is a WSL-ROS2 Terminal Instance!</p>"},{"location":"ros/using-wsl-ros/man-win/#configure-vscode","title":"Configuring Visual Studio Code","text":"<p>Visual Studio Code (or \"VS Code,\" for short) should be installed on all the University of Sheffield Managed Desktop Computers that have WSL-ROS2 pre-installed on. This is a great Integrated Development Environment (IDE) that we'll use extensively throughout the course. You'll first need to make sure it's set up correctly though, so follow the instructions here, in preparation for later.</p> <p>Tip</p> <p>You should only ever need to do this bit once: the configurations should be saved to your user profile, and should be carried over to any other University Desktop Computer that you log into!</p>"},{"location":"ros/using-wsl-ros/man-win/#backing-up-and-restoring-your-data","title":"Backing-Up (and Restoring) your Data","text":"<p>If you're working with WSL-ROS on a university managed desktop machine, the WSL-ROS Environment will only be preserved for a limited time on the machine that you installed it on. As such, any work that you do within WSL-ROS will not be preserved between sessions or across different machines automatically! It's therefore really important that you run a backup script before you close WSL-ROS down. To do so is very easy, simply run the command below from any WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will create an archive of your Home Directory (more detail here) and save it to your University <code>U:</code> Drive. Whenever you launch a fresh WSL-ROS2 Environment again on another day, or on a different machine, simply run the following command to restore your work to it:</p> <pre><code>wsl_ros restore\n</code></pre> <p>To make things a little easier, on launching WSL-ROS the system will check to see if a backup file already exists from a previous session. If it does, then you will be asked if you want to restore it straight away:</p> <pre><code>It looks like you already have a backup from a previous session:\n  U:\\wsl-ros\\ros2-backup-XXX.tar.gz\nDo you want to restore this now? [y/n]\n</code></pre> <p>Enter Y+Enter to restore your data from this backup file, or N+Enter to leave the backup file alone and work from fresh (none of your previous work will be restored). </p>"},{"location":"ros/using-wsl-ros/man-win/#re-launching-the-environment","title":"Re-Launching the Environment","text":"<p>As discussed above, the WSL-ROS2 environment is not preserved on the university managed desktops indefinitely. If you log back in to the same machine within a few hours however, then it may still be there, and you'll be presented with the following message when you launch it:</p> <p></p> <p>Enter Y+Enter to continue where you left things previously, or N+Enter to start from a fresh installation.</p> <p>Warning</p> <p>If you select N then any work that you have created in the existing environment will be deleted! Always make sure you back up your work using the procedure outlined above!</p>"},{"location":"ros/using-wsl-ros/vscode/","title":"VS Code and WSL","text":""},{"location":"ros/using-wsl-ros/vscode/#the-top","title":"Launching VS Code from the Terminal","text":"<ol> <li> <p>You can launch VS Code directly from a WSL-ROS2 terminal instance. Simply type <code>code .</code> at the terminal prompt and then hit Enter :</p> <pre><code>code .\n</code></pre> </li> <li> <p>A warning message may then pop up:</p> <p></p> <p></p> <p>Check the box to \"Permanently allow ...\" and then click the <code>Allow</code> button.</p> </li> <li> <p>VS Code should then launch, and you'll be presented with another trust dialogue:</p> <p></p> <p></p> <p>Click the blue <code>Yes, I trust the authors</code> button.</p> </li> </ol>"},{"location":"ros/using-wsl-ros/vscode/#wsl-ext","title":"Installing the WSL Extension","text":"<ol> <li> <p>The first time you launch VS Code (as above) you should be presented with a pop-up in the bottom-right of the screen, asking if you would like to \"install the recommended 'WSL' extension from Microsoft...\"</p> <p></p> <p></p> <p>Click on the blue \"Install\" button.</p> Don't see the pop-up? <p>You can also install the 'WSL' extension manually.</p> <p>Click on the \"Extensions\" icon in the left-hand toolbar (or hit Ctrl+Shift+X ), type \"wsl\" in the search box and hit the install button on the right extension, as show below:</p> <p></p> <p></p> </li> <li> <p>Once installed, close down VS Code, go back to the WSL-ROS2 terminal instance and re-launch it using the <code>code .</code> command again.</p> <p>This time, you'll be presented with yet another trust pop-up dialogue. Once again, check the box to \"Trust the authors\" and then click the blue <code>Yes, I trust the authors</code> button. </p> </li> <li> <p>You can now navigate the WSL-ROS2 filesystem in the explorer window on the left-hand side of the VS Code screen. You'll need to use this to locate the packages and scripts that you create throughout this course!</p> <p></p> <p></p> </li> </ol>"},{"location":"ros/using-wsl-ros/vscode/#verify","title":"Always make sure that the \"WSL\" extension is enabled!!","text":"<p>Check that your blue \"Remote Window\" icon in the bottom-left of the VS Code screen always looks like this:</p> <p></p> <p>If not, then go back to the top of this page and try again!</p>"},{"location":"ros/wsl-ros/","title":"The WSL-ROS2 Simulation Environment","text":""},{"location":"ros/wsl-ros/#the-wsl-ros2-simulation-environment","title":"The WSL-ROS2 Simulation Environment","text":"<p>To support this course we've created a custom ROS2 (Humble) and Ubuntu (22.04) environment which runs on Windows 10 or 11 using the Windows Subsystem for Linux (WSL). We call this \"WSL-ROS2,\" and you can find out more about it - and how to use it - here.</p> <p></p>"},{"location":"ros/wsl-ros/install/","title":"Installing WSL-ROS2 on your Own (Windows) Machine","text":"<p>Applicable to: Windows 10 or 11 personal (unmanaged) computers</p> <p>You can install WSL-ROS2 (our purpose-built ROS image for this course) via the University of Sheffield Software Download Service (University login required).</p> <p>Note</p> <p>When you download WSL-ROS2 from the Software Download Service you will receive an email with installation instructions. We recommend that you follow the instructions provided on this page instead, as this page will be kept more up-to-date throughout the semester.</p>"},{"location":"ros/wsl-ros/install/#prerequisites","title":"Prerequisites","text":"<ol> <li>Your computer must be running Windows 10 Build 19044 or higher, or Windows 11.</li> <li>Update the GPU drivers for your machine.</li> <li>Install or update WSL:<ol> <li>If you don't already have WSL installed on your machine then follow these instructions to install it.</li> <li>If you do already have WSL installed on your machine, then follow these instructions to update it.</li> </ol> </li> <li>Install the Windows Terminal.</li> <li>Install Visual Studio Code and the WSL VS Code extension.</li> </ol>"},{"location":"ros/wsl-ros/install/#installing","title":"Installing","text":"<ol> <li>Go to the IT Services Software Downloads page (you'll need to log in with your university MUSE credentials).</li> <li> <p>Scroll down to the bottom, where you should see WSL-ROS listed.</p> <p>Click on the blue \"Request WSL-ROS\" button and then wait to receive an email to your university email address.</p> </li> <li> <p>The email will contain a link to two different download locations. </p> <p>Warning</p> <p>Make sure you click on the link for WSL-ROS2, NOT WSL-ROS (this is based on an older ROS version).</p> <p>Click the correct link to download WSL-ROS2 to your machine as a <code>.zip</code> file (~2 GB).</p> </li> <li> <p>On your computer, create a new folder in the root of your <code>C:\\</code> drive called <code>WSL-ROS2</code>.</p> </li> <li>Extract the content of the downloaded <code>.zip</code> file into to the <code>C:\\WSL-ROS2\\</code> folder.</li> <li> <p>Launch PowerShell and enter the follow command to install WSL-ROS2 as a WSL distro on your machine:</p> <pre><code>wsl --import WSL-ROS2 $env:localappdata\\WSL-ROS2 `\nC:\\WSL-ROS2\\wsl-ros2-v2425.01.tar --version 2\n</code></pre> </li> <li> <p>This may take a couple of minutes. Once it's done, you can verify that it was successful with the following command:</p> <pre><code>wsl -l -v\n</code></pre> <p>Where <code>WSL-ROS2</code> should be listed.</p> </li> <li> <p>Next (optional, but recommended), open up the Windows Terminal App, then:</p> <ol> <li>Go to Settings (Ctrl+,)</li> <li>In <code>Startup</code> &gt; <code>Default profile</code> select WSL-ROS2 from the drop-down list.</li> </ol> <p>Alternatively, you could always use our own Windows Terminal Settings file (but note that this will overwrite any of your own custom Windows Terminal App settings, if you have any):</p> <pre><code>Copy-Item -Path C:\\WSL-ROS2\\settings.json -Destination `\n$env:localappdata\\Packages\\Microsoft.WindowsTerminal_8wekyb3d8bbwe\\LocalState\n</code></pre> <p>Either way, this will ensure that each time you open the Windows Terminal App or you press the New Tab () button a WSL-ROS2 Terminal Instance will be launched by default.</p> </li> <li> <p>You should now be able to launch the WSL-ROS2 environment by launching the Windows Terminal App:</p> <p></p> <p></p> </li> </ol>"},{"location":"ros/wsl-ros/install/#initial-setup","title":"Initial Setup","text":"<p>With your WSL-ROS2 terminal instance, you'll need to run some initial commands to get things set up.</p> <ol> <li> <p>First, update the distro:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre> </li> <li> <p>Next, run the following command to attempt to use native Graphical User Interface (GUI) support (which should work if you followed all the prerequisites above):</p> <pre><code>echo \"export XSERVER=false\" &gt; $HOME/.tuos/xserver.sh\n</code></pre> </li> <li> <p>Re-source your <code>.bashrc</code> file for this change to take effect:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Run the following ROS command to see if you can launch the Gazebo GUI, which is crucial for this course:</p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>This should hopefully present you with something like this:</p> <p></p> <p></p> <p>If this doesn't work, then you may need to try using a dedicated X Server (VcXsrv) instead... </p> </li> </ol>"},{"location":"ros/wsl-ros/install/#using-a-dedicated-x-server","title":"Using a Dedicated X Server","text":"<p>Only try this if you were unable to launch the robot simulation in the previous section.</p> <p>If you are unable to run GUI apps (having completed the steps in the section above), then you may need to try using a dedicated X Server instead. In the prerequisites, you should have already installed VcXsrv. </p> <ol> <li>First, install the VcXsrv Windows X Server.</li> <li> <p>Download this config file for VcXsrv and save it to your desktop as <code>wsl_ros_config.xlaunch</code>.</p> <p></p> <p></p> </li> <li> <p>Double click this to launch VcXsrv with the appropriate settings. An icon should then appear in your notification tray to indicate that the X Server is running:</p> <p></p> <p></p> </li> <li> <p>In a WSL-ROS2 Terminal Instance, run the following:</p> <pre><code>echo \"export XSERVER=true\" &gt; $HOME/.tuos/xserver.sh\n</code></pre> </li> <li> <p>Re-source your <code>.bashrc</code> file for this change to take effect:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Try running the empty world Gazebo simulation again:</p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>Important</p> <p>If this option works for you, you'll need to make sure you have the X Server running (by clicking the <code>wsl_ros_config.xlaunch</code> shortcut) every time you work with WSL-ROS2. </p> </li> </ol>"},{"location":"ros/wsl-ros/install/#hardware-acceleration","title":"Hardware Acceleration","text":"<p>Note</p> <p>Only a (potential) option for those who aren't using VcXsrv, and even then, it's not guaranteed that it'll work for everyone!</p> <ol> <li> <p>If your graphics card supports it, it may be possible to also use hardware acceleration for GUI apps. If you want to try this, then run the following command to enable this:</p> <pre><code>echo \"export LIBGL_ALWAYS_SOFTWARE=false\" &gt;&gt; $HOME/.bashrc\n</code></pre> <p>Then - once again - re-source your <code>.bashrc</code> file for this change to take effect:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol>"},{"location":"ros/wsl-ros/install/#see-also","title":"See Also","text":"<ul> <li>Setting up VS Code for WSL</li> <li>A Quick Introduction to the Linux Terminal</li> </ul>"},{"location":"ros/wsl-ros/man-win/","title":"Accessing WSL-ROS2 from A University Managed Desktop Computer on Campus","text":"<p>Applicable to: Students who do not have a Windows 10 or 11 personal computer.</p> <p>The WSL-ROS2 environment is available on a range of managed desktop computers across The University of Sheffield Campus. </p>"},{"location":"ros/wsl-ros/man-win/#diamond","title":"In the Diamond","text":"<p>WSL-ROS2 is pre-installed on all machines in the following Computer Rooms in the Diamond:</p> <ul> <li>Computer Rooms 1, 2, 4 &amp; 6</li> <li>Short Term Loan Laptops in Computer Room 5</li> <li>40 dedicated WSL-ROS laptops (made available in Computer Room 5 during the COM2009 lab sessions)</li> </ul>"},{"location":"ros/wsl-ros/man-win/#other","title":"Other Campus Computer Rooms","text":"<p>In addition to this, WSL-ROS2 can also be installed on all machines in the following additional Campus Computer Rooms (via Software Center):</p> <ul> <li>The Information Commons:<ul> <li>Classrooms 3 and 4</li> <li>Computers on Floors 1, 2, 3, 4 (including Northlights and Pavillion), and Floor 6</li> </ul> </li> <li>Sir Frederick Mappin Building:<ul> <li>Room F110</li> <li>Heartspace Room E095</li> </ul> </li> </ul>"},{"location":"ros/wsl-ros/man-win/#see-also","title":"See Also","text":"<ul> <li>Getting Started with WSL-ROS on the University Managed Desktops </li> <li>A Quick Introduction to the Linux Terminal</li> </ul>"},{"location":"waffles/","title":"Working with the Real TurtleBot3 Waffles","text":""},{"location":"waffles/#working-with-the-real-turtlebot3-waffles","title":"Working with the Real TurtleBot3 Waffles","text":"<p>This section of the Course Site contains a range of resources to support your work with our real TurtleBot3 Waffles in the Diamond. </p> <p></p>"},{"location":"waffles/basics/","title":"Waffle (& ROS) Basics","text":""},{"location":"waffles/basics/#waffle-ros-basics","title":"Waffle (&amp; ROS) Basics","text":"<p>Having completed the steps on the previous page, your robot and laptop should now be paired, and ROS should be up and running (on the robot). You're ready to bring the robot to life! </p> <p>On this page are a series of exercises for you to work through in your teams, to explore how the robots work. We'll also talk through some core ROS concepts and use some key ROS tools, in case you haven't had a chance to explore these in simulation yet.</p>"},{"location":"waffles/basics/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Making the Robot Move</li> <li>Exercise 2: Seeing the Sensors in Action</li> <li>Exercise 3: Visualising the ROS Network</li> <li>Exercise 4: Exploring ROS Topics and Interfaces</li> <li>Exercise 5: Creating A Velocity Control Node (with Python)</li> <li>Exercise 6: Using SLAM to create a map of the environment</li> </ul>"},{"location":"waffles/basics/#manual-control","title":"Manual Control","text":""},{"location":"waffles/basics/#exMove","title":"Exercise 1: Making the Robot Move","text":"<p>There's a very useful ready-made ROS application called <code>teleop_keyboard</code> (from the <code>turtlebot3_teleop</code> package) that we will use to drive a Waffle around. This node works in exactly the same way in both simulation and in the real-world!</p> <ol> <li> <p>Open up a new terminal instance on the laptop either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon, we'll refer to this as TERMINAL 1. In this terminal enter the following <code>ros2 run</code> command to launch the <code>teleop_keyboard</code> node:</p> <p>TERMINAL 1: </p><pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre><p></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around using specific buttons on the keyboard:</p> <p></p> <p></p> <p>Warning</p> <p>Take care to avoid any obstacles or other people in the lab as you do this!</p> </li> <li> <p>Once you've spent a bit of time on this, close the application down by entering Ctrl+C in TERMINAL 1.</p> </li> </ol>"},{"location":"waffles/basics/#packages-and-nodes","title":"Packages and Nodes","text":"<p>ROS applications are organised into packages. Packages are basically folders containing scripts, configurations and launch files (ways to launch those scripts and configurations).  </p> <p>Scripts tell the robot what to do and how to act. In ROS, these scripts are called nodes. ROS Nodes are executable programs that perform specific robot tasks and operations. These are typically written in C++ or Python, but it's possible to write ROS Nodes using other programming languages too.</p> <p>There are two key ways to launch ROS applications:</p> <ol> <li><code>ros2 launch</code></li> <li><code>ros2 run</code></li> </ol> <p>Recall that we just used the <code>ros2 run</code> command in Exercise 1 to launch the <code>teleop_keyboard</code> node. This command has the following structure:</p> <pre><code>ros2 run {[1] Package name} {[2] Node name}\n</code></pre> <p>Part [1] specifies the name of the ROS package containing the functionality that we want to execute. Part [2] is used to specify a single script within that package that we want to execute. We therefore use <code>ros2 run</code> commands to launch single executables (aka Nodes) onto the ROS network (in Exercise 1 for example, we launched the <code>teleop_keyboard</code> node).</p> <p>The <code>ros2 launch</code> command has a similar structure:</p> <pre><code>ros2 launch {[1] Package name} {[2] Launch file}\n</code></pre> <p>Here, Part [1] is the same as the <code>ros2 run</code> command, but Part [2] is slightly different: <code>{[2] Launch file}</code>. In this case, Part [2] is a file within that package that specifies any number of Nodes that we want to launch onto the ROS network. We can therefore launch multiple nodes at the same time from a single launch file.</p>"},{"location":"waffles/basics/#sensors-visualisation-tools","title":"Sensors &amp; Visualisation Tools","text":"<p>Our Waffles have some pretty sophisticated sensors on them, allowing them to \"see\" the world around them. Let's now see what our robot sees, using some handy ROS tools.</p>"},{"location":"waffles/basics/#exViz","title":"Exercise 2: Seeing the Sensors in Action","text":"<ol> <li> <p>There shouldn't be anything running in TERMINAL 1 now, after you closed down the <code>teleop_keyboard</code> node (using Ctrl+C) at the end of the previous exercise. Return to this terminal and enter the following command:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch tuos_tb3_tools rviz.launch.py\n</code></pre><p></p> <p>This will launch an application called RViz, which is a handy tool that allows us to visualise the data from all the sensors on-board our robots. When RViz opens, you should see something similar to the following:</p> <p></p> <p></p> <p>In the bottom left-hand corner of the RViz screen there should be a Camera panel, displaying a live image feed from the robot's camera.</p> No camera images? <ol> <li>In the top-left \"Displays\" panel, scroll down to the \"Camera\" item.</li> <li>Under \"Topic\", find the \"Reliability Policy\" option.</li> <li>From the Drop-down box, change this from \"Best Effort\" to \"Reliable\".</li> </ol> <p></p> <p></p> <p>In the main RViz panel you should see a digital render of the robot, surrounded by lots of green dots. This is a representation of the laser displacement data coming from the LiDAR sensor (the black device on the top of the robot). The LiDAR sensor spins continuously, sending out laser pulses into the environment as it does so. When a pulse hits an object it is reflected back to the sensor, and the time it takes for this to happen is used to calculate how far away the object is.</p> <p>The LiDAR sensor spins and performs this process continuously, so a full 360\u00b0 scan of the environment can be generated. This data is therefore really useful for things like obstacle avoidance and mapping. </p> </li> <li> <p>Place your hand in front of the robot and see if the position of the green dots change to match your hand's location. Move your hand up and down and consider at what height the LiDAR sensor is able to detect it.</p> </li> <li> <p>Then, move your hand closer and further away and watch how the green dots move to match this. </p> </li> <li> <p>Open up a new terminal instance (TERMINAL 2) and launch the <code>teleop_keyboard</code> node as you did in Exercise 1. Watch how the data in the RViz screen changes as you drive the robot around a bit.</p> </li> </ol>"},{"location":"waffles/basics/#exNet","title":"Exercise 3: Visualising the ROS Network","text":"<p>Using <code>ros2 run</code> and <code>ros2 launch</code>, as we have done so far, it's easy to end up with a lot of different processes or ROS Nodes running on the network, some of which we will interact with, but others may just be running in the background. It is often useful to know exactly what is running on the ROS network, and there are a few ways to do this.</p> <ol> <li> <p>Open up a new terminal instance now (TERMINAL 3) and from here use the <code>ros2 node</code> command to list the nodes that are currently running:</p> <p>TERMINAL 3: </p><pre><code>ros2 node list\n</code></pre><p></p> <p>You may notice up to 4 items in the list.</p> </li> <li> <p>We can visualise the connections between the active nodes by using a ROS node called <code>rqt_graph</code>. Launch this as follows:</p> <p>TERMINAL 3: </p><pre><code>rqt\n</code></pre><p></p> <p>A window should then open:</p> <p></p> <p></p> </li> <li> <p>From here, we then want to load the Node Graph plugin. From the top menu select <code>Plugins</code> &gt; <code>Introspection</code> &gt; <code>Node Graph</code>.</p> </li> <li> <p>In the window that opens, select <code>Nodes/Topics (active)</code> from the dropdown menu in the top left. </p> <p>What you should then see is a map of all the nodes in the list from above (as ovals), and arrows to illustrate the flow of information between them. This is a visual representation of the ROS network!</p> <p>Items that have a rectangular border are ROS Topics. ROS Topics are essentially communication channels, and ROS Nodes can read (subscribe) or write (publish) to these topics to access sensor data, pass information around the network and make things happen.</p> <p>If the <code>teleop_keyboard</code> Node is still active (in TERMINAL 2) then this graph should show us that the node is publishing messages to a topic called <code>/cmd_vel</code>, which in turn is being subscribed to by the <code>zenoh_bridge_ros2dds</code> Node. The Zenoh Bridge node handles all communication between the robot and the laptop, so this node is tunnelling the data from <code>/cmd_vel</code> to the robot to make it move.</p> </li> </ol> <p>A ROS Robot could have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to communicate and share data with the other nodes on the ROS Network.</p>"},{"location":"waffles/basics/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. Data is published to topics via message interfaces.</p> <p>Let's have a look at this in a bit more detail...</p>"},{"location":"waffles/basics/#exTopicMsg","title":"Exercise 4: Exploring ROS Topics and Interfaces","text":"<p>Much like the <code>ros2 node list</code> command, we can use <code>ros2 topic list</code> to list all the topics that are currently active on the ROS network.</p> <ol> <li> <p>Close down the RQT Graph window if you haven't done so already. This will release TERMINAL 3 so that we can enter commands in it again. Return to this terminal window and enter the following:</p> <p>TERMINAL 3: </p><pre><code>ros2 topic list\n</code></pre><p></p> <p>A much larger list of items should be printed to the terminal now. See if you can spot the <code>/cmd_vel</code> item in the list.</p> <p>This topic is used to control the velocity of the robot ('command velocity').</p> </li> <li> <p>Let's find out more about this using the <code>ros2 topic info</code> command.</p> <p>TERMINAL 3: </p><pre><code>ros2 topic info /cmd_vel\n</code></pre><p></p> <p>This should provide an output similar to the following: </p> <pre><code>Type: geometry_msgs/msg/Twist\nPublisher count: 1\nSubscription count: 1\n</code></pre> <p>This tells us that the type of data being communicated on the <code>/cmd_vel</code> topic is called: <code>geometry_msgs/msg/Twist</code>. </p> <p>The interface description has three parts:</p> <ol> <li><code>geometry_msgs</code>: The name of the ROS package that this interface belongs to.</li> <li><code>msg</code>: The type of interface. In this case message, but there are other types too. </li> <li><code>Twist</code>: The name of the message interface. </li> </ol> <p>We have just learnt then, that if we want to make the robot move we need to publish <code>Twist</code> messages to the <code>/cmd_vel</code> topic. </p> </li> <li> <p>We can use the <code>ros2 interface</code> command to find out more about the <code>Twist</code> message:</p> <p>TERMINAL 3: </p><pre><code>ros2 interface show geometry_msgs/msg/Twist\n</code></pre><p></p> <p>From this, we should obtain the following:</p> <pre><code>Vector3  linear\n        float64 x\n        float64 y\n        float64 z\nVector3  angular\n        float64 x\n        float64 y\n        float64 z\n</code></pre> <p>Let's find out what it all means...</p> </li> </ol>"},{"location":"waffles/basics/#velocity-control","title":"Velocity Control","text":"<p>The motion of any mobile robot can be defined in terms of its three principal axes: <code>X</code>, <code>Y</code> and <code>Z</code>. In the context of our TurtleBot3 Waffle, these axes (and the motion about them) are defined as follows:</p> <p></p> <p>In theory then, a robot can move linearly or angularly about any of these three axes, as shown by the arrows in the figure. That's six Degrees of Freedom (DOFs) in total, achieved based on a robot's design and the actuators it is equipped with. Take a look back at the <code>ros2 interface show</code> output above. Hopefully it's a bit clearer now that these topic messages are formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <pre><code>Vector3  linear\n        float64 x  &lt;-- Forwards (or Backwards)\n        float64 y  &lt;-- Left (or Right)\n        float64 z  &lt;-- Up (or Down)\nVector3  angular\n        float64 x  &lt;-- \"Roll\"\n        float64 y  &lt;-- \"Pitch\"\n        float64 z  &lt;-- \"Yaw\"\n</code></pre> <p>Our TurtleBot3 robot only has two motors, so it doesn't actually have six DOFs! The two motors can be controlled independently, which gives it what is called a \"differential drive\" configuration, but this still only allows it to move with two degrees of freedom in total, as illustrated below.</p> <p></p> <p>It can therefore only move linearly in the x-axis (Forwards/Backwards) and angularly in the z-axis (Yaw). </p>"},{"location":"waffles/basics/#exSimpleVelCtrl","title":"Exercise 5: Creating A Velocity Control Node (with Python)","text":"<p>Important</p> <p>Before you start this, close down RViz (click the \"Close without saving\" button, if asked) and stop the <code>teleop_keyboard</code> node by entering Ctrl+C in TERMINAL 2.</p> <p>As we've seen, making a robot move with ROS is simply a case of publishing the right ROS Interface (<code>geometry_msgs/msg/Twist</code>) to the right ROS Topic (<code>/cmd_vel</code>). Earlier we used the <code>teleop_keyboard</code> node to drive the robot around, a bit like a remote control car. In the background here all that was really happening was that the node was converting our keyboard button presses into velocity commands and publishing these to the <code>/cmd_vel</code> topic.</p> <p>In reality, robots need to be able to navigate complex environments autonomously, which is quite a difficult task, and requires us to build bespoke applications. We can build these applications using Python, and we'll look at the core concepts behind this now by building a simple node that will allow us to make our robot a bit more \"autonomous\". What we will do here forms the basis of the more complex applications that you will learn about in the lab course!</p> <ol> <li> <p>Above we talked about how ROS Nodes should be contained within packages, so let's create one now using a helper script that we've already put together. (This is covered in more detail in the ROS course, but for the purposes of this exercise let's just go ahead and run the script without worrying too much about it!)</p> <p>In TERMINAL 1, navigate to the <code>tuos_ros</code> Course Repo, which is located in the ROS2 Workspace on the laptop:</p> <pre><code>cd ~/ros2_ws/src/tuos_ros/\n</code></pre> <p>Here you'll find the <code>create_pkg.sh</code> helper script. Run this now using the following command to create a new ROS package called <code>waffle_demo</code>:</p> <pre><code>./create_pkg.sh waffle_demo\n</code></pre> </li> <li> <p>Navigate into this new package directory (using <code>cd</code>):</p> <p>TERMINAL 1: </p><pre><code>cd ../waffle_demo/scripts/ \n</code></pre><p></p> <p>Info</p> <p><code>..</code> means \"go back one directory,\" so that command above is telling <code>cd</code> to navigate out of the <code>tuos_ros</code> directory (and therefore back to <code>~/ros2_ws/src/</code>), and then go into the <code>waffle_demo</code> package directory from there (and then into the <code>scripts</code> directory within that).</p> </li> <li> <p>Here, create a Python file called <code>square.py</code> using the <code>touch</code> command:</p> <p>TERMINAL 1: </p><pre><code>touch square.py\n</code></pre><p></p> </li> <li> <p>You'll need to change the execution permissions for this file in order to be able to run it later on. This is also covered in more depth in the ROS course but, for now, simply run the following command:</p> <p>TERMINAL 1: </p><pre><code>chmod +x square.py\n</code></pre><p></p> </li> <li> <p>Now we need to start editing files in our package, and we'll do that using Visual Studio Code (VS Code). </p> <p>First, use <code>cd</code> to navigate back one directory, to get us back to the root of our package:</p> <p>TERMINAL 1: </p><pre><code>cd ..\n</code></pre><p></p> <p>Verify that you're in the right place by using the <code>pwd</code> command, which should provide the following output:</p> <pre><code>$ pwd\n/home/student/ros2_ws/src/waffle_demo\n</code></pre> <p>Having confirmed that you're in the right place, open up VS Code in this directory:</p> <pre><code>code .\n</code></pre> <p>Note</p> <p>Don't forget to include the <code>.</code> at the end there, it's important!!</p> </li> <li> <p>Next, we need to add our <code>square.py</code> file as an executable to our package's <code>CMakeLists.txt</code>. </p> <p>In VS Code, open the <code>CMakeLists.txt</code> file that is at the root of the <code>waffle_demo</code> package directory (<code>/home/student/ros2_ws/src/waffle_demo/CMakeLists.txt</code>). </p> <p>Locate the lines (near the bottom of the file) that read:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/minimal_node.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> <p>Replace <code>minimal_node.py</code> with <code>square.py</code> to define this as an executable of your package:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/square.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Next, in the VS Code file explorer, open up the <code>scripts</code> directory and click on the <code>square.py</code> file to open it up in the editor.</p> </li> <li> <p>Paste the following content into the <code>square.py</code> file: </p> square.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node # (1)!\nfrom rclpy.signals import SignalHandlerOptions\n\nfrom geometry_msgs.msg import Twist # (2)!\nfrom math import sqrt, pow, pi # (3)!\n\nclass Square(Node): # (4)!\n\n    def __init__(self):\n        super().__init__(\"square\") # (5)!\n\n        self.vel_pub = self.create_publisher(\n            msg_type=Twist,\n            topic=\"cmd_vel\",\n            qos_profile=10,\n        ) # (6)!\n\n        self.state = 1\n        self.change_state = True\n\n        self.vel = Twist() # (7)!\n\n        ctrl_rate = 10 \n        self.timer = self.create_timer(\n            timer_period_sec=1/ctrl_rate,\n            callback=self.timer_callback,\n        ) # (8)!\n\n        self.timestamp = self.get_clock().now().nanoseconds # (9)!\n        self.shutdown = False\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\"\n        )\n\n    def timer_callback(self): # (10)!\n        time_now = self.get_clock().now().nanoseconds\n        elapsed_time = (time_now - self.timestamp) * 1e-9 # (11)!\n        if self.change_state: # (12)!\n            self.timestamp = self.get_clock().now().nanoseconds\n            self.change_state = False\n            self.vel.linear.x = 0.0\n            self.vel.angular.z = 0.0\n            self.get_logger().info(\n                f\"Changing to state: {self.state}\")\n        elif self.state == 1: # (13)!\n            if elapsed_time &gt; 2:\n                self.state = 2\n                self.change_state = True\n            else:\n                self.vel.linear.x = 0.05\n                self.vel.angular.z = 0.0\n        elif self.state == 2: # (14)!\n            if elapsed_time &gt; 4:\n                self.state = 1\n                self.change_state = True\n            else:\n                self.vel.angular.z = 0.2\n                self.vel.linear.x = 0.0\n\n        self.get_logger().info(\n            f\"Publishing Velocities:\\n\"\n            f\"  linear.x: {self.vel.linear.x:.2f} [m/s] | angular.z: {self.vel.angular.z:.2f} [rad/s].\",\n            throttle_duration_sec=1,\n        )\n        self.vel_pub.publish(self.vel) # (15)!\n\n    def on_shutdown(self): # (17)!\n        for i in range(5):\n            self.vel_pub.publish(Twist())\n        self.shutdown = True\n\ndef main(args=None): # (16)!\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    )\n    node = Square()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.on_shutdown()\n    finally:\n        while not node.shutdown:\n            continue\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li><code>rclpy</code> is the ROS client library for Python. We need this (and the <code>Node</code> class from it) in order to build and run Python ROS nodes.</li> <li>We know from earlier that in order to make a robot move we need to publish messages to the <code>/cmd_vel</code> topic, and that this topic uses the <code>geometry_msgs/msg/Twist</code> interface messages. This is how we import that message into the node, in order to create velocity commands in Python (which we'll get to shortly...)</li> <li> <p>Here we're importing some mathematical operators that could be useful... </p> Mathematical Operation Python Syntax \\(\\sqrt{a+b}\\) <code>sqrt(a+b)</code> \\(a^{2}+(bc)^{3}\\) <code>pow(a, 2) + pow(b*c, 3)</code> \\(\\pi r^2\\) <code>pi * pow(r, 2)</code> </li> <li> <p>All the functionality of our Python node is contained within a Class called <code>Square</code>.    </p> </li> <li>When we initialise this class, we provide a name for our node, which is the name that is used to register it on the ROS network. We're calling this one \"square\".</li> <li>Here we are setting up a publisher to the <code>/cmd_vel</code> topic so that the node can write <code>Twist</code> messages to this topic to make the robot move.</li> <li>We're instantiating a <code>Twist</code> message here and calling it <code>vel</code> (we'll assign velocity values to this later on). A <code>Twist</code> message contains six different components that we can assign values to. Any idea what these six values might represent?  </li> <li>We want our node to run at a rate of 10 times per second (10 Hz), so we create a timer object here, set the timer period (<code>timer_period_sec</code>) and then point the object to a \"callback function\" that is defined later on in the code. This callback function will execute at the rate that we specify with <code>timer_period_sec</code>...</li> <li>What time is it right now? (This will help us to keep track of elapsed time in our main timer callback...)</li> <li>Here we're defining our timer callback function. Everything here will execute at the rate that we specified earlier, so we can encapsulate our main control code in here and be confident that it will execute repeatedly (and indefinitely) at our desired rate.</li> <li>Here we're comparing the time now to the time the last time we checked, to tell us how much time has elapsed since then (converting from nanoseconds to seconds by multiplying by <code>1e-9</code>). We'll use that information to decide what the robot should do...</li> <li>This variable is used to stop the robot (if necessary), check the time again, and then move into a new state.</li> <li>In state <code>1</code> we set velocities that will make the robot move forwards (linear-X velocity only). If the elapsed time is greater than 2 seconds however, we move on to state <code>2</code>.</li> <li>In state <code>2</code> we set velocities that will make the robot turn on the spot (angular-Z velocity only). In this case, if the elapsed time is greater than 4 seconds, we move back to state <code>1</code>.</li> <li>Regardless of what happens in the <code>if</code> statements above, we always publish a velocity command to the <code>/cmd_vel</code> topic here (i.e. every time this timer callback executes).</li> <li>The rest of the code here is \"boilerplate\": a standard approach that we'll use to instantiate our nodes and execute them. You'll learn about all of this throughout the lab course.</li> <li>We're defining a class method here that we can call when our node needs to shut down. We're controlling a robot here, so it's important to make sure the robot stops moving when this happens.</li> </ol> <p>Click on the  icons above to expand the code annotations. Read these carefully to ensure that you understand what's going on and how this code works.</p> </li> <li> <p>Having programmed our node and defined it as an executable in our package, we're now ready to build the package so that we can run it. We use a tool called \"Colcon\" to do this, but this MUST be run from the root of the ROS Workspace (i.e.: <code>~/ros2_ws/</code>), so let's navigate there now using <code>cd</code>:</p> <p>TERMINAL 1: </p><pre><code>cd ~/ros2_ws/ \n</code></pre><p></p> <p>Then, use the <code>colcon build</code> command to build your package:</p> <pre><code>colcon build --packages-select waffle_demo --symlink-install\n</code></pre> <p>And finally, \"re-source\" the environment:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Now, run the code.</p> <p>Note</p> <p>Make sure the robot is on the floor and has enough room to roam around before you do this!</p> <p>TERMINAL 1: </p><pre><code>ros2 run waffle_demo square.py\n</code></pre><p></p> <p>Observe what the robot does. When you've seen enough, enter Ctrl+C in TERMINAL 1 to stop the node from running, which should also stop the robot from moving.</p> </li> <li> <p>Now it's time to adapt the code:</p> <p>The aim here is to make the robot follow a square motion path. What you may have observed when you actually ran the code is that the robot doesn't actually do that! We're using a time-based approach to make the robot switch between two different states continuously:</p> <ol> <li>Moving forwards</li> <li>Turning on the spot</li> </ol> <p>Have a look at the code to work out how much time the robot will currently spend in each state.</p> <p>We want the robot to follow a 0.5m x 0.5m square motion path.  In order to properly achieve this you'll need to adjust the timings, or the robot's velocity, or both. Edit the code so that the robot actually follows a 0.5m x 0.5m square motion path!</p> </li> </ol>"},{"location":"waffles/basics/#slam","title":"SLAM","text":"<p>Simultaneous Localisation and Mapping (SLAM) is a sophisticated tool that is built into ROS. Using data from the robot's LiDAR sensor, plus knowledge of how far the robot has moved<sup>1</sup> a robot is able to create a map of its environment and keep track of its location within that environment at the same time. In the exercise that follows you'll see how easy it is to implement SLAM with the Waffle.  </p>"},{"location":"waffles/basics/#exSlam","title":"Exercise 6: Using SLAM to create a map of the environment","text":"<ol> <li> <p>In TERMINAL 1 enter the following command to launch all the necessary SLAM nodes on the laptop:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch turtlebot3_cartographer cartographer.launch.py\n</code></pre><p></p> Tip <p>On the laptop, this command is also available as an alias: <code>tb3_slam</code>!</p> <p>This will launch a new RViz instance, showing a top-down view of the environment, and dots of various colours representing the real-time LiDAR data. Rather than a robot model, the robot is now represented in the environment as a series of links denoted by 3-dimensional red/green/blue crosses (you can turn these off by unchecking the <code>TF</code> option in the left-hand \"Displays\" menu, if you want to). </p> <p></p> <p></p> <p>SLAM will already have begun processing this data to start building a map of the boundaries that are currently visible to the Waffle based on its location in the environment.</p> </li> <li> <p>Return to TERMINAL 2 and launch the <code>teleop_keyboard</code> node. Start to drive the robot around slowly and carefully to build up a complete map of the area.</p> <p>Tip</p> <p>It's best to do this slowly and perform multiple circuits of the area to build up a more accurate map.</p> <p></p> <p></p> <p></p> </li> <li> <p>Once you're happy that your robot has built up a good map of its environment, you can save this map using the <code>map_saver_cli</code> node from a package called <code>nav2_map_server</code>:</p> <ol> <li> <p>First, create a new directory within your ROS package on the laptop. Return to TERMINAL 3 and navigate to the root of the <code>waffle_demo</code> package that you created earlier. We can use the <code>colcon_cd</code> tool to do this now:</p> <p>TERMINAL 3: </p><pre><code>colcon_cd waffle_demo\n</code></pre><p></p> </li> <li> <p>Create a directory in here called <code>maps</code>: </p> <p>TERMINAL 3: </p><pre><code>mkdir maps\n</code></pre><p></p> </li> <li> <p>Navigate into this directory:</p> <p>TERMINAL 3: </p><pre><code>cd maps/\n</code></pre><p></p> </li> <li> <p>Then, use <code>ros2 run</code> to run the <code>map_saver_cli</code> node and save a copy of your robot's map:</p> <p>TERMINAL 3: </p><pre><code>ros2 run nav2_map_server map_saver_cli -f MAP_NAME\n</code></pre><p></p> <p>Replacing <code>MAP_NAME</code> with an appropriate name for your map. This will create two files: </p> <ol> <li>a <code>MAP_NAME.pgm</code> </li> <li>a <code>MAP_NAME.yaml</code> file</li> </ol> <p>...both of which contain data related to the map that you have just created.</p> </li> <li> <p>The <code>.pgm</code> file can be opened using an application called <code>eog</code> on the laptop: </p> <p>TERMINAL 3: </p><pre><code>eog MAP_NAME.pgm\n</code></pre><p></p> </li> </ol> </li> <li> <p>Return to TERMINAL 1 and close down SLAM by pressing Ctrl+C. The process should stop and RViz should close down.</p> </li> <li> <p>Close down the <code>teleop_keyboard</code> node in TERMINAL 2 as well, if that's still running.</p> </li> </ol>"},{"location":"waffles/basics/#next-steps","title":"Next Steps","text":"<p>\"Pro Tips\": There are some important things to consider when working with the Real Waffles. Move onto the next page to find out more...</p> <p>... and when you've done that, don't forget to power off your robot properly. </p> <ol> <li> <p>You'll learn much more about \"Robot Odometry\" in the lab course.\u00a0\u21a9</p> </li> </ol>"},{"location":"waffles/essentials/","title":"Further (Essential) Exercises","text":"<p>By working through the additional short exercises below, you will become aware of the key differences in how our TurtleBot3 Waffles work in the real world, compared to how they work in simulation. Be mindful of the differences that we are trying to highlight here and the implications that they will have on the applications that you develop. </p> <p>As a general rule, when developing code for real-world applications, it's a good idea to get the basics working in simulation first, but always test out your code in the real-world... just because it works in simulation, doesn't automatically mean it will work on the real robots!!</p>"},{"location":"waffles/essentials/#ex1","title":"Essential Exercise 1: Publishing Velocity Commands","text":"<p>This one actually applies to both the real Waffles and the simulation too. The issue is more critical when working with real thing though, since we're working with real hardware in a real-world environment, where things could get damaged or people could get hurt. </p> <p>In the previous exercises you made the robot move using the <code>teleop_keyboard</code> node, and also built a Python node to control the robot's velocity. Ultimately, making a robot move is achieved by publishing velocity commands (i.e. <code>geometry_msgs/msg/Twist</code> interface messages) to the <code>/cmd_vel</code> topic. Another way to do this is by using the <code>ros2 topic pub</code> command in a terminal. Run the following command and observe what the robot does:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.2\"\n</code></pre> <p>... the robot should turn on the spot.</p> <p>Enter Ctrl+C now to stop the <code>ros2 topic pub</code> command, what happens now?</p> <p>... the robot continues to turn on the spot!</p> <p>In order to actually stop the robot, we need to run the command again, but this time set all the velocities back to zero:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.0\"\n</code></pre>"},{"location":"waffles/essentials/#why-does-this-matter","title":"Why Does This Matter?","text":"<p>If we don't issue a stop command to a robot that is moving, then it will continue to move with the last velocity command that was sent to it. </p> <p>The same applies to ROS nodes when we shut them down: if a stop command is not sent to <code>cmd_vel</code> before the node stops running (i.e. via Ctrl+C) the robot will continue to move, which clearly isn't very good!</p> <p>You must ensure that your nodes are programmed with correct shutdown procedures, to ensure that the robot actually stops moving when a node is terminated. This is covered in Part 2 of Assignment #1, but you can also see this in action in the Velocity Control Node that we created in the previous section. </p>"},{"location":"waffles/essentials/#ex2","title":"Essential Exercise 2: Out of Range LiDAR Data","text":"<p>The robot's LiDAR sensor can only obtain measurements from objects within a certain distance range. In Part 3 we look at how to work out what this range is, using the <code>ros2 topic echo</code> command. Let's apply the same techniques to the real robot now to discover the maximum and minimum distances that the real robot's LiDAR sensor can measure:</p> <p></p><pre><code>ros2 topic echo /scan --field range_min --once\n</code></pre> <pre><code>ros2 topic echo /scan --field range_max --once\n</code></pre><p></p> <p>The LiDAR sensor's measurement range is the same in simulation and on the real robots, but how out-of-range values are reported is different!</p> <p>If the LiDAR sensor detects an object that falls within this range then it will report the exact distance to this object (in meters). Conversely, if it doesn't detect anything within this range then it will report a default out-of-range value instead. In simulation, the out-of-range value is <code>inf</code>.</p> <p>Warning</p> <p>The out-of-range value reported by the real robot's LiDAR sensor is not <code>inf</code>!</p> <p>Use the <code>ros2 topic echo</code> command to view the raw LiDAR data:</p> <pre><code>ros2 topic echo /scan --field ranges\n</code></pre> <p>See if you can position the robot in the environment so that some LiDAR measurements will fall outside the measurement range that you have just determined. How are these values reported in the terminal? </p>"},{"location":"waffles/essentials/#why-does-this-matter_1","title":"Why Does This Matter?","text":"<p>In Part 3 of Course Assignment #1 we illustrate how the LiDAR <code>ranges</code> array can be filtered to remove any out-of-range values: </p> <pre><code>valid_data = front[front != float(\"inf\")] # (1)!\n</code></pre> <ol> <li>Assuming <code>front</code> is a <code>numpy</code> array (see Part 3).</li> </ol> <p>If you apply the same technique to a real-world application, then this filtering will be ineffective, because out-of-range values are not <code>inf</code> here... You'll need to adapt this for real-world scenarios<sup>1</sup>.</p>"},{"location":"waffles/essentials/#ex3","title":"Essential Exercise 3: The Camera Image Topic","text":"<p>In Part 6 of Assignment #1 we work extensively with the robot's camera and the processing of the images that are captured by it. This is done in simulation (of course), where the image data is published to a topic called <code>/camera/image_raw</code>. The name of the camera image topic is not the same on the real robots!</p> <p>With the real robot to hand now, use ROS command-line tools such as <code>ros2 topic list</code> and <code>ros2 topic info</code> to interrogate the real robot ROS Network and identify the name of the camera image topic used by the real robot.</p>"},{"location":"waffles/essentials/#why-does-this-matter_2","title":"Why Does This Matter?","text":"<p>You'll likely do a lot of development work for your real-robot applications in simulation, outside the lab sessions. Some of these applications may involve camera data and image processing. If you set up a subscriber to a topic that doesn't exist, then ROS will not warn you about it! It will simply sit quietly and wait, assuming that the topic will (sooner or later) become available. As a result, if you are running an application on a real robot, that is subscribing to image data on the <code>/camera/image_raw</code> topic, then your application will never receive any image data and any associated callback functions will never execute!<sup>2</sup></p>"},{"location":"waffles/essentials/#ex4","title":"Essential Exercise 4: Camera Image Resolution","text":"<p>In Part 6 of Assignment #1 we also explore how images can be reduced in size by cropping them, to make the data processing a bit quicker and easier. We need to be aware of the original image size (i.e. resolution) however, in order to apply cropping techniques appropriately. </p> <p>As we learn in Part 6, it's possible to discover the resolution of each image that is published to the camera image topic by echoing the <code>height</code> and <code>width</code> parameters that are published as part of the image message interface (alongside the image data itself). We can of course do this now with <code>ros2 topic echo</code>, to determine the resolution of the camera images obtained on the real robot:</p> <pre><code>ros2 topic echo /camera/color/image_raw --field height --once\n</code></pre> <pre><code>ros2 topic echo /camera/color/image_raw --field width --once\n</code></pre> <p>The outputs here will indicate the <code>height</code> and <code>width</code> of the images, in pixels. See how these compare with values that you obtain from the simulation, when you get to Part 6 of the Course.</p> <p>Warning</p> <p>The real robot's camera captures images at a lower image resolution! </p>"},{"location":"waffles/essentials/#why-does-this-matter_3","title":"Why Does This Matter?","text":"<p>We'll learn a lot about image cropping and other image processing techniques through simulation, but (as above) the native image resolution of the simulated robot's camera is much larger than that of the real robot. As such, if you apply the same cropping techniques to real-world applications, without adjustment, then you will end up cropping too much of the image out, leaving nothing to actually apply any further processing to!<sup>3</sup></p>"},{"location":"waffles/essentials/#ex5","title":"Essential Exercise 5: Object Detection","text":"<p>In general, image detection gets a little more challenging in the real-world, where the same object might appear (to a robot's camera) to have slightly different colour tones under different light conditions, from different angles, in different levels of shade, etc. In simulation (again in Part 6 of the Course), you may build an extremely effective <code>colour_search.py</code> node to detect each of the four coloured pillars in the <code>tuos_simulations/coloured_pillars</code> world, but this might not perform as well in the real world without some fine-tuning</p>"},{"location":"waffles/essentials/#why-does-this-matter_4","title":"Why Does This Matter?","text":"<p>Always test out your code in the real-world, just because it works in simulation, doesn't mean it will work on the real robots!!</p>"},{"location":"waffles/essentials/#summary","title":"Summary","text":"<p>You will naturally do a fair bit of development work in simulation, where it's easier to test things out and less disastrous if things go wrong! Overall, you'll be able to develop things much faster this way, and you can do this outside of your weekly lab sessions too. Whilst you're doing this though, keep in mind all the differences that we have identified above, so that there are less nasty surprises when you come to deploy your ROS applications in the real world. </p> <p>Throughout the design phase, think about how your applications could be developed more flexibly to accommodate these variations, or how you could design things so that only small/quick changes/switches need to be made when you transition from testing in simulation, to deploying on a real Waffle. </p> <ol> <li> <p>Exercise 2 Hint: Out-of-range values on the real robots are actually reported as <code>0.0</code>!\u00a0\u21a9</p> </li> <li> <p>Exercise 3 Hint: On the real robots, the camera image topic is <code>/camera/color/image_raw</code>!\u00a0\u21a9</p> </li> <li> <p>Exercise 4 Hint: In simulation, camera images have a resolution of 1080x1920 pixels, whereas on the real robots the resolution is 640x480 pixels.\u00a0\u21a9</p> </li> </ol>"},{"location":"waffles/intro/","title":"Introduction","text":""},{"location":"waffles/intro/#introduction","title":"Introduction","text":""},{"location":"waffles/intro/#handling-the-robots","title":"Handling the Robots","text":"<p>Health and Safety</p> <p>Everyone needs to complete a health and safety quiz related to working with the real robots. This quiz (and the deadline for completing it) is available on Blackboard.</p> <p>Make sure at least one member of your team has completed this before proceeding!</p> <p></p> <p>As you can see from the figure above, the robots have lots of exposed sensors and electronics, so you must take great care when handling them to avoid the robots becoming damaged in any way.  When handling a robot, always hold it by either the black Waffle Layers, or the vertical Support Pillars (as highlighted in the figure above).</p> <p>Important</p> <p>Do not pick the robot up or carry it by the camera or LiDAR sensor! These are delicate devices that could be easily damaged!</p> <p>A lot of the robot's electronics are housed on the middle waffle layer. Try not to touch any of the circuit boards, and take care not to pull on any of the cabling or try to remove or rehouse any of the connections. If you have any concerns with any of the electronics or cabling, if something has come loose, or if your robot doesn't seem to be working properly then ask a member of the teaching team to have a look for you.</p> <p>The robots will be provided to you with a battery already installed and ready to go. Don't try to disconnect or remove the battery yourselves! The robot will beep when the battery is low, and if this happens ask a member of the team to get you a replacement (we have plenty).</p>"},{"location":"waffles/intro/#laptops","title":"The Robot Laptops","text":"<p>You'll be provided with one of our pre-configured Robot Laptops in the lab when working with the real Waffles. These Laptops (and the Robots) run Ubuntu 22.04 with ROS 2 Humble. </p> <p>There's a \"student\" user account already set up, and you'll need to use this when working in the lab. The laptop should log you into this user account automatically on startup, but we'll provide you with the account password as well, during the lab sessions, should you need it.</p>"},{"location":"waffles/intro/#network","title":"Network","text":"<p>The Robots and Laptops must be able to connect to one another over an internet connection. The robots connect to a dedicated wireless network running in the Diamond called 'DIA-LAB'. In order for the laptops to be able to \"see\" the robots, they must be connected to the university network using any of the following options:</p> <ol> <li> <p>A wired (ethernet) connection (Recommended)</p> <ul> <li>(subject to provision of network sockets in CR5 by February 2025)</li> </ul> </li> <li> <p>The eduroam WiFi SSID</p> </li> <li>The DIA-LAB WiFi SSID (no internet access!)</li> </ol> <p>WiFi credentials for DIA-LAB and eduroam have already been set on the laptops, allowing you to connect to either network straight away, but speak to a member of the teaching team if you are having any issues.</p>"},{"location":"waffles/intro/#vs-code","title":"VS Code","text":"<p>Visual Studio Code is installed on the laptops for you to use when working on your ROS applications for the assignment tasks. Launch VS Code from any terminal by simply typing <code>code</code>. You can also launch it by clicking the icon in the favourites bar on the left-hand side of the screen:</p> <p></p>"},{"location":"waffles/launching-ros/","title":"Launching ROS (and Pairing the Laptop)","text":"<p>The first step is to launch ROS on the Waffle.</p> <p>Important</p> <p>This ensures that all the core ROS functionality is executed on the robot, without this the robot won't be able to do anything!</p>"},{"location":"waffles/launching-ros/#step-1-identify-your-waffle","title":"Step 1: Identify your Waffle","text":"<p>Robots are named as follows:</p> <pre><code>dia-waffleX\n</code></pre> <p>... where <code>X</code> is the 'Robot Number' (a number between 1 and 50). Make sure you know which robot you are working with by checking the label printed on top of it!</p>"},{"location":"waffles/launching-ros/#step-2-pairing-your-waffle-to-a-laptop","title":"Step 2: Pairing your Waffle to a Laptop","text":"<p>As discussed earlier, you'll be provided with one of our Robotics Laptops to work with in the lab, and the robot needs to be paired with this in order for the two to work together.  </p> <ol> <li> <p>Open up a terminal instance on the laptop, either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon in the favourites bar on the left-hand side of the desktop:</p> <p></p> <p></p> </li> <li> <p>We'll use our purpose-built <code>waffle</code> CLI to handle the pairing process. Run this in the terminal by entering the following command to pair the laptop and robot:</p> <p></p><pre><code>waffle X pair\n</code></pre> Replacing <code>X</code> with the number of the robot that you are working with.<p></p> </li> <li> <p>You may see a message like this early on in the pairing process:</p> <p></p> <p></p> <p>If so, just type <code>yes</code> and then hit Enter to confirm that you want to continue.</p> </li> <li> <p>Enter the password for the robot when requested (we'll tell you what this is in the lab!)</p> <p>Note</p> <p>You won't see anything change on the screen when you are entering the password. This is normal, just keep typing!!</p> </li> <li> <p>The pairing process will take a minute, but once it's finished you should see a message saying <code>pairing complete</code>, displayed in blue in the terminal. </p> </li> <li> <p>Then, in the same terminal, enter the following command: </p> <p></p><pre><code>waffle X term\n</code></pre> (again, replacing <code>X</code> with the number of your robot).<p></p> <p>A green banner should appear across the bottom of the terminal window:</p> <p></p> <p></p> <p>This is a terminal instance running on the robot, and any commands that you enter here will be executed on the robot (not the laptop!)</p> </li> </ol>"},{"location":"waffles/launching-ros/#step-3-launching-ros","title":"Step 3: Launching ROS","text":"<p>Launch ROS on the robot by entering the following command:</p> <pre><code>tb3_bringup\n</code></pre> <p>If all is well then the robot will play a nice \"do-re-me\" sound and a message like this should appear (amongst all the other text):</p> <pre><code>[tb3_status.py-#] ######################################\n[tb3_status.py-#] ### dia-waffleX is up and running! ###\n[tb3_status.py-#] ######################################\n</code></pre> <p>You shouldn't need to interact with this terminal instance any more now, but the screen will provide you with some regular real-time info related to the status of the robot. As such, keep this terminal open in the background and check on the <code>Battery</code> indicator every now and then:</p> <pre><code>Battery: 12.40V [100%]\n</code></pre> <p>Low Battery </p> <p>The robot's battery won't last a full 2-hour lab session!!</p> <p>When the capacity indicator reaches around 15% then it will start to beep, and when it reaches ~10% it will stop working all together.  Let a member of the teaching team know when the battery is running low and we'll replace it for you. (It's easier to do this when it reaches 15%, rather than waiting until it runs below 10%!)</p>"},{"location":"waffles/launching-ros/#step-4-robot-laptop-bridging","title":"Step 4: Robot-Laptop 'Bridging'","text":"<p>The Waffle and laptop both communicate over the University network via a Zenoh Bridge. The bridge should already be running on the robot after having run the <code>tb3_bringup</code> command in Step 3 above. </p> <p>The next crucial step is to establish a connection to this bridge from the laptop, so that all ROS nodes, topics etc. can flow between the two devices as necessary. </p> <p>This is Essential!</p> <p>You always need to have the bridge running on the laptop in order to be able to communicate with your robot!</p> <p>Open up a new terminal instance on the laptop (either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon) and enter the following command:</p> <p></p><pre><code>waffle X bridge\n</code></pre> You should now have two terminals active: <p></p> <ol> <li>The robot terminal where you ran <code>tb3_bringup</code> to launch ROS in Step 3<sup>1</sup></li> <li>The laptop terminal where you just ran the <code>bridge</code> command</li> </ol> <p>Leave both of these terminals alone, but keep them running in the background at all times while working with your robot.</p>"},{"location":"waffles/launching-ros/#shutting-down-at-the-end-of-a-lab-session","title":"Shutting Down (at the end of a Lab Session)","text":"<p>When you've finished working with a robot it's really important to shut it down properly before turning off the power switch. Please refer to the safe shutdown procedures for more info.</p> <ol> <li> <p>If you happen to have closed down the robot terminal, you can return to it by entering <code>waffle X term</code> from a new terminal instance on the laptop.\u00a0\u21a9</p> </li> </ol>"},{"location":"waffles/shutdown/","title":"Shutdown Procedures","text":""},{"location":"waffles/shutdown/#robots","title":"Robots","text":"<p>The Waffles are powered by a Single Board Computer (SBC), which runs a full-blown operating system. As with any operating system, it's important to shut it down properly, rather than simply disconnecting the power, to avoid any data loss or other issues. </p> <p>Therefore, once you've finished working with a robot during a lab session, follow the steps below to shut it down.</p> <ol> <li> <p>Open a new terminal instance on the laptop (Ctrl+Alt+T), and enter the following:</p> <p></p><pre><code>waffle X off\n</code></pre> ... replacing <code>X</code> with the number of the robot that you have been working with.<p></p> </li> <li> <p>You'll be asked to confirm that you want to shut the robot down: </p> <pre><code>[INPUT] Are you sure you want to shutdown dia-waffleX? [y/n] &gt;&gt; \n</code></pre> <p>Enter Y and hit Enter and the robot's SBC will be shut down. </p> </li> <li> <p>Once the blue light on the corner of the SBC goes out, it's then safe to slide the power button to the left to completely turn off the device. </p> <p></p> <p></p> </li> </ol>"},{"location":"waffles/shutdown/#laptops","title":"Laptops","text":"<p>Once you've turned off the robot, remember to shut down the laptop too! Do this by clicking the battery icon in the top right of the desktop and selecting the \"Power Off / Log Out\" option in the drop-down menu.</p> <p></p> <p></p><p></p> <p>Hand your robot and laptop back to a member of the teaching team who will put it away for you!</p> <p></p><p></p>"}]}